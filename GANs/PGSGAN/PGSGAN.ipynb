{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9e6140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbd04af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Based Simulation Code PRINTING STREAMS\n",
    "\n",
    "# Steam representation: Bid price / Ask price / Sell or Buy (1 Sell, 0 Buy) / New Order or Cancellation (1 New, 0 Cancel) \n",
    "#                       / Market or Limit (1 Market, 0 Limit) / Relative Distance (in ticks) / Volume (just 1 in our case)\n",
    "\n",
    "StreamList = [1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Model parameters \n",
    "\n",
    "# Number of price levels\n",
    "\n",
    "N = 20\n",
    "\n",
    "priceLevels = list(range(1, N+1))\n",
    "# print(priceLevels)\n",
    "\n",
    "\n",
    "# Sell side limit order parameters \n",
    "\n",
    "# Sell side limit order arrival hyperparameters \n",
    "ks = 1.92\n",
    "alphas = 0.52\n",
    "\n",
    "lambdaSell = np.zeros(N)\n",
    "\n",
    "\n",
    "for i in priceLevels: \n",
    "    lambdaSell[i-1] = ks / (i ** alphas )\n",
    "\n",
    "lambdaSell[0] = 1.85\n",
    "lambdaSell[1] = 1.51\n",
    "lambdaSell[2] = 1.09\n",
    "lambdaSell[3] = 0.88\n",
    "lambdaSell[4] = 0.77\n",
    "\n",
    "\n",
    "# Buy side limit order parameters \n",
    "\n",
    "# Buy side limit order arrival hyperparameters \n",
    "kb = 1.92\n",
    "alphab = 0.52\n",
    "\n",
    "lambdaBuy = np.zeros(N)\n",
    "\n",
    "for i in priceLevels: \n",
    "    lambdaBuy[i-1] = kb / (i ** alphab )\n",
    "\n",
    "lambdaBuy[0] = 1.85\n",
    "lambdaBuy[1] = 1.51\n",
    "lambdaBuy[2] = 1.09\n",
    "lambdaBuy[3] = 0.88\n",
    "lambdaBuy[4] = 0.77\n",
    "\n",
    "\n",
    "\n",
    "# Sell and buy side market order parameter \n",
    "\n",
    "gammaSell = 0.94\n",
    "gammaBuy = 0.94\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sell side cancelation parameters \n",
    "\n",
    "thetaSell = np.zeros(N)\n",
    "\n",
    "for i in priceLevels: \n",
    "    thetaSell[i-1] = 0.47\n",
    "\n",
    "\n",
    "thetaSell[0] = 0.71\n",
    "thetaSell[1] = 0.81\n",
    "thetaSell[2] = 0.68\n",
    "thetaSell[3] = 0.56\n",
    "thetaSell[4] = 0.47\n",
    "\n",
    "\n",
    "\n",
    "# Buy side cancelation parameters \n",
    "\n",
    "thetaBuy = np.zeros(N)\n",
    "\n",
    "for i in priceLevels: \n",
    "    thetaBuy[i-1] = 0.47\n",
    "\n",
    "\n",
    "thetaBuy[0] = 0.71\n",
    "thetaBuy[1] = 0.81\n",
    "thetaBuy[2] = 0.68\n",
    "thetaBuy[3] = 0.56\n",
    "thetaBuy[4] = 0.47\n",
    "\n",
    "\n",
    "\n",
    "# Initiating current state \n",
    "\n",
    "s = np.zeros(N+2)\n",
    "\n",
    "# Including -1 and +1 artificial limits outside our price range \n",
    "\n",
    "s[0] = -1 \n",
    "s[N+1] = +1 \n",
    "\n",
    "# Initiating an artificial instance\n",
    "\n",
    "for i in priceLevels: \n",
    "    if i <= N/4 :\n",
    "        s[i] = -i\n",
    "    elif i <= N/2 :\n",
    "        s[i] = -(N/2 - i + 1)\n",
    "    elif i <= 3*N/4 :\n",
    "        s[i] = i-(N/2)\n",
    "    else: \n",
    "        s[i] = N-i+1\n",
    "\n",
    "# print(s)\n",
    "\n",
    "\n",
    "#sampleList = []\n",
    "\n",
    "\n",
    "\n",
    "# Running orders upto a specified time \n",
    "\n",
    "T = 100    # Final time\n",
    "t = 0      # Current time \n",
    "\n",
    "while t <= T: \n",
    "    # Identifying the bid and ask price \n",
    "    #sampleList.append(s)\n",
    "    \n",
    "    b = -1 \n",
    "    a = N+1\n",
    "    \n",
    "    currentStream = []\n",
    "    \n",
    "    for i in priceLevels: \n",
    "        if s[i] < 0:\n",
    "            b = i          # Our bid price \n",
    "        \n",
    "        if s[N+1-i] > 0: \n",
    "            a = N+1-i      # Our ask price\n",
    "    # print([a, b])\n",
    "\n",
    "    currentStream.append(b)   # Adding the bid price (before order arrives)\n",
    "    currentStream.append(a)   # Adding the ask price (before order arrives)\n",
    "\n",
    "    # Computing the rate of the next executed order\n",
    "\n",
    "    orderExeRate = 0 \n",
    "\n",
    "    # Adding buy limit rates \n",
    "\n",
    "    for l in list(range(1, a)):\n",
    "        orderExeRate = orderExeRate + lambdaBuy[l-1]\n",
    "\n",
    "    # Adding sell limit rates \n",
    "\n",
    "    for l in list(range(1, N-b+1)):\n",
    "        orderExeRate = orderExeRate + lambdaSell[l-1]\n",
    "\n",
    "\n",
    "    # Addding market buy and market sell rates \n",
    "\n",
    "    orderExeRate = orderExeRate + gammaSell + gammaBuy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Adding buy cencelation rates \n",
    "\n",
    "    for l in list(range(1, a)):\n",
    "        orderExeRate = orderExeRate + thetaBuy[l-1] * (-s[a-l])\n",
    "        # orderExeRate = orderExeRate + thetaBuy[a-l-1] * (-s[l])\n",
    "\n",
    "    # Adding sell cencelation rates \n",
    "\n",
    "    for l in list(range(1, N-b+1)):\n",
    "        orderExeRate = orderExeRate + thetaSell[l-1] * s[b+l]\n",
    "\n",
    "    # print(orderExeRate)\n",
    "\n",
    "\n",
    "    # Creating the next executed order time and updating current time\n",
    "\n",
    "    U = np.random.uniform(low = 0.0, high = 1.0, size = None)            \n",
    "    V = - math.log(1-U) \n",
    "    \n",
    "    t = t + V / orderExeRate \n",
    "\n",
    "\n",
    "    \n",
    "    # Deciding which event will be realized and updating the state\n",
    "\n",
    "    W = np.random.uniform(low = 0.0, high = 1.0, size = None) * orderExeRate \n",
    "\n",
    "    orderExeRate2 = 0\n",
    "\n",
    "    # Checking if limit buy order is realized\n",
    "    for l in list(range(1, a)):\n",
    "        orderExeRate2 = orderExeRate2 + lambdaBuy[l-1]\n",
    "        if orderExeRate2 > W:\n",
    "            # a limit buy at distance l to ask price is ordered \n",
    "            s[a-l] = s[a-l] - 1\n",
    "\n",
    "            W = 100 * orderExeRate \n",
    "            \n",
    "            currentStream.append(0)   # Buy side, hence 0\n",
    "            currentStream.append(1)   # New order, hence 1\n",
    "            currentStream.append(0)   # Limit order, hence 0\n",
    "            currentStream.append(l)   # Relative distance, l\n",
    "            currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "            # print(currentStream)\n",
    "            # print(s)\n",
    "\n",
    "            StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "            break\n",
    "\n",
    "    # Checking if limit sell order is realized\n",
    "    for l in list(range(1, N-b+1)):\n",
    "        orderExeRate2 = orderExeRate2 + lambdaSell[l-1]\n",
    "        if orderExeRate2 > W:\n",
    "            # a limit sell at distance l to bid price is ordered \n",
    "            s[b+l] = s[b+l] + 1\n",
    "\n",
    "            W = 100 * orderExeRate \n",
    "\n",
    "            currentStream.append(1)   # Sell side, hence 1\n",
    "            currentStream.append(1)   # New order, hence 1\n",
    "            currentStream.append(0)   # Limit order, hence 0\n",
    "            currentStream.append(l)   # Relative distance, l\n",
    "            currentStream.append(1)   # Volume, 1, no data \n",
    "            \n",
    "            # print(currentStream)\n",
    "            # print(s)\n",
    "\n",
    "            StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    # Checking if market buy order is realized\n",
    "    orderExeRate2 = orderExeRate2 + gammaBuy\n",
    "    if orderExeRate2 > W:\n",
    "        # a market buy order is realized\n",
    "        s[a] = s[a] - 1\n",
    "\n",
    "        W = 100 * orderExeRate \n",
    "\n",
    "        currentStream.append(0)   # Buy side, hence 0\n",
    "        currentStream.append(1)   # New order, hence 1\n",
    "        currentStream.append(1)   # Market order, hence 1\n",
    "        currentStream.append(0)   # Relative distance, 0\n",
    "        currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "        # print(currentStream)\n",
    "        # print(s)\n",
    "\n",
    "        StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "    # Checking if market sell order is realized\n",
    "    orderExeRate2 = orderExeRate2 + gammaSell\n",
    "    if orderExeRate2 > W:\n",
    "        # a market sell order is realized\n",
    "        s[b] = s[b] + 1\n",
    "\n",
    "        W = 100 * orderExeRate \n",
    "        \n",
    "        currentStream.append(1)   # Sell side, hence 1\n",
    "        currentStream.append(1)   # New order, hence 1\n",
    "        currentStream.append(1)   # Market order, hence 1\n",
    "        currentStream.append(0)   # Relative distance, 0\n",
    "        currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "        # print(currentStream)\n",
    "        # print(s)\n",
    "\n",
    "        StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "    # Checking if limit buy order cancellation is realized\n",
    "\n",
    "    for l in list(range(1, a)):\n",
    "        orderExeRate2 = orderExeRate2 + thetaBuy[l-1] * (-s[a-l])\n",
    "        # orderExeRate = orderExeRate + thetaBuy[a-l-1] * (-s[l])\n",
    "        if orderExeRate2 > W:\n",
    "            # a limit buy at distance l to bid price is cancelled \n",
    "            s[a-l] = s[a-l] + 1\n",
    "            \n",
    "            W = 100 * orderExeRate \n",
    "\n",
    "            currentStream.append(0)   # Buy side, hence 0\n",
    "            currentStream.append(0)   # Cancellation of order, hence 0\n",
    "            currentStream.append(0)   # Limit order, hence 0\n",
    "            currentStream.append(l)   # Relative distance, l\n",
    "            currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "            # print(currentStream)\n",
    "            # print(s)\n",
    "\n",
    "            StreamList = np.vstack([StreamList, currentStream])\n",
    "            \n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    # Checking if limit sell order cancellation is realized\n",
    "\n",
    "    for l in list(range(1, N-b+1)):\n",
    "        orderExeRate2 = orderExeRate2 + thetaSell[l-1] * s[b+l]\n",
    "        \n",
    "        if orderExeRate2 > W:\n",
    "            # a limit buy at distance l to ask price is cancelled \n",
    "            s[b+l] = s[b+l] - 1\n",
    "\n",
    "            W = 100 * orderExeRate \n",
    "\n",
    "            currentStream.append(1)   # Sell side, hence 1\n",
    "            currentStream.append(0)   # Cancellation of order, hence 0\n",
    "            currentStream.append(0)   # Limit order, hence 0\n",
    "            currentStream.append(l)   # Relative distance, l\n",
    "            currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "            # print(currentStream)\n",
    "            # print(s)\n",
    "\n",
    "            StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "# print(StreamList)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## convert your array into a dataframe\n",
    "df = pd.DataFrame (StreamList)\n",
    "\n",
    "## save to xlsx file\n",
    "\n",
    "filepath = 'my_excel_file.xlsx'\n",
    "\n",
    "df.to_excel(filepath, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8012ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Model Based Simulation Code PRINTING STREAMS\n",
    "\n",
    "# Steam representation: Bid price / Ask price / Sell or Buy (1 Sell, 0 Buy) / New Order or Cancellation (1 New, 0 Cancel) \n",
    "#                       / Market or Limit (1 Market, 0 Limit) / Relative Distance (in ticks) / Volume (just 1 in our case)\n",
    "\n",
    "StreamList = [1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Model parameters \n",
    "\n",
    "# Number of price levels\n",
    "\n",
    "N = 20\n",
    "\n",
    "priceLevels = list(range(1, N+1))\n",
    "# print(priceLevels)\n",
    "\n",
    "\n",
    "# Sell side limit order parameters \n",
    "\n",
    "# Sell side limit order arrival hyperparameters \n",
    "ks = 1.92\n",
    "alphas = 0.52\n",
    "\n",
    "lambdaSell = np.zeros(N)\n",
    "\n",
    "\n",
    "for i in priceLevels: \n",
    "    lambdaSell[i-1] = ks / (i ** alphas )\n",
    "\n",
    "lambdaSell[0] = 1.85\n",
    "lambdaSell[1] = 1.51\n",
    "lambdaSell[2] = 1.09\n",
    "lambdaSell[3] = 0.88\n",
    "lambdaSell[4] = 0.77\n",
    "\n",
    "\n",
    "# Buy side limit order parameters \n",
    "\n",
    "# Buy side limit order arrival hyperparameters \n",
    "kb = 1.92\n",
    "alphab = 0.52\n",
    "\n",
    "lambdaBuy = np.zeros(N)\n",
    "\n",
    "for i in priceLevels: \n",
    "    lambdaBuy[i-1] = kb / (i ** alphab )\n",
    "\n",
    "lambdaBuy[0] = 1.85\n",
    "lambdaBuy[1] = 1.51\n",
    "lambdaBuy[2] = 1.09\n",
    "lambdaBuy[3] = 0.88\n",
    "lambdaBuy[4] = 0.77\n",
    "\n",
    "\n",
    "\n",
    "# Sell and buy side market order parameter \n",
    "\n",
    "gammaSell = 0.94\n",
    "gammaBuy = 0.94\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sell side cancelation parameters \n",
    "\n",
    "thetaSell = np.zeros(N)\n",
    "\n",
    "for i in priceLevels: \n",
    "    thetaSell[i-1] = 0.47\n",
    "\n",
    "\n",
    "thetaSell[0] = 0.71\n",
    "thetaSell[1] = 0.81\n",
    "thetaSell[2] = 0.68\n",
    "thetaSell[3] = 0.56\n",
    "thetaSell[4] = 0.47\n",
    "\n",
    "\n",
    "\n",
    "# Buy side cancelation parameters \n",
    "\n",
    "thetaBuy = np.zeros(N)\n",
    "\n",
    "for i in priceLevels: \n",
    "    thetaBuy[i-1] = 0.47\n",
    "\n",
    "\n",
    "thetaBuy[0] = 0.71\n",
    "thetaBuy[1] = 0.81\n",
    "thetaBuy[2] = 0.68\n",
    "thetaBuy[3] = 0.56\n",
    "thetaBuy[4] = 0.47\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Running Big Replications \n",
    "\n",
    "# BigRepNumber = 10\n",
    "# BigReplications = list(range(1, BigRepNumber+1))\n",
    "RequiredtotalSuccessfulRepNumber = 10\n",
    "totalSuccessfulRepNumber = 0\n",
    "\n",
    "\n",
    "BigStreamList = [1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# for r in BigReplications: \n",
    "while totalSuccessfulRepNumber < RequiredtotalSuccessfulRepNumber:\n",
    "\n",
    "    StreamList = [1, 1, 1, 1, 1, 1, 1] \n",
    "\n",
    "\n",
    "    # Initiating current state \n",
    "\n",
    "    s = np.zeros(N+2)\n",
    "\n",
    "    # Including -1 and +1 artificial limits outside our price range \n",
    "\n",
    "    s[0] = -1 \n",
    "    s[N+1] = +1 \n",
    "\n",
    "    # Initiating an artificial instance\n",
    "\n",
    "    for i in priceLevels: \n",
    "        if i <= N/4 :\n",
    "            s[i] = -i\n",
    "        elif i <= N/2 :\n",
    "            s[i] = -(N/2 - i + 1)\n",
    "        elif i <= 3*N/4 :\n",
    "            s[i] = i-(N/2)\n",
    "        else: \n",
    "            s[i] = N-i+1\n",
    "\n",
    "    # print(s)\n",
    "\n",
    "    #sampleList = []\n",
    "\n",
    "\n",
    "\n",
    "    # Running orders upto a specified time \n",
    "\n",
    "    T = 100    # Final time\n",
    "    t = 0      # Current time \n",
    "\n",
    "    while t <= T: \n",
    "        # Identifying the bid and ask price \n",
    "        #sampleList.append(s)\n",
    "        \n",
    "        b = -1 \n",
    "        a = N+1\n",
    "        \n",
    "        currentStream = []\n",
    "        \n",
    "        for i in priceLevels: \n",
    "            if s[i] < 0:\n",
    "                b = i          # Our bid price \n",
    "            \n",
    "            if s[N+1-i] > 0: \n",
    "                a = N+1-i      # Our ask price\n",
    "        # print([a, b])\n",
    "\n",
    "        # Lets go out of while loop for weird cases\n",
    "        if b==-1: \n",
    "            break\n",
    "        elif a==N+1:\n",
    "            break \n",
    "\n",
    "        currentStream.append(b)   # Adding the bid price (before order arrives)\n",
    "        currentStream.append(a)   # Adding the ask price (before order arrives)\n",
    "\n",
    "        # Computing the rate of the next executed order\n",
    "\n",
    "        orderExeRate = 0 \n",
    "\n",
    "        # Adding buy limit rates \n",
    "\n",
    "        for l in list(range(1, a)):\n",
    "            orderExeRate = orderExeRate + lambdaBuy[l-1]\n",
    "\n",
    "        # Adding sell limit rates \n",
    "\n",
    "        for l in list(range(1, N-b+1)):\n",
    "            orderExeRate = orderExeRate + lambdaSell[l-1]\n",
    "\n",
    "\n",
    "        # Addding market buy and market sell rates \n",
    "\n",
    "        orderExeRate = orderExeRate + gammaSell + gammaBuy \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Adding buy cencelation rates \n",
    "\n",
    "        for l in list(range(1, a)):\n",
    "            orderExeRate = orderExeRate + thetaBuy[l-1] * (-s[a-l])\n",
    "            # orderExeRate = orderExeRate + thetaBuy[a-l-1] * (-s[l])\n",
    "\n",
    "        # Adding sell cencelation rates \n",
    "\n",
    "        for l in list(range(1, N-b+1)):\n",
    "            orderExeRate = orderExeRate + thetaSell[l-1] * s[b+l]\n",
    "\n",
    "        # print(orderExeRate)\n",
    "\n",
    "\n",
    "        # Creating the next executed order time and updating current time\n",
    "\n",
    "        U = np.random.uniform(low = 0.0, high = 1.0, size = None)            \n",
    "        V = - math.log(1-U) \n",
    "        \n",
    "        t = t + V / orderExeRate \n",
    "\n",
    "\n",
    "        \n",
    "        # Deciding which event will be realized and updating the state\n",
    "\n",
    "        W = np.random.uniform(low = 0.0, high = 1.0, size = None) * orderExeRate \n",
    "\n",
    "        orderExeRate2 = 0\n",
    "\n",
    "        # Checking if limit buy order is realized\n",
    "        for l in list(range(1, a)):\n",
    "            orderExeRate2 = orderExeRate2 + lambdaBuy[l-1]\n",
    "            if orderExeRate2 > W:\n",
    "                # a limit buy at distance l to ask price is ordered \n",
    "                s[a-l] = s[a-l] - 1\n",
    "\n",
    "                W = 100 * orderExeRate \n",
    "                \n",
    "                currentStream.append(0)   # Buy side, hence 0\n",
    "                currentStream.append(1)   # New order, hence 1\n",
    "                currentStream.append(0)   # Limit order, hence 0\n",
    "                currentStream.append(l)   # Relative distance, l\n",
    "                currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "                # print(currentStream)\n",
    "                # print(s)\n",
    "\n",
    "                StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "                break\n",
    "\n",
    "        # Checking if limit sell order is realized\n",
    "        for l in list(range(1, N-b+1)):\n",
    "            orderExeRate2 = orderExeRate2 + lambdaSell[l-1]\n",
    "            if orderExeRate2 > W:\n",
    "                # a limit sell at distance l to bid price is ordered \n",
    "                s[b+l] = s[b+l] + 1\n",
    "\n",
    "                W = 100 * orderExeRate \n",
    "\n",
    "                currentStream.append(1)   # Sell side, hence 1\n",
    "                currentStream.append(1)   # New order, hence 1\n",
    "                currentStream.append(0)   # Limit order, hence 0\n",
    "                currentStream.append(l)   # Relative distance, l\n",
    "                currentStream.append(1)   # Volume, 1, no data \n",
    "                \n",
    "                # print(currentStream)\n",
    "                # print(s)\n",
    "\n",
    "                StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        # Checking if market buy order is realized\n",
    "        orderExeRate2 = orderExeRate2 + gammaBuy\n",
    "        if orderExeRate2 > W:\n",
    "            # a market buy order is realized\n",
    "            s[a] = s[a] - 1\n",
    "\n",
    "            W = 100 * orderExeRate \n",
    "\n",
    "            currentStream.append(0)   # Buy side, hence 0\n",
    "            currentStream.append(1)   # New order, hence 1\n",
    "            currentStream.append(1)   # Market order, hence 1\n",
    "            currentStream.append(0)   # Relative distance, 0\n",
    "            currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "            # print(currentStream)\n",
    "            # print(s)\n",
    "\n",
    "            StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "        # Checking if market sell order is realized\n",
    "        orderExeRate2 = orderExeRate2 + gammaSell\n",
    "        if orderExeRate2 > W:\n",
    "            # a market sell order is realized\n",
    "            s[b] = s[b] + 1\n",
    "\n",
    "            W = 100 * orderExeRate \n",
    "            \n",
    "            currentStream.append(1)   # Sell side, hence 1\n",
    "            currentStream.append(1)   # New order, hence 1\n",
    "            currentStream.append(1)   # Market order, hence 1\n",
    "            currentStream.append(0)   # Relative distance, 0\n",
    "            currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "            # print(currentStream)\n",
    "            # print(s)\n",
    "\n",
    "            StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "        # Checking if limit buy order cancellation is realized\n",
    "\n",
    "        for l in list(range(1, a)):\n",
    "            orderExeRate2 = orderExeRate2 + thetaBuy[l-1] * (-s[a-l])\n",
    "            # orderExeRate = orderExeRate + thetaBuy[a-l-1] * (-s[l])\n",
    "            if orderExeRate2 > W:\n",
    "                # a limit buy at distance l to bid price is cancelled \n",
    "                s[a-l] = s[a-l] + 1\n",
    "                \n",
    "                W = 100 * orderExeRate \n",
    "\n",
    "                currentStream.append(0)   # Buy side, hence 0\n",
    "                currentStream.append(0)   # Cancellation of order, hence 0\n",
    "                currentStream.append(0)   # Limit order, hence 0\n",
    "                currentStream.append(l)   # Relative distance, l\n",
    "                currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "                # print(currentStream)\n",
    "                # print(s)\n",
    "\n",
    "                StreamList = np.vstack([StreamList, currentStream])\n",
    "                \n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        # Checking if limit sell order cancellation is realized\n",
    "\n",
    "        for l in list(range(1, N-b+1)):\n",
    "            orderExeRate2 = orderExeRate2 + thetaSell[l-1] * s[b+l]\n",
    "            \n",
    "            if orderExeRate2 > W:\n",
    "                # a limit buy at distance l to ask price is cancelled \n",
    "                s[b+l] = s[b+l] - 1\n",
    "\n",
    "                W = 100 * orderExeRate \n",
    "\n",
    "                currentStream.append(1)   # Sell side, hence 1\n",
    "                currentStream.append(0)   # Cancellation of order, hence 0\n",
    "                currentStream.append(0)   # Limit order, hence 0\n",
    "                currentStream.append(l)   # Relative distance, l\n",
    "                currentStream.append(1)   # Volume, 1, no data \n",
    "\n",
    "                # print(currentStream)\n",
    "                # print(s)\n",
    "\n",
    "                StreamList = np.vstack([StreamList, currentStream])\n",
    "\n",
    "                break\n",
    "\n",
    "    # Lets not include bad cases \n",
    "    if b>-1:\n",
    "        if a<N+1: \n",
    "            StreamList = np.delete(StreamList, slice(0, N**2), axis=0)\n",
    "            BigStreamList = np.vstack([BigStreamList, StreamList])\n",
    "            totalSuccessfulRepNumber = totalSuccessfulRepNumber + 1\n",
    "    \n",
    "    # print(StreamList)  \n",
    "    # BigStreamList = np.vstack([BigStreamList, StreamList])\n",
    "    # totalSuccessfulRepNumber = totalSuccessfulRepNumber + 1\n",
    "\n",
    "print(totalSuccessfulRepNumber)\n",
    "BigStreamList = np.delete(BigStreamList, slice(0, 1), axis=0) \n",
    "\n",
    "## convert your array into a dataframe\n",
    "df = pd.DataFrame (BigStreamList)\n",
    "\n",
    "## save to xlsx file\n",
    "\n",
    "filepath = 'Data_1.xlsx'\n",
    "\n",
    "df.to_excel(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dee15c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "  \n",
    "# Read and store content\n",
    "# of an excel file \n",
    "read_file = pd.read_excel (\"Data_1.xlsx\")\n",
    "  \n",
    "# Write the dataframe object\n",
    "# into csv file\n",
    "read_file.to_csv (\"Data_1.csv\", \n",
    "                  index = None,\n",
    "                  header=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7d72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional\n",
    "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils._testing import ignore_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "033e190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('Data_1.csv')\n",
    "\n",
    "df = pd.concat([data1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0e3dc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 34380\n",
      "Number of columns: 7\n",
      "Columns Names: Index(['0', '1', '2', '3', '4', '5', '6'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34375</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34376</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34377</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34378</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34379</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34380 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2  3  4   5  6\n",
       "0      7  8  0  1  0   2  1\n",
       "1      7  8  1  0  0   6  1\n",
       "2      7  8  0  0  0   5  1\n",
       "3      7  8  0  0  0   4  1\n",
       "4      7  8  1  0  0   1  1\n",
       "...   .. .. .. .. ..  .. ..\n",
       "34375  7  8  0  0  0   7  1\n",
       "34376  7  8  1  1  0   7  1\n",
       "34377  7  8  1  1  0  13  1\n",
       "34378  7  8  1  1  0  10  1\n",
       "34379  7  8  1  0  0  11  1\n",
       "\n",
       "[34380 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows:\", (df.values).shape[0])\n",
    "print(\"Number of columns:\", (df.values).shape[1])\n",
    "print(\"Columns Names:\", df.columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de34dcd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before discarding: 34380\n",
      "Number of rows after discarding: 33870\n"
     ]
    }
   ],
   "source": [
    "discard = []\n",
    "\n",
    "# Output\n",
    "print(\"Number of rows before discarding:\", (df.values).shape[0])\n",
    "\n",
    "# Appending indices in a list which will be discarded\n",
    "for i in range(0, (df.values).shape[0]):\n",
    "  if((df['1'][i] - df['0'][i]) >= 4):\n",
    "    discard.append(i)\n",
    "\n",
    "# Dropping those indices from the dataframe\n",
    "for i in discard:\n",
    "  df = df.drop(i)\n",
    "\n",
    "# Output\n",
    "print(\"Number of rows after discarding:\", (df.values).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13c954a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33870 entries, 0 to 34379\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       33870 non-null  int64\n",
      " 1   1       33870 non-null  int64\n",
      " 2   2       33870 non-null  int64\n",
      " 3   3       33870 non-null  int64\n",
      " 4   4       33870 non-null  int64\n",
      " 5   5       33870 non-null  int64\n",
      " 6   6       33870 non-null  int64\n",
      "dtypes: int64(7)\n",
      "memory usage: 2.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Checking the datatype of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0eaff83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping column 0 and column 1, since those are not \n",
    "# relevant to our study\n",
    "df = df.drop(['0'], axis = 1)\n",
    "df = df.drop(['1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eaff9db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Distribution in column 2: \n",
      "0    16978\n",
      "1    16892\n",
      "Name: 2, dtype: int64\n",
      "\n",
      "Data Distribution in column 3: \n",
      "1    18565\n",
      "0    15305\n",
      "Name: 3, dtype: int64\n",
      "\n",
      "Data Distribution in column 4: \n",
      "0    32197\n",
      "1     1673\n",
      "Name: 4, dtype: int64\n",
      "\n",
      "Data Distribution in column 5: \n",
      "1     5521\n",
      "2     5194\n",
      "3     3979\n",
      "4     3129\n",
      "5     2589\n",
      "6     2483\n",
      "7     2149\n",
      "8     1907\n",
      "0     1673\n",
      "9     1553\n",
      "10    1155\n",
      "11     942\n",
      "12     619\n",
      "13     433\n",
      "14     247\n",
      "15     147\n",
      "16      78\n",
      "17      54\n",
      "18      16\n",
      "19       2\n",
      "Name: 5, dtype: int64\n",
      "\n",
      "Data Distribution in column 6: \n",
      "1    33870\n",
      "Name: 6, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Distribution in column 2: \")\n",
    "print(df['2'].value_counts())\n",
    "print(\"\\nData Distribution in column 3: \")\n",
    "print(df['3'].value_counts())\n",
    "print(\"\\nData Distribution in column 4: \")\n",
    "print(df['4'].value_counts())\n",
    "print(\"\\nData Distribution in column 5: \")\n",
    "print(df['5'].value_counts())\n",
    "print(\"\\nData Distribution in column 6: \")\n",
    "print(df['6'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64dd924a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Distribution in column 2: \n",
      "0    16978\n",
      "1    16892\n",
      "Name: 2, dtype: int64\n",
      "\n",
      "Data Distribution in column 3: \n",
      "1    18565\n",
      "0    15305\n",
      "Name: 3, dtype: int64\n",
      "\n",
      "Data Distribution in column 4: \n",
      "0    32197\n",
      "1     1673\n",
      "Name: 4, dtype: int64\n",
      "\n",
      "Data Distribution in column 5: \n",
      "1     5521\n",
      "2     5194\n",
      "3     3979\n",
      "4     3129\n",
      "5     2589\n",
      "6     2483\n",
      "7     2149\n",
      "8     1907\n",
      "0     1673\n",
      "9     1553\n",
      "10    1155\n",
      "11     942\n",
      "12     619\n",
      "13     433\n",
      "14     247\n",
      "15     147\n",
      "16      78\n",
      "17      54\n",
      "18      16\n",
      "19       2\n",
      "Name: 5, dtype: int64\n",
      "\n",
      "Data Distribution in column 6: \n",
      "1    33870\n",
      "Name: 6, dtype: int64\n",
      "Column 2 (Buy): Real Ratio: 0.501269560082669\n",
      "Column 3 (New (not cancel)):  Real Ratio:  0.45187481547091823\n",
      "Column 4 (Limit): Real Ratio:  0.9506052553882491\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Distribution in column 2: \")\n",
    "print(df['2'].value_counts())\n",
    "print(\"\\nData Distribution in column 3: \")\n",
    "print(df['3'].value_counts())\n",
    "print(\"\\nData Distribution in column 4: \")\n",
    "print(df['4'].value_counts())\n",
    "print(\"\\nData Distribution in column 5: \")\n",
    "print(df['5'].value_counts())\n",
    "print(\"\\nData Distribution in column 6: \")\n",
    "print(df['6'].value_counts())\n",
    "\n",
    "realCol2 = df['2'].value_counts()\n",
    "realCol3 = df['3'].value_counts()\n",
    "realCol4 = df['4'].value_counts()\n",
    "\n",
    "realCol2Ratio = realCol2[0] / ( realCol2[0] + realCol2[1]  )\n",
    "realCol3Ratio = realCol3[0] / ( realCol3[0] + realCol3[1]  )\n",
    "realCol4Ratio = realCol4[0] / ( realCol4[0] + realCol4[1]  )\n",
    "\n",
    "\n",
    "print(\"Column 2 (Buy): Real Ratio:\", realCol2Ratio)\n",
    "print(\"Column 3 (New (not cancel)):  Real Ratio: \", realCol3Ratio)\n",
    "print(\"Column 4 (Limit): Real Ratio: \", realCol4Ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "396e0fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(object):\n",
    "    def __init__(self, data, output_info, log_frequency):\n",
    "        self.model = []\n",
    "\n",
    "        start = 0\n",
    "        skip = False\n",
    "        max_interval = 0\n",
    "        counter = 0\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                start += item[0]\n",
    "                skip = True\n",
    "                continue\n",
    "\n",
    "            elif item[1] == 'softmax':\n",
    "                if skip:\n",
    "                    skip = False\n",
    "                    start += item[0]\n",
    "                    continue\n",
    "\n",
    "                end = start + item[0]\n",
    "                max_interval = max(max_interval, end - start)\n",
    "                counter += 1\n",
    "                self.model.append(np.argmax(data[:, start:end], axis=-1))\n",
    "                start = end\n",
    "\n",
    "            else:\n",
    "                assert 0\n",
    "\n",
    "        assert start == data.shape[1]\n",
    "\n",
    "        self.interval = []\n",
    "        self.n_col = 0\n",
    "        self.n_opt = 0\n",
    "        skip = False\n",
    "        start = 0\n",
    "        self.p = np.zeros((counter, max_interval))\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                skip = True\n",
    "                start += item[0]\n",
    "                continue\n",
    "            elif item[1] == 'softmax':\n",
    "                if skip:\n",
    "                    start += item[0]\n",
    "                    skip = False\n",
    "                    continue\n",
    "                end = start + item[0]\n",
    "                tmp = np.sum(data[:, start:end], axis=0)\n",
    "                if log_frequency:\n",
    "                    tmp = np.log(tmp + 1)\n",
    "                tmp = tmp / np.sum(tmp)\n",
    "                self.p[self.n_col, :item[0]] = tmp\n",
    "                self.interval.append((self.n_opt, item[0]))\n",
    "                self.n_opt += item[0]\n",
    "                self.n_col += 1\n",
    "                start = end\n",
    "            else:\n",
    "                assert 0\n",
    "\n",
    "        self.interval = np.asarray(self.interval)\n",
    "\n",
    "    def random_choice_prob_index(self, idx):\n",
    "        a = self.p[idx]\n",
    "        r = np.expand_dims(np.random.rand(a.shape[0]), axis=1)\n",
    "        return (a.cumsum(axis=1) > r).argmax(axis=1)\n",
    "\n",
    "    def sample(self, batch):\n",
    "        if self.n_col == 0:\n",
    "            return None\n",
    "\n",
    "        batch = batch\n",
    "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
    "\n",
    "        vec1 = np.zeros((batch, self.n_opt), dtype='float32')\n",
    "        mask1 = np.zeros((batch, self.n_col), dtype='float32')\n",
    "        mask1[np.arange(batch), idx] = 1\n",
    "        opt1prime = self.random_choice_prob_index(idx)\n",
    "        opt1 = self.interval[idx, 0] + opt1prime\n",
    "        vec1[np.arange(batch), opt1] = 1\n",
    "\n",
    "        return vec1, mask1, idx, opt1prime\n",
    "\n",
    "    def sample_zero(self, batch):\n",
    "        if self.n_col == 0:\n",
    "            return None\n",
    "\n",
    "        vec = np.zeros((batch, self.n_opt), dtype='float32')\n",
    "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
    "        for i in range(batch):\n",
    "            col = idx[i]\n",
    "            pick = int(np.random.choice(self.model[col]))\n",
    "            vec[i, pick + self.interval[col, 0]] = 1\n",
    "\n",
    "        return vec\n",
    "\n",
    "class Discriminator(Module):\n",
    "\n",
    "    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "\n",
    "        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
    "        alpha = alpha.repeat(1, pac, real_data.size(1))\n",
    "        alpha = alpha.view(-1, real_data.size(1))\n",
    "\n",
    "        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "        disc_interpolates = self(interpolates)\n",
    "\n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=disc_interpolates, inputs=interpolates,\n",
    "            grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
    "            create_graph=True, retain_graph=True, only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        gradient_penalty = ((\n",
    "            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1\n",
    "        ) ** 2).mean() * lambda_\n",
    "\n",
    "        return gradient_penalty\n",
    "\n",
    "    def __init__(self, input_dim, dis_dims, pack=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        dim = input_dim * pack\n",
    "        self.pack = pack\n",
    "        self.packdim = dim\n",
    "        seq = []\n",
    "        for item in list(dis_dims):\n",
    "            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]\n",
    "            dim = item\n",
    "\n",
    "        seq += [Linear(dim, 1)]\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        assert input.size()[0] % self.pack == 0\n",
    "        return self.seq(input.view(-1, self.packdim))\n",
    "\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, i, o):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fc = Linear(i, o)\n",
    "        self.bn = BatchNorm1d(o)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.fc(input)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return torch.cat([out, input], dim=1)\n",
    "\n",
    "\n",
    "class Generator(Module):\n",
    "    def __init__(self, embedding_dim, gen_dims, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        dim = embedding_dim\n",
    "        seq = []\n",
    "        for item in list(gen_dims):\n",
    "            seq += [Residual(dim, item)]\n",
    "            dim += item\n",
    "        seq.append(Linear(dim, data_dim))\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        data = self.seq(input)\n",
    "        return data\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"docstring for Sampler.\"\"\"\n",
    "\n",
    "    def __init__(self, data, output_info):\n",
    "        super(Sampler, self).__init__()\n",
    "        self.data = data\n",
    "        self.model = []\n",
    "        self.n = len(data)\n",
    "\n",
    "        st = 0\n",
    "        skip = False\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                st += item[0]\n",
    "                skip = True\n",
    "            elif item[1] == 'softmax':\n",
    "                if skip:\n",
    "                    skip = False\n",
    "                    st += item[0]\n",
    "                    continue\n",
    "\n",
    "                ed = st + item[0]\n",
    "                tmp = []\n",
    "                for j in range(item[0]):\n",
    "                    tmp.append(np.nonzero(data[:, st + j])[0])\n",
    "\n",
    "                self.model.append(tmp)\n",
    "                st = ed\n",
    "            else:\n",
    "                assert 0\n",
    "\n",
    "        assert st == data.shape[1]\n",
    "\n",
    "    def sample(self, n, col, opt):\n",
    "        if col is None:\n",
    "            idx = np.random.choice(np.arange(self.n), n)\n",
    "            return self.data[idx]\n",
    "\n",
    "        idx = []\n",
    "        for c, o in zip(col, opt):\n",
    "            idx.append(np.random.choice(self.model[c][o]))\n",
    "\n",
    "        return self.data[idx]\n",
    "\n",
    "class DataTransformer(object):\n",
    "    def __init__(self, n_clusters=10, epsilon=0.005):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def _fit_continuous(self, column, data):\n",
    "        gm = BayesianGaussianMixture(\n",
    "            self.n_clusters,\n",
    "            weight_concentration_prior_type='dirichlet_process',\n",
    "            weight_concentration_prior=0.001,\n",
    "            n_init=1\n",
    "        )\n",
    "        gm.fit(data)\n",
    "        components = gm.weights_ > self.epsilon\n",
    "        num_components = components.sum()\n",
    "\n",
    "        return {\n",
    "            'name': column,\n",
    "            'model': gm,\n",
    "            'components': components,\n",
    "            'output_info': [(1, 'tanh'), (num_components, 'softmax')],\n",
    "            'output_dimensions': 1 + num_components,\n",
    "        }\n",
    "\n",
    "    def _fit_discrete(self, column, data):\n",
    "        ohe = OneHotEncoder(sparse=False)\n",
    "        ohe.fit(data)\n",
    "        categories = len(ohe.categories_[0])\n",
    "\n",
    "        return {\n",
    "            'name': column,\n",
    "            'encoder': ohe,\n",
    "            'output_info': [(categories, 'softmax')],\n",
    "            'output_dimensions': categories\n",
    "        }\n",
    "\n",
    "    def fit(self, data, discrete_columns=tuple()):\n",
    "        self.output_info = []\n",
    "        self.output_dimensions = 0\n",
    "\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            self.dataframe = False\n",
    "            data = pd.DataFrame(data)\n",
    "        else:\n",
    "            self.dataframe = True\n",
    "\n",
    "        self.meta = []\n",
    "        for column in data.columns:\n",
    "            column_data = data[[column]].values\n",
    "            if column in discrete_columns:\n",
    "                meta = self._fit_discrete(column, column_data)\n",
    "            else:\n",
    "                meta = self._fit_continuous(column, column_data)\n",
    "\n",
    "            self.output_info += meta['output_info']\n",
    "            self.output_dimensions += meta['output_dimensions']\n",
    "            self.meta.append(meta)\n",
    "\n",
    "    def _transform_continuous(self, column_meta, data):\n",
    "        components = column_meta['components']\n",
    "        model = column_meta['model']\n",
    "\n",
    "        means = model.means_.reshape((1, self.n_clusters))\n",
    "        stds = np.sqrt(model.covariances_).reshape((1, self.n_clusters))\n",
    "        features = (data - means) / (4 * stds)\n",
    "\n",
    "        probs = model.predict_proba(data)\n",
    "\n",
    "        n_opts = components.sum()\n",
    "        features = features[:, components]\n",
    "        probs = probs[:, components]\n",
    "\n",
    "        opt_sel = np.zeros(len(data), dtype='int')\n",
    "        for i in range(len(data)):\n",
    "            pp = probs[i] + 1e-6\n",
    "            pp = pp / pp.sum()\n",
    "            opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)\n",
    "\n",
    "        idx = np.arange((len(features)))\n",
    "        features = features[idx, opt_sel].reshape([-1, 1])\n",
    "        features = np.clip(features, -.99, .99)\n",
    "\n",
    "        probs_onehot = np.zeros_like(probs)\n",
    "        probs_onehot[np.arange(len(probs)), opt_sel] = 1\n",
    "        return [features, probs_onehot]\n",
    "\n",
    "    def _transform_discrete(self, column_meta, data):\n",
    "        encoder = column_meta['encoder']\n",
    "        return encoder.transform(data)\n",
    "\n",
    "    def transform(self, data):\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            data = pd.DataFrame(data)\n",
    "\n",
    "        values = []\n",
    "        for meta in self.meta:\n",
    "            column_data = data[[meta['name']]].values\n",
    "            if 'model' in meta:\n",
    "                values += self._transform_continuous(meta, column_data)\n",
    "            else:\n",
    "                values.append(self._transform_discrete(meta, column_data))\n",
    "\n",
    "        return np.concatenate(values, axis=1).astype(float)\n",
    "\n",
    "    def _inverse_transform_continuous(self, meta, data, sigma):\n",
    "        model = meta['model']\n",
    "        components = meta['components']\n",
    "\n",
    "        u = data[:, 0]\n",
    "        v = data[:, 1:]\n",
    "\n",
    "        if sigma is not None:\n",
    "            u = np.random.normal(u, sigma)\n",
    "\n",
    "        u = np.clip(u, -1, 1)\n",
    "        v_t = np.ones((len(data), self.n_clusters)) * -100\n",
    "        v_t[:, components] = v\n",
    "        v = v_t\n",
    "        means = model.means_.reshape([-1])\n",
    "        stds = np.sqrt(model.covariances_).reshape([-1])\n",
    "        p_argmax = np.argmax(v, axis=1)\n",
    "        std_t = stds[p_argmax]\n",
    "        mean_t = means[p_argmax]\n",
    "        column = u * 4 * std_t + mean_t\n",
    "\n",
    "        return column\n",
    "\n",
    "    def _inverse_transform_discrete(self, meta, data):\n",
    "        encoder = meta['encoder']\n",
    "        return encoder.inverse_transform(data)\n",
    "\n",
    "    def inverse_transform(self, data, sigmas):\n",
    "        start = 0\n",
    "        output = []\n",
    "        column_names = []\n",
    "        for meta in self.meta:\n",
    "            dimensions = meta['output_dimensions']\n",
    "            columns_data = data[:, start:start + dimensions]\n",
    "\n",
    "            if 'model' in meta:\n",
    "                sigma = sigmas[start] if sigmas else None\n",
    "                inverted = self._inverse_transform_continuous(meta, columns_data, sigma)\n",
    "            else:\n",
    "                inverted = self._inverse_transform_discrete(meta, columns_data)\n",
    "\n",
    "            output.append(inverted)\n",
    "            column_names.append(meta['name'])\n",
    "            start += dimensions\n",
    "\n",
    "        output = np.column_stack(output)\n",
    "        if self.dataframe:\n",
    "            output = pd.DataFrame(output, columns=column_names)\n",
    "        \n",
    "        x = list(output[(output['2'] == 0) & (output['3'] == 0) & (output['4'] == 1)].index)\n",
    "        y = list(output[(output['2'] == 1) & (output['3'] == 0) & (output['4'] == 1)].index)\n",
    "\n",
    "        for i in x:\n",
    "          output = output.drop(i)\n",
    "        \n",
    "        for j in y:\n",
    "          output = output.drop(j)\n",
    "      \n",
    "        return output\n",
    "\n",
    "class PSGANS(object):\n",
    "    def __init__(self, embedding_dim=128, gen_dim=(256, 256), dis_dim=(256, 256),\n",
    "                 l2scale=1e-6, batch_size=500):\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.gen_dim = gen_dim\n",
    "        self.dis_dim = dis_dim\n",
    "\n",
    "        self.l2scale = l2scale\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.trained_epoches = 0\n",
    "\n",
    "    def _apply_activate(self, data):\n",
    "        data_t = []\n",
    "        st = 0\n",
    "        for item in self.transformer.output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                ed = st + item[0]\n",
    "                data_t.append(torch.tanh(data[:, st:ed]))\n",
    "                st = ed\n",
    "            elif item[1] == 'softmax':\n",
    "                ed = st + item[0]\n",
    "                data_t.append(functional.gumbel_softmax(data[:, st:ed], tau=0.2))\n",
    "                st = ed\n",
    "            else:\n",
    "                assert 0\n",
    "\n",
    "        return torch.cat(data_t, dim=1)\n",
    "\n",
    "    def _cond_loss(self, data, c, m):\n",
    "        loss = []\n",
    "        st = 0\n",
    "        st_c = 0\n",
    "        skip = False\n",
    "        for item in self.transformer.output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                st += item[0]\n",
    "                skip = True\n",
    "\n",
    "            elif item[1] == 'softmax':\n",
    "                if skip:\n",
    "                    skip = False\n",
    "                    st += item[0]\n",
    "                    continue\n",
    "\n",
    "                ed = st + item[0]\n",
    "                ed_c = st_c + item[0]\n",
    "                tmp = functional.cross_entropy(\n",
    "                    data[:, st:ed],\n",
    "                    torch.argmax(c[:, st_c:ed_c], dim=1),\n",
    "                    reduction='none'\n",
    "                )\n",
    "                loss.append(tmp)\n",
    "                st = ed\n",
    "                st_c = ed_c\n",
    "\n",
    "            else:\n",
    "                assert 0\n",
    "\n",
    "        loss = torch.stack(loss, dim=1)\n",
    "\n",
    "        return (loss * m).sum() / data.size()[0]\n",
    "\n",
    "    def fit(self, train_data, discrete_columns=tuple(), epochs=300, log_frequency=True):\n",
    "        if not hasattr(self, \"transformer\"):\n",
    "            self.transformer = DataTransformer()\n",
    "            self.transformer.fit(train_data, discrete_columns)\n",
    "        train_data = self.transformer.transform(train_data)\n",
    "\n",
    "        data_sampler = Sampler(train_data, self.transformer.output_info)\n",
    "\n",
    "        data_dim = self.transformer.output_dimensions\n",
    "\n",
    "        if not hasattr(self, \"cond_generator\"):\n",
    "            self.cond_generator = ConditionalGenerator(\n",
    "                train_data,\n",
    "                self.transformer.output_info,\n",
    "                log_frequency\n",
    "            )\n",
    "\n",
    "        if not hasattr(self, \"generator\"):\n",
    "            self.generator = Generator(\n",
    "                self.embedding_dim + self.cond_generator.n_opt,\n",
    "                self.gen_dim,\n",
    "                data_dim\n",
    "            ).to(self.device)\n",
    "\n",
    "        if not hasattr(self, \"discriminator\"):\n",
    "            self.discriminator = Discriminator(\n",
    "                data_dim + self.cond_generator.n_opt,\n",
    "                self.dis_dim\n",
    "            ).to(self.device)\n",
    "\n",
    "        if not hasattr(self, \"optimizerG\"):\n",
    "            self.optimizerG = optim.Adam(\n",
    "                self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9),\n",
    "                weight_decay=self.l2scale\n",
    "            )\n",
    "\n",
    "        if not hasattr(self, \"optimizerD\"):\n",
    "            self.optimizerD = optim.Adam(\n",
    "                self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "\n",
    "        assert self.batch_size % 2 == 0\n",
    "        mean = torch.zeros(self.batch_size, self.embedding_dim, device=self.device)\n",
    "        std = mean + 1\n",
    "\n",
    "        steps_per_epoch = max(len(train_data) // self.batch_size, 1)\n",
    "        for i in range(epochs):\n",
    "            self.trained_epoches += 1\n",
    "            for id_ in range(steps_per_epoch):\n",
    "                fakez = torch.normal(mean=mean, std=std)\n",
    "\n",
    "                condvec = self.cond_generator.sample(self.batch_size)\n",
    "                if condvec is None:\n",
    "                    c1, m1, col, opt = None, None, None, None\n",
    "                    real = data_sampler.sample(self.batch_size, col, opt)\n",
    "                else:\n",
    "                    c1, m1, col, opt = condvec\n",
    "                    c1 = torch.from_numpy(c1).to(self.device)\n",
    "                    m1 = torch.from_numpy(m1).to(self.device)\n",
    "                    fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                    perm = np.arange(self.batch_size)\n",
    "                    np.random.shuffle(perm)\n",
    "                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])\n",
    "                    c2 = c1[perm]\n",
    "\n",
    "                fake = self.generator(fakez)\n",
    "                fakeact = self._apply_activate(fake)\n",
    "\n",
    "                real = torch.from_numpy(real.astype('float32')).to(self.device)\n",
    "\n",
    "                if c1 is not None:\n",
    "                    fake_cat = torch.cat([fakeact, c1], dim=1)\n",
    "                    real_cat = torch.cat([real, c2], dim=1)\n",
    "                else:\n",
    "                    real_cat = real\n",
    "                    fake_cat = fake\n",
    "\n",
    "                y_fake = self.discriminator(fake_cat)\n",
    "                y_real = self.discriminator(real_cat)\n",
    "\n",
    "                pen = self.discriminator.calc_gradient_penalty(\n",
    "                    real_cat, fake_cat, self.device)\n",
    "                loss_d = -(torch.mean(y_real) - torch.mean(y_fake))\n",
    "\n",
    "                self.optimizerD.zero_grad()\n",
    "                pen.backward(retain_graph=True)\n",
    "                loss_d.backward()\n",
    "                self.optimizerD.step()\n",
    "\n",
    "                fakez = torch.normal(mean=mean, std=std)\n",
    "                condvec = self.cond_generator.sample(self.batch_size)\n",
    "\n",
    "                if condvec is None:\n",
    "                    c1, m1, col, opt = None, None, None, None\n",
    "                else:\n",
    "                    c1, m1, col, opt = condvec\n",
    "                    c1 = torch.from_numpy(c1).to(self.device)\n",
    "                    m1 = torch.from_numpy(m1).to(self.device)\n",
    "                    fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "                fake = self.generator(fakez)\n",
    "                fakeact = self._apply_activate(fake)\n",
    "\n",
    "                if c1 is not None:\n",
    "                    y_fake = self.discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "                else:\n",
    "                    y_fake = self.discriminator(fakeact)\n",
    "\n",
    "                if condvec is None:\n",
    "                    cross_entropy = 0\n",
    "                else:\n",
    "                    cross_entropy = self._cond_loss(fake, c1, m1)\n",
    "\n",
    "                loss_g = -torch.mean(y_fake) + cross_entropy\n",
    "\n",
    "                self.optimizerG.zero_grad()\n",
    "                loss_g.backward()\n",
    "                self.optimizerG.step()\n",
    "\n",
    "            l_average = (loss_g.item() + loss_d.item()) / 2\n",
    "            print(\"Epoch %d, Loss Generator: %.4f, Loss Discriminator: %.4f\" %\n",
    "                  (self.trained_epoches, loss_g.detach().cpu(), loss_d.detach().cpu()), flush=True)\n",
    "\n",
    "    def sample(self, n, condition_column=None, condition_value=None):\n",
    "        if condition_column is not None and condition_value is not None:\n",
    "            condition_info = self.transformer.covert_column_name_value_to_id(\n",
    "                condition_column, condition_value)\n",
    "            global_condition_vec = self.cond_generator.generate_cond_from_condition_column_info(\n",
    "                condition_info, self.batch_size)\n",
    "        else:\n",
    "            global_condition_vec = None\n",
    "\n",
    "        steps = n // self.batch_size + 1\n",
    "        data = []\n",
    "        for i in range(steps):\n",
    "            mean = torch.zeros(self.batch_size, self.embedding_dim)\n",
    "            std = mean + 1\n",
    "            fakez = torch.normal(mean=mean, std=std).to(self.device)\n",
    "\n",
    "            if global_condition_vec is not None:\n",
    "                condvec = global_condition_vec.copy()\n",
    "            else:\n",
    "                condvec = self.cond_generator.sample_zero(self.batch_size)\n",
    "\n",
    "            if condvec is None:\n",
    "                pass\n",
    "            else:\n",
    "                c1 = condvec\n",
    "                c1 = torch.from_numpy(c1).to(self.device)\n",
    "                fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "            fake = self.generator(fakez)\n",
    "            fakeact = self._apply_activate(fake)\n",
    "            data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "        data = np.concatenate(data, axis=0)\n",
    "        data = data[:n]\n",
    "\n",
    "        return self.transformer.inverse_transform(data, None)\n",
    "\n",
    "    def save(self, path):\n",
    "        assert hasattr(self, \"generator\")\n",
    "        assert hasattr(self, \"discriminator\")\n",
    "        assert hasattr(self, \"transformer\")\n",
    "\n",
    "        device_bak = self.device\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.generator.to(self.device)\n",
    "        self.discriminator.to(self.device)\n",
    "\n",
    "        torch.save(self, path)\n",
    "\n",
    "        self.device = device_bak\n",
    "        self.generator.to(self.device)\n",
    "        self.discriminator.to(self.device)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        model = torch.load(path)\n",
    "        model.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.generator.to(model.device)\n",
    "        model.discriminator.to(model.device)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c882435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss Generator: 0.5926, Loss Discriminator: -0.0462\n",
      "Epoch 2, Loss Generator: 0.4152, Loss Discriminator: -0.0308\n",
      "Epoch 3, Loss Generator: 0.1879, Loss Discriminator: -0.0324\n",
      "Epoch 4, Loss Generator: 0.1707, Loss Discriminator: -0.0375\n",
      "Epoch 5, Loss Generator: 0.1892, Loss Discriminator: 0.0205\n",
      "Epoch 6, Loss Generator: 0.0555, Loss Discriminator: -0.1240\n",
      "Epoch 7, Loss Generator: 0.0510, Loss Discriminator: -0.0306\n",
      "Epoch 8, Loss Generator: -0.1899, Loss Discriminator: -0.0104\n",
      "Epoch 9, Loss Generator: -0.1805, Loss Discriminator: -0.1138\n",
      "Epoch 10, Loss Generator: -0.2340, Loss Discriminator: -0.0835\n",
      "Epoch 11, Loss Generator: -0.1612, Loss Discriminator: -0.0323\n",
      "Epoch 12, Loss Generator: -0.2830, Loss Discriminator: -0.0160\n",
      "Epoch 13, Loss Generator: -0.2712, Loss Discriminator: -0.0053\n",
      "Epoch 14, Loss Generator: -0.4481, Loss Discriminator: -0.0989\n",
      "Epoch 15, Loss Generator: -0.3588, Loss Discriminator: -0.0550\n",
      "Epoch 16, Loss Generator: -0.6495, Loss Discriminator: 0.0794\n",
      "Epoch 17, Loss Generator: -0.5984, Loss Discriminator: -0.0307\n",
      "Epoch 18, Loss Generator: -0.7074, Loss Discriminator: 0.1004\n",
      "Epoch 19, Loss Generator: -0.7321, Loss Discriminator: -0.0116\n",
      "Epoch 20, Loss Generator: -0.7684, Loss Discriminator: -0.1453\n",
      "Epoch 21, Loss Generator: -0.8152, Loss Discriminator: -0.0420\n",
      "Epoch 22, Loss Generator: -0.9866, Loss Discriminator: 0.0145\n",
      "Epoch 23, Loss Generator: -1.2059, Loss Discriminator: -0.1608\n",
      "Epoch 24, Loss Generator: -0.8409, Loss Discriminator: -0.0420\n",
      "Epoch 25, Loss Generator: -0.9005, Loss Discriminator: -0.1862\n",
      "Epoch 26, Loss Generator: -0.8693, Loss Discriminator: -0.1965\n",
      "Epoch 27, Loss Generator: -1.0107, Loss Discriminator: -0.0406\n",
      "Epoch 28, Loss Generator: -0.8526, Loss Discriminator: -0.0567\n",
      "Epoch 29, Loss Generator: -0.6492, Loss Discriminator: -0.0326\n",
      "Epoch 30, Loss Generator: -0.8944, Loss Discriminator: 0.0307\n",
      "Epoch 31, Loss Generator: -0.9725, Loss Discriminator: -0.0646\n",
      "Epoch 32, Loss Generator: -0.8760, Loss Discriminator: -0.0941\n",
      "Epoch 33, Loss Generator: -0.8241, Loss Discriminator: -0.0881\n",
      "Epoch 34, Loss Generator: -0.7457, Loss Discriminator: -0.0126\n",
      "Epoch 35, Loss Generator: -0.9436, Loss Discriminator: -0.0099\n",
      "Epoch 36, Loss Generator: -0.8386, Loss Discriminator: -0.0191\n",
      "Epoch 37, Loss Generator: -0.5710, Loss Discriminator: -0.0657\n",
      "Epoch 38, Loss Generator: -0.7867, Loss Discriminator: -0.0143\n",
      "Epoch 39, Loss Generator: -0.5607, Loss Discriminator: -0.1723\n",
      "Epoch 40, Loss Generator: -0.5389, Loss Discriminator: 0.0267\n",
      "Epoch 41, Loss Generator: -0.7695, Loss Discriminator: -0.1769\n",
      "Epoch 42, Loss Generator: -0.8558, Loss Discriminator: 0.0537\n",
      "Epoch 43, Loss Generator: -0.5924, Loss Discriminator: -0.2085\n",
      "Epoch 44, Loss Generator: -0.6600, Loss Discriminator: -0.0567\n",
      "Epoch 45, Loss Generator: -0.5512, Loss Discriminator: -0.0169\n",
      "Epoch 46, Loss Generator: -0.3916, Loss Discriminator: 0.1050\n",
      "Epoch 47, Loss Generator: -0.4324, Loss Discriminator: 0.0388\n",
      "Epoch 48, Loss Generator: -0.5195, Loss Discriminator: 0.1206\n",
      "Epoch 49, Loss Generator: -0.5045, Loss Discriminator: 0.0230\n",
      "Epoch 50, Loss Generator: -0.4153, Loss Discriminator: -0.1069\n",
      "Epoch 51, Loss Generator: -0.6770, Loss Discriminator: 0.0511\n",
      "Epoch 52, Loss Generator: -0.7314, Loss Discriminator: -0.0440\n",
      "Epoch 53, Loss Generator: -0.8865, Loss Discriminator: 0.1355\n",
      "Epoch 54, Loss Generator: -0.8439, Loss Discriminator: 0.0657\n",
      "Epoch 55, Loss Generator: -0.5780, Loss Discriminator: -0.0683\n",
      "Epoch 56, Loss Generator: -0.5330, Loss Discriminator: 0.0470\n",
      "Epoch 57, Loss Generator: -0.5916, Loss Discriminator: 0.1251\n",
      "Epoch 58, Loss Generator: -0.6139, Loss Discriminator: 0.1550\n",
      "Epoch 59, Loss Generator: -0.7306, Loss Discriminator: 0.0798\n",
      "Epoch 60, Loss Generator: -0.7794, Loss Discriminator: 0.2055\n",
      "Epoch 61, Loss Generator: -0.5153, Loss Discriminator: -0.0869\n",
      "Epoch 62, Loss Generator: -0.5741, Loss Discriminator: 0.0076\n",
      "Epoch 63, Loss Generator: -0.5833, Loss Discriminator: 0.0290\n",
      "Epoch 64, Loss Generator: -0.5446, Loss Discriminator: -0.1065\n",
      "Epoch 65, Loss Generator: -0.6735, Loss Discriminator: -0.1681\n",
      "Epoch 66, Loss Generator: -0.4501, Loss Discriminator: -0.2041\n",
      "Epoch 67, Loss Generator: -0.5957, Loss Discriminator: -0.2898\n",
      "Epoch 68, Loss Generator: -0.7170, Loss Discriminator: -0.0482\n",
      "Epoch 69, Loss Generator: -0.6701, Loss Discriminator: -0.1040\n",
      "Epoch 70, Loss Generator: -0.6546, Loss Discriminator: -0.1746\n",
      "Epoch 71, Loss Generator: -0.4677, Loss Discriminator: -0.0603\n",
      "Epoch 72, Loss Generator: -0.6017, Loss Discriminator: -0.0571\n",
      "Epoch 73, Loss Generator: -0.4991, Loss Discriminator: 0.1575\n",
      "Epoch 74, Loss Generator: -0.4482, Loss Discriminator: -0.1481\n",
      "Epoch 75, Loss Generator: -0.6032, Loss Discriminator: 0.1997\n",
      "Epoch 76, Loss Generator: -0.5941, Loss Discriminator: -0.0919\n",
      "Epoch 77, Loss Generator: -0.2129, Loss Discriminator: -0.0761\n",
      "Epoch 78, Loss Generator: -0.4968, Loss Discriminator: -0.3044\n",
      "Epoch 79, Loss Generator: -0.6414, Loss Discriminator: 0.0758\n",
      "Epoch 80, Loss Generator: -0.3879, Loss Discriminator: 0.0443\n",
      "Epoch 81, Loss Generator: -0.6004, Loss Discriminator: 0.0798\n",
      "Epoch 82, Loss Generator: -0.3102, Loss Discriminator: -0.0802\n",
      "Epoch 83, Loss Generator: -0.2617, Loss Discriminator: 0.1203\n",
      "Epoch 84, Loss Generator: -0.5303, Loss Discriminator: -0.1267\n",
      "Epoch 85, Loss Generator: -0.3528, Loss Discriminator: 0.2054\n",
      "Epoch 86, Loss Generator: -0.6659, Loss Discriminator: -0.1066\n",
      "Epoch 87, Loss Generator: -0.4805, Loss Discriminator: -0.1292\n",
      "Epoch 88, Loss Generator: -0.4906, Loss Discriminator: 0.0883\n",
      "Epoch 89, Loss Generator: -0.5529, Loss Discriminator: -0.2578\n",
      "Epoch 90, Loss Generator: -0.4529, Loss Discriminator: -0.0237\n",
      "Epoch 91, Loss Generator: -0.3051, Loss Discriminator: -0.0899\n",
      "Epoch 92, Loss Generator: 0.0116, Loss Discriminator: 0.1225\n",
      "Epoch 93, Loss Generator: -0.4540, Loss Discriminator: -0.0671\n",
      "Epoch 94, Loss Generator: -0.2394, Loss Discriminator: -0.0702\n",
      "Epoch 95, Loss Generator: -0.5051, Loss Discriminator: 0.1333\n",
      "Epoch 96, Loss Generator: -0.4712, Loss Discriminator: -0.1779\n",
      "Epoch 97, Loss Generator: -0.3962, Loss Discriminator: 0.1490\n",
      "Epoch 98, Loss Generator: -0.0437, Loss Discriminator: -0.0646\n",
      "Epoch 99, Loss Generator: -0.3770, Loss Discriminator: -0.2749\n",
      "Epoch 100, Loss Generator: -0.3044, Loss Discriminator: -0.0368\n"
     ]
    }
   ],
   "source": [
    "# Specifying columns from dataframe\n",
    "discrete_cols = df.columns.values\n",
    "# Defining PSGANS\n",
    "model = PSGANS()\n",
    "# Training PSGANS model\n",
    "model.fit(df, discrete_cols, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc91317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 1000 Fake Data\n",
    "samples = model.sample(1000) # Change the parameter to get desired number of fake data\n",
    "\n",
    "# Saving the data into a csv file\n",
    "df.to_csv('Fake_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "561f43f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [2, 3, 4, 5, 6]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for \"1 0 1\" combination in column 2, 3, 4 respectively\n",
    "samples.loc[(samples['2'] == 1) & (samples['3'] == 0) & (samples['4'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2a3f2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [2, 3, 4, 5, 6]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for \"0 0 1\" combination in column 2, 3, 4 respectively\n",
    "samples.loc[(samples['2'] == 0) & (samples['3'] == 0) & (samples['4'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8213316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Data Distribution in column 2: \n",
      "0    522\n",
      "1    473\n",
      "Name: 2, dtype: int64\n",
      "\n",
      "Fake Data Distribution in column 3: \n",
      "1    656\n",
      "0    339\n",
      "Name: 3, dtype: int64\n",
      "\n",
      "Fake Data Distribution in column 4: \n",
      "0    885\n",
      "1    110\n",
      "Name: 4, dtype: int64\n",
      "\n",
      "Fake Data Distribution in column 5: \n",
      "1     127\n",
      "3     116\n",
      "5     108\n",
      "0      96\n",
      "6      95\n",
      "2      94\n",
      "4      79\n",
      "7      59\n",
      "8      54\n",
      "11     43\n",
      "9      39\n",
      "10     35\n",
      "12     20\n",
      "13     13\n",
      "14     12\n",
      "15      3\n",
      "19      1\n",
      "16      1\n",
      "Name: 5, dtype: int64\n",
      "\n",
      "Fake Data Distribution in column 6: \n",
      "1    995\n",
      "Name: 6, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Fake Data Distribution in column 2: \")\n",
    "print(samples['2'].value_counts())\n",
    "print(\"\\nFake Data Distribution in column 3: \")\n",
    "print(samples['3'].value_counts())\n",
    "print(\"\\nFake Data Distribution in column 4: \")\n",
    "print(samples['4'].value_counts())\n",
    "print(\"\\nFake Data Distribution in column 5: \")\n",
    "print(samples['5'].value_counts())\n",
    "print(\"\\nFake Data Distribution in column 6: \")\n",
    "print(samples['6'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce8a7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Distribution in column 2: \n",
      "0    16978\n",
      "1    16892\n",
      "Name: 2, dtype: int64\n",
      "\n",
      "Data Distribution in column 3: \n",
      "1    18565\n",
      "0    15305\n",
      "Name: 3, dtype: int64\n",
      "\n",
      "Data Distribution in column 4: \n",
      "0    32197\n",
      "1     1673\n",
      "Name: 4, dtype: int64\n",
      "\n",
      "Data Distribution in column 5: \n",
      "1     5521\n",
      "2     5194\n",
      "3     3979\n",
      "4     3129\n",
      "5     2589\n",
      "6     2483\n",
      "7     2149\n",
      "8     1907\n",
      "0     1673\n",
      "9     1553\n",
      "10    1155\n",
      "11     942\n",
      "12     619\n",
      "13     433\n",
      "14     247\n",
      "15     147\n",
      "16      78\n",
      "17      54\n",
      "18      16\n",
      "19       2\n",
      "Name: 5, dtype: int64\n",
      "\n",
      "Data Distribution in column 6: \n",
      "1    33870\n",
      "Name: 6, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Distribution in column 2: \")\n",
    "print(df['2'].value_counts())\n",
    "print(\"\\nData Distribution in column 3: \")\n",
    "print(df['3'].value_counts())\n",
    "print(\"\\nData Distribution in column 4: \")\n",
    "print(df['4'].value_counts())\n",
    "print(\"\\nData Distribution in column 5: \")\n",
    "print(df['5'].value_counts())\n",
    "print(\"\\nData Distribution in column 6: \")\n",
    "print(df['6'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c2196e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 2 (Buy): Fake Ratio:  0.5246231155778894 Real Ratio:  0.501269560082669\n",
      "Column 3 (New (not cancel)): Fake Ratio:  0.3407035175879397 Real Ratio:  0.45187481547091823\n",
      "Column 4 (Limit): Fake Ratio:  0.8894472361809045 Real Ratio:  0.9506052553882491\n"
     ]
    }
   ],
   "source": [
    "fakeCol2 = samples['2'].value_counts()\n",
    "fakeCol3 = samples['3'].value_counts()\n",
    "fakeCol4 = samples['4'].value_counts()\n",
    "\n",
    "realCol2 = df['2'].value_counts()\n",
    "realCol3 = df['3'].value_counts()\n",
    "realCol4 = df['4'].value_counts()\n",
    "\n",
    "fakeCol2Ratio = fakeCol2[0] / ( fakeCol2[0] + fakeCol2[1]  )\n",
    "fakeCol3Ratio = fakeCol3[0] / ( fakeCol3[0] + fakeCol3[1]  )\n",
    "fakeCol4Ratio = fakeCol4[0] / ( fakeCol4[0] + fakeCol4[1]  )\n",
    "\n",
    "realCol2Ratio = realCol2[0] / ( realCol2[0] + realCol2[1]  )\n",
    "realCol3Ratio = realCol3[0] / ( realCol3[0] + realCol3[1]  )\n",
    "realCol4Ratio = realCol4[0] / ( realCol4[0] + realCol4[1]  )\n",
    "\n",
    "\n",
    "print(\"Column 2 (Buy): Fake Ratio: \", fakeCol2Ratio, \"Real Ratio: \", realCol2Ratio)\n",
    "print(\"Column 3 (New (not cancel)): Fake Ratio: \", fakeCol3Ratio, \"Real Ratio: \", realCol3Ratio)\n",
    "print(\"Column 4 (Limit): Fake Ratio: \", fakeCol4Ratio, \"Real Ratio: \", realCol4Ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
