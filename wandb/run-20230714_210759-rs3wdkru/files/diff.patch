diff --git a/gymnax_exchange/jaxrl/ppoRnnExecCont.py b/gymnax_exchange/jaxrl/ppoRnnExecCont.py
index 18b110a..cca3716 100644
--- a/gymnax_exchange/jaxrl/ppoRnnExecCont.py
+++ b/gymnax_exchange/jaxrl/ppoRnnExecCont.py
@@ -220,6 +220,7 @@ def make_train(config):
             runner_state, traj_batch = jax.lax.scan(
                 _env_step, runner_state, None, config["NUM_STEPS"]
             )
+            jax.debug.breakpoint()
 
             # CALCULATE ADVANTAGE
             train_state, env_state, last_obs, last_done, hstate, rng = runner_state
@@ -370,15 +371,17 @@ def make_train(config):
                     return_values = info["returned_episode_returns"][
                         info["returned_episode"]
                     ]
+                    
                     timesteps = (
                         info["timestep"][info["returned_episode"]] * config["NUM_ENVS"]
                     )
-                    
+                    jax.debug.breakpoint()
                     revenues = info["total_revenue"][info["returned_episode"]]
                     quant_executed = info["quant_executed"][info["returned_episode"]]
 
                     
                     for t in range(len(timesteps)):
+                        print(info["returned_episode_returns"])
                         # print(
                         # f"global step={timesteps[t]}, episodic return={return_values[t]}, episodic revenue={revenues[t]}"
                         # # f"global step={timesteps[t]}, episodic return={return_values[t]}"
@@ -427,8 +430,8 @@ if __name__ == "__main__":
     ppo_config = {
         "LR": 2.5e-4,
         #"NUM_ENVS": 1,
-        "NUM_ENVS": 1000,
-        "NUM_STEPS": 10,
+        "NUM_ENVS": 12,
+        "NUM_STEPS": 4,
         "TOTAL_TIMESTEPS": 1e7,
         "UPDATE_EPOCHS": 4,
         #"NUM_MINIBATCHES": 1,
