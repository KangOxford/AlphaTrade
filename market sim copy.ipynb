{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "221709f7",
   "metadata": {},
   "source": [
    "# MARKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2039a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TradingBatched:\n",
    "    def __init__(self, batch_size, num_assets, num_agents, num_steps):\n",
    "        self.b = batch_size\n",
    "        self.num_assets = num_assets\n",
    "        self.num_agents = num_agents\n",
    "        self.num_steps  = num_steps\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.true_values = 5.0 * torch.normal(mean=0, std=1, size=(self.b, self.num_assets)).to(device) # N(0, 1)\n",
    "        self.stds = 3.0 * torch.sigmoid(torch.normal(mean=0, std=1, size=(self.b, self.num_agents, self.num_assets))).to(device) # N(0, 1)\n",
    "        self.estimates = torch.normal(mean=self.true_values.unsqueeze(1), std=self.stds).to(device)\n",
    "        \n",
    "        self.cash = torch.zeros(self.b, self.num_agents).to(device)\n",
    "        self.cur_alloc = torch.ones(self.b, self.num_agents, self.num_assets).to(device) * 10000 # everyone just starts w/ one of everythign\n",
    "        \n",
    "        self.order_book = torch.zeros(self.b, self.num_agents, self.num_assets, 4).to(device) # TODO: Attention over assets?\n",
    "        self.t = 0\n",
    "        \n",
    "        return self._naive_states()\n",
    "    \n",
    "    def _naive_states(self):\n",
    "        states = []\n",
    "        for i in range(self.num_agents):\n",
    "#             order_book = self.order_book.clone()\n",
    "            \n",
    "            # TODO: DO THIS CORRECTLY?\n",
    "#             order_book[:,0], order_book[:,i] = order_book[:,i], order_book[:,0] # AGENT ALWAYS FIRST\n",
    "            states.append(torch.cat([\n",
    "                self.order_book.flatten(start_dim=1),\n",
    "                self.cash[:,i].unsqueeze(-1),\n",
    "                self.cur_alloc[:,i],\n",
    "                self.estimates[:,i],\n",
    "                self.stds[:,i],\n",
    "            ], dim=-1))\n",
    "\n",
    "        return states\n",
    "    \n",
    "    def step(self, actions_bna4, debug=False):\n",
    "        \n",
    "        self.order_book = actions_bna4.clone()\n",
    "        \n",
    "        # PRIORIY BASED ON BIDDING\n",
    "        curr_order_book = actions_bna4.clone()\n",
    "        \n",
    "        # CANT SELL MORE THAN YOU HAVE. I DONT CARE IF YOU GET MORE OVER THE COURSE OF TRADING.\n",
    "        curr_order_book[:,:,:,3] = torch.min(curr_order_book[:,:,:,3], self.cur_alloc)\n",
    "        \n",
    "        info = []\n",
    "#         agent_indices = torch.arange(self.num_agents).repeat_interleave(self.num_assets).view(1,self.num_agents,self.num_assets,1).repeat(self.b,1,1,1).to(device)\n",
    "#         curr_order_book = torch.cat([curr_order_book, agent_indices], dim=-1)\n",
    "        \n",
    "\n",
    "#         curr_order_book = torch.cat([curr_order_book, torch.arange(self.num_agents).repeat_interleave(self.num_assets)[None,None,None,:]], dim=-1)\n",
    "#         bids_indices_sorted = torch.argsort(curr_order_book[])\n",
    "        \n",
    "        while True:\n",
    "            # REMOVE EMPTY VOLUME. \n",
    "            # REMOVE SELF TRADES\n",
    "            \n",
    "            # LATER THIS SHOULDNT BE NEEDED\n",
    "            curr_order_book[:,:,:,2:] = torch.round(curr_order_book[:,:,:,2:])\n",
    "\n",
    "            # TODO: THIS DOESNT WORK. COULD WE DO IT A BETTER WAY?\n",
    "#             curr_order_book[curr_order_book[:,:,:,2] <= 0][:,0] = torch.FloatTensor([float(\"Inf\")]).to(device) \n",
    "#             curr_order_book[curr_order_book[:,:,:,3] <= 0][:,1] = torch.FloatTensor([float(\"Inf\")]).to(device)\n",
    "\n",
    "            # WORKS BUT IS PITA\n",
    "#             curr_order_book[curr_order_book[:,:,:,2] <= 0] = torch.cat([-float(\"Inf\") + torch.zeros_like(curr_order_book[curr_order_book[:,:,:,2] <= 0][:,0:1]), curr_order_book[curr_order_book[:,:,:,2] <= 0][:,1:]],dim=-1)\n",
    "\n",
    "\n",
    "            tmp = curr_order_book[curr_order_book[:,:,:,2] <= 0]\n",
    "            tmp[:,0] = -float(\"Inf\")\n",
    "            curr_order_book[curr_order_book[:,:,:,2] <= 0] = tmp\n",
    "            \n",
    "            tmp = curr_order_book[curr_order_book[:,:,:,3] <= 0]\n",
    "            tmp[:,1] = float(\"Inf\")\n",
    "            curr_order_book[curr_order_book[:,:,:,3] <= 0] = tmp\n",
    "\n",
    "#             can trade with self i guess\n",
    "#             curr_order_book[curr_order_book[:,:,:,0] > curr_order_book[:,:,:,1]] = torch.FloatTensor([-float(\"Inf\"), float(\"Inf\"), 0, 0]).to(device)\n",
    "\n",
    "            max_bids = torch.max(curr_order_book[:,:,:,0], dim=1)\n",
    "            highest_bidders, highest_bids_ba = max_bids.indices, max_bids.values\n",
    "            \n",
    "            h_ind = highest_bidders.unsqueeze(1).unsqueeze(-1).repeat(1,1,1,4)\n",
    "            highest_row = torch.gather(curr_order_book, 1, h_ind).squeeze(1)\n",
    "        \n",
    "            min_asks = torch.min(curr_order_book[:,:,:,1], dim=1)\n",
    "            lowest_askers, lowest_asks_ba = min_asks.indices, min_asks.values\n",
    "\n",
    "            l_ind = lowest_askers.unsqueeze(1).unsqueeze(-1).repeat(1,1,1,4)\n",
    "            lowest_row = torch.gather(curr_order_book, 1, l_ind).squeeze(1)\n",
    "            \n",
    "            # Break if all lowest asks are higher than all highest bids across all batches and assets\n",
    "            if torch.all(highest_bids_ba < lowest_asks_ba) or torch.all(lowest_row[:,:,3] <= 0) or torch.all(highest_row[:,:,2] <= 0):\n",
    "                break\n",
    "            \n",
    "            num_trades = torch.where(\n",
    "                lowest_asks_ba <= highest_bids_ba, \n",
    "                torch.min(highest_row[:,:,2], lowest_row[:,:,3]),\n",
    "                torch.zeros_like(highest_row[:,:,2])\n",
    "            )\n",
    "\n",
    "            \n",
    "            # TODO: CLEAN. I DONT EVEN KNOW IF THIS IS CORRECT.\n",
    "            # APPARENTLY BUYER BUYS AT OWN PRICE\n",
    "#             cash_diff = torch.nan_to_num(num_trades * lowest_asks_ba).unsqueeze(-1)\n",
    "            cash_diff = torch.nan_to_num(num_trades * highest_bids_ba).unsqueeze(-1)\n",
    "\n",
    "            highest_cash_diff = torch.zeros_like(self.cash.unsqueeze(1).repeat(1,self.num_assets,1)).scatter_(-1,highest_bidders.unsqueeze(-1),cash_diff).sum(1)\n",
    "            lowest_cash_diff = torch.zeros_like(self.cash.unsqueeze(1).repeat(1,self.num_assets,1)).scatter_(-1,lowest_askers.unsqueeze(-1),cash_diff).sum(1)\n",
    "            self.cash -= highest_cash_diff\n",
    "            self.cash += lowest_cash_diff\n",
    "            \n",
    "            # TODO: CLEAN\n",
    "            highest_alloc_diff = torch.zeros_like(self.cur_alloc).scatter_(1, highest_bidders.unsqueeze(1).repeat(1,self.num_agents,1), num_trades.unsqueeze(1).repeat(1,self.num_agents,1))\n",
    "            lowest_alloc_diff = torch.zeros_like(self.cur_alloc).scatter_(1, lowest_askers.unsqueeze(1).repeat(1,self.num_agents,1), num_trades.unsqueeze(1).repeat(1,self.num_agents,1))\n",
    "            self.cur_alloc += highest_alloc_diff\n",
    "            self.cur_alloc -= lowest_alloc_diff\n",
    "            \n",
    "            \n",
    "            curr_order_book[:,:,:,2].scatter_(1, h_ind[:,:,:,0], -num_trades.unsqueeze(1), reduce='add')\n",
    "            curr_order_book[:,:,:,3].scatter_(1, l_ind[:,:,:,0], -num_trades.unsqueeze(1), reduce='add')\n",
    "            if debug:\n",
    "                info.append(\n",
    "                    {\n",
    "                        \"num_trades\": num_trades,\n",
    "                        \"lowest_asks\": lowest_asks_ba,\n",
    "                        \"cash_diff\": cash_diff,\n",
    "                        \"highest_alloc_diff\": highest_alloc_diff,\n",
    "                        \"lowest_alloc_diff\": lowest_alloc_diff,\n",
    "                        \"curr_order_book\": curr_order_book.clone()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        self.t += 1\n",
    "        if self.t == self.num_steps:\n",
    "            reward = (self.true_values.unsqueeze(1) * self.cur_alloc).sum(dim=-1) + self.cash\n",
    "        else:\n",
    "            reward = torch.zeros(self.b, self.num_agents).to(device)\n",
    "        \n",
    "        return self._naive_states(), reward, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "151bfa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TradingBatched(3, 2, 2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6666bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]], device='cuda:0')\n",
      "tensor([[ 103998.7266,  104000.4141],\n",
      "        [-252608.1562, -252647.0938],\n",
      "        [ 115266.1484,  115228.2422]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "asdf = env.reset()\n",
    "for _ in range(8):\n",
    "    order_book = torch.cat([torch.normal(mean=0.0,std=1.0,size=(3,2,2,2)), torch.randint(0,5,[3,2,2,2])], dim=-1).to(device)\n",
    "    _, r, _ = env.step(order_book)\n",
    "    print(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9661e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import OneHotCategorical\n",
    "import numpy as np\n",
    "\n",
    "from torch.distributions import MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std, initialization=False):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        # action mean range -1 to 1\n",
    "        self.actor =  nn.Sequential(\n",
    "                nn.Linear(state_dim, 256),\n",
    "                nn.Tanh(),\n",
    "#                 nn.Linear(128, 128),\n",
    "#                 nn.Tanh(),\n",
    "                nn.Linear(256, action_dim * 2),\n",
    "# #                 nn.Tanh()\n",
    "                )\n",
    "        \n",
    "        \n",
    "        \n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(state_dim, 256),\n",
    "                nn.Tanh(),\n",
    "#                 nn.Linear(128, 128),\n",
    "#                 nn.Tanh(),\n",
    "                nn.Linear(256, 1),\n",
    "                )\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        self.init_var = torch.full((action_dim,), action_std*action_std).to(device)\n",
    "#         self.init_std = action_std\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state, memory):\n",
    "        if state.sum() < 0:\n",
    "#             if self.initialization:\n",
    "#                 action_mean = self.initial_policy.repeat(state.shape[0], 1)\n",
    "#             else:\n",
    "            action_mean = torch.zeros(state.shape[0], self.action_dim).to(device)\n",
    "            cov_mat = torch.diag(self.init_var)\n",
    "        else:\n",
    "#             import pdb; pdb.set_trace()\n",
    "            action_mean, action_var = torch.split(self.actor(state), action_dim, dim=-1)\n",
    "            cov_mat = torch.diag_embed(F.softplus(action_var))\n",
    "\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "\n",
    "        memory.states.append(state)\n",
    "        memory.actions.append(action)\n",
    "        memory.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.detach()\n",
    "\n",
    "    def evaluate(self, state, action):         \n",
    "        action_mean, action_var = torch.split(self.actor(state), action_dim, dim=-1)\n",
    "        action_var = F.softplus(action_var)\n",
    "            \n",
    "        # NO GRADIENT FOR THE INITIAL STATE\n",
    "        action_mean = torch.where(state.sum((1,2),keepdim=True) < 0, torch.zeros_like(action_mean), action_mean)\n",
    "        action_var = torch.where(state.sum((1,2),keepdim=True) < 0, torch.ones_like(action_var) * 0.5, action_var)\n",
    "        \n",
    "#         action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var).to(device)\n",
    "\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "\n",
    "        state_value = self.critic(state)\n",
    "        state_value = torch.where(state.sum((1,2),keepdim=True) < 0, torch.zeros_like(state_value), state_value)\n",
    "\n",
    "        return action_logprobs, torch.squeeze(state_value), dist_entropy\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip):\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr, betas=betas)\n",
    "\n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state, memory):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.policy_old.act(state, memory).cpu().data.numpy().flatten()\n",
    "\n",
    "    def update(self, memory):\n",
    "        # Monte Carlo estimate of rewards:\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward in reversed(memory.rewards):\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards:\n",
    "#         rewards = torch.stack(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = torch.stack(rewards).squeeze(-1)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.stack(memory.states).detach()\n",
    "        old_actions = torch.stack(memory.actions).detach()\n",
    "        old_logprobs = torch.stack(memory.logprobs).detach()\n",
    "\n",
    "        \n",
    "        # Optimize policy for K epochs:\n",
    "        for _ in range(self.K_epochs):\n",
    "            # Evaluating old actions and values :\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old):\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss:\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy:\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c958bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 (0.9, 0.999)\n",
      "====================================================================================================\n",
      "p0 reward: 69.67228698730469\n",
      "====================================================================================================\n",
      "p0 reward: 638.2947998046875\n",
      "====================================================================================================\n",
      "p0 reward: 525.2352905273438\n",
      "====================================================================================================\n",
      "p0 reward: 37.2098388671875\n",
      "====================================================================================================\n",
      "p0 reward: -91.80204010009766\n",
      "====================================================================================================\n",
      "p0 reward: -556.6376953125\n",
      "====================================================================================================\n",
      "p0 reward: -245.86444091796875\n",
      "====================================================================================================\n",
      "p0 reward: -345.2394714355469\n",
      "====================================================================================================\n",
      "p0 reward: -456.9969482421875\n",
      "====================================================================================================\n",
      "p0 reward: -321.96588134765625\n",
      "====================================================================================================\n",
      "p0 reward: 82.23880767822266\n",
      "====================================================================================================\n",
      "p0 reward: 97.52543640136719\n",
      "====================================================================================================\n",
      "p0 reward: 471.89752197265625\n",
      "====================================================================================================\n",
      "p0 reward: -174.44322204589844\n",
      "====================================================================================================\n",
      "p0 reward: 345.3282775878906\n",
      "====================================================================================================\n",
      "p0 reward: 118.52265930175781\n",
      "====================================================================================================\n",
      "p0 reward: -156.521240234375\n",
      "====================================================================================================\n",
      "p0 reward: -759.7158813476562\n",
      "====================================================================================================\n",
      "p0 reward: -235.6131134033203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1333155/2095127547.py\", line 61, in <module>\n",
      "    ppo[0].update(memory[0])\n",
      "  File \"/tmp/ipykernel_1333155/1372387200.py\", line 153, in update\n",
      "    loss.mean().backward()\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3444, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1333155/2095127547.py\", line 61, in <module>\n",
      "    ppo[0].update(memory[0])\n",
      "  File \"/tmp/ipykernel_1333155/1372387200.py\", line 153, in update\n",
      "    loss.mean().backward()\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3364, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3461, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2066, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2064, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "\n",
    "num_assets=2\n",
    "num_agents=2\n",
    "num_steps=8\n",
    "\n",
    "state_dim = num_assets * num_agents * 4 + num_assets * 3 + 1\n",
    "# FLATTENED ORDER BOOK + CURRENT ALLOC/ESTIAMTE/VARIANCE + CASH\n",
    "\n",
    "action_dim = num_assets * 4\n",
    "# ORDER BOOK\n",
    "\n",
    "action_std = 0.5 # constant std for action distribution (Multivariate Normal)\n",
    "# K_epochs = 80               # update policy for K epochs\n",
    "K_epochs = 4               # update policy for K epochs\n",
    "\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.95                # discount factor\n",
    "\n",
    "lr = 0.0002                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "\n",
    "max_episodes = 10000\n",
    "batch_size = 50000\n",
    "random_seed = None\n",
    "# num_steps = 100\n",
    "#############################################\n",
    "\n",
    "# creating environment\n",
    "env = TradingBatched(batch_size, num_assets, num_agents, num_steps)\n",
    "\n",
    "memory = [Memory() for _ in range(num_agents)]\n",
    "ppo = [PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip) for _ in range(num_agents)]\n",
    "\n",
    "print(lr,betas)\n",
    "\n",
    "# training loop\n",
    "\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    \n",
    "    running_rewards = torch.zeros(batch_size, num_agents).to(device)\n",
    "    \n",
    "    last_reward = 0\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "\n",
    "        # Running policy_old:\n",
    "        asdf = []\n",
    "        for i in range(num_agents):\n",
    "            asdf.append(ppo[i].policy_old.act(state[i], memory[i]))\n",
    "        \n",
    "        actions = torch.stack([torch.stack(a.split(2, dim=-1),dim=-1) for a in asdf], dim=1)\n",
    "        state, reward, _ = env.step(actions)\n",
    "        running_rewards += reward\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            memory[i].rewards.append(reward[:,i])\n",
    "\n",
    "    ppo[0].update(memory[0])\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        memory[i].clear_memory()\n",
    "        \n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    print(f\"p0 reward: {running_rewards[:,0].mean()}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e9bd2055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 1.], device='cuda:0')\n",
      "tensor([11.5655, -0.6388,  5.8381,  2.0847], device='cuda:0')\n",
      "tensor([[[ 0.6745,  2.2489,  2.6252, -3.0702],\n",
      "         [ 0.3061,  0.6118,  3.1051,  1.7450]],\n",
      "\n",
      "        [[ 1.1132,  0.0641,  0.8648,  0.8373],\n",
      "         [ 0.4544,  0.8206, -0.6243, -0.1486]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "b = 9348\n",
    "print(state[0][b,-6:-4])\n",
    "print(state[0][b,-4:])\n",
    "print(actions[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52db2d5",
   "metadata": {},
   "source": [
    "# FIGGIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2641fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FiggieBatched:\n",
    "    def __init__(self, batch_size, num_steps):\n",
    "        self.b = batch_size\n",
    "        self.num_assets = 4\n",
    "        self.num_agents = 4\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "    def reset(self):\n",
    "        self.suit_shuff = torch.argsort(torch.rand((self.b,4)), dim=-1).to(device)\n",
    "        cards = torch.cat([\n",
    "            self.suit_shuff[:,0:1] * torch.ones((self.b, 12)).to(device),\n",
    "            self.suit_shuff[:,1:2] * torch.ones((self.b, 8)).to(device),\n",
    "            self.suit_shuff[:,2:3] * torch.ones((self.b, 10)).to(device),\n",
    "            self.suit_shuff[:,3:4] * torch.ones((self.b, 10)).to(device),\n",
    "        ], dim=-1)\n",
    "        cards = cards[torch.arange(self.b).unsqueeze(-1), torch.argsort(torch.rand((self.b,40)), dim=-1)]\n",
    "        hands = cards.split(10,dim=-1)\n",
    "        self.cur_alloc = torch.stack([\n",
    "             torch.stack([\n",
    "                (hands[j]==i).sum(dim=-1)        \n",
    "            for j in range(4)])\n",
    "        for i in range(4)]).swapaxes(0,2).to(device).float() # TODO: THIS SHOULDNT BE FLOAT\n",
    "        self.goal_suit = (self.suit_shuff[:,0] + 2) % 4\n",
    "        self.bonus = torch.where(self.goal_suit == self.suit_shuff[:,1], 120.0, 100.0).to(device)\n",
    "        \n",
    "        self.cash = torch.zeros(self.b, self.num_agents).to(device)\n",
    "#         self.cur_alloc = torch.ones(self.b, self.num_agents, self.num_assets).to(device) # everyone just starts w/ one of everythign\n",
    "        \n",
    "        self.order_book = torch.zeros(self.b, self.num_agents, self.num_assets, 4).to(device) # TODO: Attention over assets?\n",
    "        self.t = 0\n",
    "        \n",
    "        return self._naive_states()\n",
    "    \n",
    "    def _naive_states(self):\n",
    "        states = []\n",
    "        for i in range(self.num_agents):\n",
    "            states.append(torch.cat([\n",
    "                self.order_book.flatten(start_dim=1),\n",
    "                self.cash[:,i].unsqueeze(-1),\n",
    "                self.cur_alloc[:,i],\n",
    "            ], dim=-1))\n",
    "\n",
    "        return states\n",
    "    \n",
    "    def step(self, actions_bna4, debug=False):\n",
    "        \n",
    "        self.order_book = actions_bna4.clone()\n",
    "        \n",
    "        # PRIORIY BASED ON BIDDING\n",
    "        curr_order_book = actions_bna4.clone()\n",
    "        \n",
    "        # CANT SELL MORE THAN YOU HAVE. I DONT CARE IF YOU GET MORE OVER THE COURSE OF TRADING.\n",
    "        curr_order_book[:,:,:,3] = torch.min(curr_order_book[:,:,:,3], self.cur_alloc)\n",
    "        \n",
    "        info = []\n",
    "        \n",
    "        while True:\n",
    "            # REMOVE EMPTY VOLUME. \n",
    "            # REMOVE SELF TRADES\n",
    "            \n",
    "            \n",
    "            # LATER THIS SHOULDNT BE NEEDED\n",
    "            curr_order_book[:,:,:,2:] = torch.round(curr_order_book[:,:,:,2:])\n",
    "\n",
    "            # TODO: THIS DOESNT WORK. COULD WE DO IT A BETTER WAY?\n",
    "#             curr_order_book[curr_order_book[:,:,:,2] <= 0][:,0] = torch.FloatTensor([float(\"Inf\")]).to(device) \n",
    "#             curr_order_book[curr_order_book[:,:,:,3] <= 0][:,1] = torch.FloatTensor([float(\"Inf\")]).to(device)\n",
    "\n",
    "            # WORKS BUT IS PITA\n",
    "#             curr_order_book[curr_order_book[:,:,:,2] <= 0] = torch.cat([-float(\"Inf\") + torch.zeros_like(curr_order_book[curr_order_book[:,:,:,2] <= 0][:,0:1]), curr_order_book[curr_order_book[:,:,:,2] <= 0][:,1:]],dim=-1)\n",
    "\n",
    "\n",
    "            tmp = curr_order_book[curr_order_book[:,:,:,2] <= 0]\n",
    "            tmp[:,0] = -float(\"Inf\")\n",
    "            curr_order_book[curr_order_book[:,:,:,2] <= 0] = tmp\n",
    "            \n",
    "            tmp = curr_order_book[curr_order_book[:,:,:,3] <= 0]\n",
    "            tmp[:,1] = float(\"Inf\")\n",
    "            curr_order_book[curr_order_book[:,:,:,3] <= 0] = tmp\n",
    "\n",
    "#             can trade with self i guess\n",
    "#             curr_order_book[curr_order_book[:,:,:,0] > curr_order_book[:,:,:,1]] = torch.FloatTensor([-float(\"Inf\"), float(\"Inf\"), 0, 0]).to(device)\n",
    "\n",
    "            max_bids = torch.max(curr_order_book[:,:,:,0], dim=1)\n",
    "            highest_bidders, highest_bids_ba = max_bids.indices, max_bids.values\n",
    "            \n",
    "            h_ind = highest_bidders.unsqueeze(1).unsqueeze(-1).repeat(1,1,1,4)\n",
    "            highest_row = torch.gather(curr_order_book, 1, h_ind).squeeze(1)\n",
    "        \n",
    "            min_asks = torch.min(curr_order_book[:,:,:,1], dim=1)\n",
    "            lowest_askers, lowest_asks_ba = min_asks.indices, min_asks.values\n",
    "\n",
    "            l_ind = lowest_askers.unsqueeze(1).unsqueeze(-1).repeat(1,1,1,4)\n",
    "            lowest_row = torch.gather(curr_order_book, 1, l_ind).squeeze(1)\n",
    "            \n",
    "            # Break if all lowest asks are higher than all highest bids across all batches and assets\n",
    "            if torch.all(highest_bids_ba < lowest_asks_ba) or torch.all(lowest_row[:,:,3] <= 0) or torch.all(highest_row[:,:,2] <= 0):\n",
    "                break\n",
    "            \n",
    "            num_trades = torch.where(\n",
    "                lowest_asks_ba < highest_bids_ba, \n",
    "                torch.min(highest_row[:,:,2], lowest_row[:,:,3]),\n",
    "                torch.zeros_like(highest_row[:,:,2])\n",
    "            )\n",
    "\n",
    "            \n",
    "            # TODO: CLEAN. I DONT EVEN KNOW IF THIS IS CORRECT.\n",
    "            # APPARENTLY BUYER BUYS AT OWN PRICE\n",
    "#             cash_diff = torch.nan_to_num(num_trades * lowest_asks_ba).unsqueeze(-1)\n",
    "            cash_diff = torch.nan_to_num(num_trades * highest_bids_ba).unsqueeze(-1)\n",
    "\n",
    "            highest_cash_diff = torch.zeros_like(self.cash.unsqueeze(1).repeat(1,self.num_assets,1)).scatter_(-1,highest_bidders.unsqueeze(-1),cash_diff).sum(1)\n",
    "            lowest_cash_diff = torch.zeros_like(self.cash.unsqueeze(1).repeat(1,self.num_assets,1)).scatter_(-1,lowest_askers.unsqueeze(-1),cash_diff).sum(1)\n",
    "            self.cash -= highest_cash_diff\n",
    "            self.cash += lowest_cash_diff\n",
    "            \n",
    "            # TODO: CLEAN\n",
    "            highest_alloc_diff = torch.zeros_like(self.cur_alloc).scatter_(1, highest_bidders.unsqueeze(1).repeat(1,self.num_agents,1), num_trades.unsqueeze(1).repeat(1,self.num_agents,1))\n",
    "            lowest_alloc_diff = torch.zeros_like(self.cur_alloc).scatter_(1, lowest_askers.unsqueeze(1).repeat(1,self.num_agents,1), num_trades.unsqueeze(1).repeat(1,self.num_agents,1))\n",
    "            self.cur_alloc += highest_alloc_diff\n",
    "            self.cur_alloc -= lowest_alloc_diff\n",
    "            \n",
    "            \n",
    "            curr_order_book[:,:,:,2].scatter_(1, h_ind[:,:,:,0], -num_trades.unsqueeze(1), reduce='add')\n",
    "            curr_order_book[:,:,:,3].scatter_(1, l_ind[:,:,:,0], -num_trades.unsqueeze(1), reduce='add')\n",
    "\n",
    "            if debug:\n",
    "                info.append(\n",
    "                    {\n",
    "                        \"num_trades\": num_trades,\n",
    "                        \"lowest_asks\": lowest_asks_ba,\n",
    "                        \"cash_diff\": cash_diff,\n",
    "                        \"highest_alloc_diff\": highest_alloc_diff,\n",
    "                        \"lowest_alloc_diff\": lowest_alloc_diff,\n",
    "                        \"curr_order_book\": curr_order_book.clone()\n",
    "                    }\n",
    "                )\n",
    "        self.t += 1\n",
    "        if self.t == self.num_steps:\n",
    "#             reward = self.cur_alloc[:,self.goal_suit] * 10.0 + self.cash - 50.0\n",
    "            num_goal_cards = torch.gather(self.cur_alloc,-1, self.goal_suit.unsqueeze(-1).unsqueeze(-1).repeat(1,4,1)).squeeze(-1)\n",
    "            reward = num_goal_cards * 10.0 + self.cash - 50.0\n",
    "            maxvals, _ = torch.max(num_goal_cards, dim=-1)\n",
    "            reward = reward + torch.where(\n",
    "                num_goal_cards == maxvals.unsqueeze(-1),\n",
    "                self.bonus.unsqueeze(-1) / (num_goal_cards == maxvals.unsqueeze(-1)).sum(dim=-1).unsqueeze(-1),\n",
    "                torch.zeros_like(num_goal_cards)\n",
    "            )\n",
    "#             bonus = \n",
    "#             reward = (self.true_values.unsqueeze(1) * self.cur_alloc).sum(dim=-1) + self.cash - 50.0\n",
    "        else:\n",
    "            reward = torch.zeros(self.b, self.num_agents).to(device)\n",
    "        \n",
    "        return self._naive_states(), reward, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "29d71aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FiggieBatched(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7719d02d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-40.5816, 123.9113, -41.3803, -41.9494],\n",
      "        [-49.5777, -23.9371, 113.8453, -40.3306],\n",
      "        [-18.5984, -43.2542,  97.8560, -36.0034]], device='cuda:0')\n",
      "tensor([0.0000e+00, 0.0000e+00, 7.6294e-06], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "asdf = env.reset()\n",
    "for i in range(4):\n",
    "    order_book = torch.cat([torch.normal(mean=0.0,std=1.0,size=(3,4,4,2)), torch.randint(0,5,[3,4,4,2])], dim=-1).to(device)\n",
    "    _, r, _ = env.step(order_book)\n",
    "    if i == 3:\n",
    "        print(r)\n",
    "        print(r.sum(-1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "39d73e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002 (0.9, 0.999)\n",
      "tensor([[0., 2., 2., 6.],\n",
      "        [4., 1., 3., 2.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[2., 2., 6., 5.],\n",
      "        [3., 2., 1., 1.],\n",
      "        [3., 0., 1., 3.],\n",
      "        [2., 4., 2., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(-0.0344, device='cuda:0')\n",
      "tensor(-0.0303, device='cuda:0')\n",
      "p0 reward: 9.06660270690918\n",
      "p1 reward: -1.2037676572799683\n",
      "p2 reward: -13.09607219696045\n",
      "p3 reward: 5.233236789703369\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 1., 4.],\n",
      "        [4., 2., 3., 1.],\n",
      "        [1., 3., 3., 3.],\n",
      "        [2., 5., 1., 2.]], device='cuda:0')\n",
      "tensor([[3., 2., 0., 6.],\n",
      "        [3., 1., 2., 0.],\n",
      "        [4., 6., 3., 2.],\n",
      "        [0., 3., 3., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(-0.0248, device='cuda:0')\n",
      "tensor(-0.0422, device='cuda:0')\n",
      "p0 reward: 8.278311729431152\n",
      "p1 reward: -0.13259805738925934\n",
      "p2 reward: -12.778722763061523\n",
      "p3 reward: 4.6330084800720215\n",
      "====================================================================================================\n",
      "tensor([[2., 4., 1., 3.],\n",
      "        [4., 2., 1., 3.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[4., 3., 1., 2.],\n",
      "        [6., 3., 1., 5.],\n",
      "        [0., 2., 3., 3.],\n",
      "        [0., 2., 3., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(-0.0264, device='cuda:0')\n",
      "tensor(-0.0473, device='cuda:0')\n",
      "p0 reward: 10.225672721862793\n",
      "p1 reward: -0.4182369112968445\n",
      "p2 reward: -13.145554542541504\n",
      "p3 reward: 3.338120222091675\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 2., 4.],\n",
      "        [4., 0., 4., 2.],\n",
      "        [1., 3., 1., 5.],\n",
      "        [2., 4., 3., 1.]], device='cuda:0')\n",
      "tensor([[5., 3., 0., 4.],\n",
      "        [2., 0., 4., 3.],\n",
      "        [1., 1., 2., 3.],\n",
      "        [2., 4., 4., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(-0.0117, device='cuda:0')\n",
      "tensor(-0.0200, device='cuda:0')\n",
      "p0 reward: 8.60409164428711\n",
      "p1 reward: 0.4587612748146057\n",
      "p2 reward: -13.851173400878906\n",
      "p3 reward: 4.7883219718933105\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 2., 3.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [1., 5., 1., 3.],\n",
      "        [2., 3., 4., 1.]], device='cuda:0')\n",
      "tensor([[4., 3., 2., 2.],\n",
      "        [2., 0., 2., 3.],\n",
      "        [1., 5., 1., 2.],\n",
      "        [1., 4., 5., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(-0.0031, device='cuda:0')\n",
      "tensor(0.0091, device='cuda:0')\n",
      "p0 reward: 7.829328536987305\n",
      "p1 reward: 1.5631579160690308\n",
      "p2 reward: -12.539426803588867\n",
      "p3 reward: 3.1469390392303467\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 1., 4.],\n",
      "        [5., 3., 1., 1.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [0., 1., 6., 3.]], device='cuda:0')\n",
      "tensor([[4., 3., 2., 4.],\n",
      "        [5., 4., 2., 0.],\n",
      "        [0., 0., 0., 5.],\n",
      "        [1., 1., 6., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.0215, device='cuda:0')\n",
      "tensor(0.0219, device='cuda:0')\n",
      "p0 reward: 7.094723224639893\n",
      "p1 reward: 3.055642604827881\n",
      "p2 reward: -12.760504722595215\n",
      "p3 reward: 2.6101388931274414\n",
      "====================================================================================================\n",
      "tensor([[2., 4., 1., 3.],\n",
      "        [0., 3., 4., 3.],\n",
      "        [3., 3., 0., 4.],\n",
      "        [3., 0., 5., 2.]], device='cuda:0')\n",
      "tensor([[3., 3., 0., 3.],\n",
      "        [1., 3., 3., 2.],\n",
      "        [2., 0., 0., 1.],\n",
      "        [2., 4., 7., 6.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.0510, device='cuda:0')\n",
      "tensor(0.0368, device='cuda:0')\n",
      "p0 reward: 7.385436058044434\n",
      "p1 reward: 4.807796001434326\n",
      "p2 reward: -14.095037460327148\n",
      "p3 reward: 1.9018057584762573\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [6., 2., 1., 1.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [2., 4., 1., 3.]], device='cuda:0')\n",
      "tensor([[4., 3., 4., 3.],\n",
      "        [5., 2., 0., 3.],\n",
      "        [0., 2., 4., 2.],\n",
      "        [3., 3., 0., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.0641, device='cuda:0')\n",
      "tensor(0.0599, device='cuda:0')\n",
      "p0 reward: 6.601985454559326\n",
      "p1 reward: 3.602093458175659\n",
      "p2 reward: -13.342924118041992\n",
      "p3 reward: 3.1388449668884277\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 3., 2.],\n",
      "        [3., 3., 1., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [3., 1., 3., 3.]], device='cuda:0')\n",
      "tensor([[5., 3., 4., 5.],\n",
      "        [1., 4., 2., 2.],\n",
      "        [1., 2., 0., 2.],\n",
      "        [1., 1., 4., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.0798, device='cuda:0')\n",
      "tensor(0.0650, device='cuda:0')\n",
      "p0 reward: 6.629751205444336\n",
      "p1 reward: 3.294703722000122\n",
      "p2 reward: -11.556695938110352\n",
      "p3 reward: 1.6322405338287354\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 3., 3.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [4., 1., 3., 2.]], device='cuda:0')\n",
      "tensor([[4., 4., 0., 0.],\n",
      "        [2., 2., 2., 3.],\n",
      "        [3., 0., 4., 5.],\n",
      "        [1., 2., 6., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.1027, device='cuda:0')\n",
      "tensor(0.0862, device='cuda:0')\n",
      "p0 reward: 5.434380054473877\n",
      "p1 reward: 4.478776454925537\n",
      "p2 reward: -10.78800106048584\n",
      "p3 reward: 0.8748447895050049\n",
      "====================================================================================================\n",
      "tensor([[0., 3., 4., 3.],\n",
      "        [3., 4., 1., 2.],\n",
      "        [4., 1., 3., 2.],\n",
      "        [5., 2., 2., 1.]], device='cuda:0')\n",
      "tensor([[3., 3., 4., 1.],\n",
      "        [1., 4., 3., 3.],\n",
      "        [4., 1., 2., 1.],\n",
      "        [4., 2., 1., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.1294, device='cuda:0')\n",
      "tensor(0.1035, device='cuda:0')\n",
      "p0 reward: 5.641793251037598\n",
      "p1 reward: 4.197163105010986\n",
      "p2 reward: -10.528847694396973\n",
      "p3 reward: 0.6898905038833618\n",
      "====================================================================================================\n",
      "tensor([[3., 3., 3., 1.],\n",
      "        [0., 1., 3., 6.],\n",
      "        [4., 2., 3., 1.],\n",
      "        [5., 2., 1., 2.]], device='cuda:0')\n",
      "tensor([[3., 6., 3., 0.],\n",
      "        [0., 0., 2., 7.],\n",
      "        [2., 2., 1., 0.],\n",
      "        [7., 0., 4., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.1354, device='cuda:0')\n",
      "tensor(0.1354, device='cuda:0')\n",
      "p0 reward: 4.204712867736816\n",
      "p1 reward: 5.214953422546387\n",
      "p2 reward: -10.638895034790039\n",
      "p3 reward: 1.2192286252975464\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 0., 5.],\n",
      "        [2., 1., 6., 1.],\n",
      "        [1., 4., 3., 2.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[5., 4., 0., 4.],\n",
      "        [0., 0., 7., 4.],\n",
      "        [1., 3., 2., 2.],\n",
      "        [2., 3., 3., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.1598, device='cuda:0')\n",
      "tensor(0.1540, device='cuda:0')\n",
      "p0 reward: 5.265462398529053\n",
      "p1 reward: 4.593759536743164\n",
      "p2 reward: -11.058172225952148\n",
      "p3 reward: 1.1989514827728271\n",
      "====================================================================================================\n",
      "tensor([[3., 0., 4., 3.],\n",
      "        [0., 4., 5., 1.],\n",
      "        [3., 2., 0., 5.],\n",
      "        [2., 4., 1., 3.]], device='cuda:0')\n",
      "tensor([[3., 1., 3., 2.],\n",
      "        [1., 5., 5., 6.],\n",
      "        [4., 1., 0., 1.],\n",
      "        [0., 3., 2., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.1880, device='cuda:0')\n",
      "tensor(0.1674, device='cuda:0')\n",
      "p0 reward: 5.432884216308594\n",
      "p1 reward: 3.3884389400482178\n",
      "p2 reward: -11.260340690612793\n",
      "p3 reward: 2.4390180110931396\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 1., 5.],\n",
      "        [2., 4., 4., 0.],\n",
      "        [4., 2., 4., 0.],\n",
      "        [3., 1., 1., 5.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 5.],\n",
      "        [3., 4., 6., 0.],\n",
      "        [3., 2., 4., 0.],\n",
      "        [5., 1., 0., 5.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.2106, device='cuda:0')\n",
      "tensor(0.1876, device='cuda:0')\n",
      "p0 reward: 4.43131685256958\n",
      "p1 reward: 5.319802761077881\n",
      "p2 reward: -9.24509334564209\n",
      "p3 reward: -0.5060266256332397\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 2., 2.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [1., 4., 3., 2.],\n",
      "        [3., 3., 3., 1.]], device='cuda:0')\n",
      "tensor([[7., 2., 4., 2.],\n",
      "        [0., 4., 0., 3.],\n",
      "        [0., 4., 0., 1.],\n",
      "        [3., 2., 6., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(0.2346, device='cuda:0')\n",
      "tensor(0.2093, device='cuda:0')\n",
      "p0 reward: 3.634650945663452\n",
      "p1 reward: 4.728289604187012\n",
      "p2 reward: -7.714867115020752\n",
      "p3 reward: -0.6480741500854492\n",
      "====================================================================================================\n",
      "tensor([[2., 0., 6., 2.],\n",
      "        [2., 1., 4., 3.],\n",
      "        [1., 5., 1., 3.],\n",
      "        [5., 2., 1., 2.]], device='cuda:0')\n",
      "tensor([[1., 2., 5., 2.],\n",
      "        [4., 1., 3., 4.],\n",
      "        [0., 5., 2., 4.],\n",
      "        [5., 0., 2., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.2527, device='cuda:0')\n",
      "tensor(0.2340, device='cuda:0')\n",
      "p0 reward: 4.158263206481934\n",
      "p1 reward: 5.197745323181152\n",
      "p2 reward: -8.739951133728027\n",
      "p3 reward: -0.6160575747489929\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [4., 3., 2., 1.],\n",
      "        [1., 1., 2., 6.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[4., 2., 4., 2.],\n",
      "        [2., 2., 2., 3.],\n",
      "        [1., 1., 1., 4.],\n",
      "        [3., 3., 3., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.2881, device='cuda:0')\n",
      "tensor(0.2634, device='cuda:0')\n",
      "p0 reward: 4.560697555541992\n",
      "p1 reward: 5.274217128753662\n",
      "p2 reward: -8.376543045043945\n",
      "p3 reward: -1.4583708047866821\n",
      "====================================================================================================\n",
      "tensor([[3., 4., 2., 1.],\n",
      "        [1., 0., 3., 6.],\n",
      "        [4., 0., 4., 2.],\n",
      "        [2., 4., 3., 1.]], device='cuda:0')\n",
      "tensor([[4., 5., 1., 1.],\n",
      "        [0., 1., 3., 7.],\n",
      "        [6., 0., 6., 0.],\n",
      "        [0., 2., 2., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.3056, device='cuda:0')\n",
      "tensor(0.2617, device='cuda:0')\n",
      "p0 reward: 3.6626315116882324\n",
      "p1 reward: 4.536127090454102\n",
      "p2 reward: -8.699657440185547\n",
      "p3 reward: 0.5008984208106995\n",
      "====================================================================================================\n",
      "tensor([[0., 6., 2., 2.],\n",
      "        [3., 3., 1., 3.],\n",
      "        [4., 1., 2., 3.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[2., 6., 1., 1.],\n",
      "        [4., 1., 2., 4.],\n",
      "        [4., 0., 2., 3.],\n",
      "        [0., 5., 3., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(0.3065, device='cuda:0')\n",
      "tensor(0.2911, device='cuda:0')\n",
      "p0 reward: 2.819504499435425\n",
      "p1 reward: 5.300347805023193\n",
      "p2 reward: -8.25614070892334\n",
      "p3 reward: 0.13628843426704407\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 3., 3.],\n",
      "        [4., 2., 3., 1.],\n",
      "        [2., 4., 1., 3.],\n",
      "        [1., 3., 5., 1.]], device='cuda:0')\n",
      "tensor([[4., 2., 2., 3.],\n",
      "        [2., 0., 4., 2.],\n",
      "        [3., 7., 0., 3.],\n",
      "        [1., 1., 6., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.3348, device='cuda:0')\n",
      "tensor(0.3214, device='cuda:0')\n",
      "p0 reward: 2.5568249225616455\n",
      "p1 reward: 6.419927597045898\n",
      "p2 reward: -8.105108261108398\n",
      "p3 reward: -0.8716449737548828\n",
      "====================================================================================================\n",
      "tensor([[0., 6., 2., 2.],\n",
      "        [3., 1., 6., 0.],\n",
      "        [2., 0., 3., 5.],\n",
      "        [5., 3., 1., 1.]], device='cuda:0')\n",
      "tensor([[0., 4., 2., 1.],\n",
      "        [3., 1., 8., 1.],\n",
      "        [2., 0., 2., 5.],\n",
      "        [5., 5., 0., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.3614, device='cuda:0')\n",
      "tensor(0.3465, device='cuda:0')\n",
      "p0 reward: 2.241408348083496\n",
      "p1 reward: 4.9554057121276855\n",
      "p2 reward: -6.629261016845703\n",
      "p3 reward: -0.5675526857376099\n",
      "====================================================================================================\n",
      "tensor([[2., 0., 3., 5.],\n",
      "        [2., 4., 1., 3.],\n",
      "        [2., 5., 1., 2.],\n",
      "        [2., 1., 5., 2.]], device='cuda:0')\n",
      "tensor([[4., 0., 4., 5.],\n",
      "        [1., 4., 0., 3.],\n",
      "        [2., 6., 1., 2.],\n",
      "        [1., 0., 5., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.3790, device='cuda:0')\n",
      "tensor(0.3659, device='cuda:0')\n",
      "p0 reward: 2.7884209156036377\n",
      "p1 reward: 5.422773361206055\n",
      "p2 reward: -7.232516765594482\n",
      "p3 reward: -0.9786773920059204\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 1., 3.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [0., 2., 4., 4.]], device='cuda:0')\n",
      "tensor([[3., 3., 0., 2.],\n",
      "        [3., 3., 2., 5.],\n",
      "        [2., 1., 4., 1.],\n",
      "        [0., 3., 4., 4.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.3980, device='cuda:0')\n",
      "tensor(0.3660, device='cuda:0')\n",
      "p0 reward: 1.549582600593567\n",
      "p1 reward: 5.809421062469482\n",
      "p2 reward: -6.963289737701416\n",
      "p3 reward: -0.3957141041755676\n",
      "====================================================================================================\n",
      "tensor([[3., 4., 2., 1.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [4., 1., 2., 3.],\n",
      "        [2., 3., 1., 4.]], device='cuda:0')\n",
      "tensor([[6., 4., 2., 0.],\n",
      "        [2., 4., 2., 3.],\n",
      "        [3., 0., 2., 2.],\n",
      "        [1., 2., 2., 5.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.4183, device='cuda:0')\n",
      "tensor(0.4008, device='cuda:0')\n",
      "p0 reward: 1.3997968435287476\n",
      "p1 reward: 5.580965995788574\n",
      "p2 reward: -5.798084259033203\n",
      "p3 reward: -1.18267822265625\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 4., 2.],\n",
      "        [4., 3., 1., 2.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[0., 3., 4., 2.],\n",
      "        [4., 3., 0., 2.],\n",
      "        [1., 3., 2., 2.],\n",
      "        [3., 1., 6., 4.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.4342, device='cuda:0')\n",
      "tensor(0.4208, device='cuda:0')\n",
      "p0 reward: 0.274387389421463\n",
      "p1 reward: 4.538571357727051\n",
      "p2 reward: -5.666704177856445\n",
      "p3 reward: 0.8537446856498718\n",
      "====================================================================================================\n",
      "tensor([[2., 1., 5., 2.],\n",
      "        [3., 4., 1., 2.],\n",
      "        [4., 2., 3., 1.],\n",
      "        [3., 3., 1., 3.]], device='cuda:0')\n",
      "tensor([[3., 2., 7., 3.],\n",
      "        [7., 4., 0., 2.],\n",
      "        [2., 1., 3., 0.],\n",
      "        [0., 3., 0., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.4642, device='cuda:0')\n",
      "tensor(0.4351, device='cuda:0')\n",
      "p0 reward: 2.532554864883423\n",
      "p1 reward: 2.3557186126708984\n",
      "p2 reward: -5.313133239746094\n",
      "p3 reward: 0.4248594045639038\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [3., 0., 4., 3.],\n",
      "        [3., 3., 3., 1.],\n",
      "        [3., 3., 0., 4.]], device='cuda:0')\n",
      "tensor([[2., 2., 0., 1.],\n",
      "        [2., 0., 5., 6.],\n",
      "        [4., 3., 4., 1.],\n",
      "        [4., 3., 1., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.4935, device='cuda:0')\n",
      "tensor(0.4744, device='cuda:0')\n",
      "p0 reward: 0.8480414152145386\n",
      "p1 reward: 4.64990758895874\n",
      "p2 reward: -4.714916229248047\n",
      "p3 reward: -0.7830327749252319\n",
      "====================================================================================================\n",
      "tensor([[3., 5., 1., 1.],\n",
      "        [4., 5., 1., 0.],\n",
      "        [3., 0., 3., 4.],\n",
      "        [2., 0., 5., 3.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 4., 0., 1.],\n",
      "        [1., 4., 1., 2.],\n",
      "        [3., 2., 3., 3.],\n",
      "        [4., 0., 6., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.5099, device='cuda:0')\n",
      "tensor(0.4890, device='cuda:0')\n",
      "p0 reward: 0.9275861978530884\n",
      "p1 reward: 3.627269983291626\n",
      "p2 reward: -4.589491367340088\n",
      "p3 reward: 0.03463510423898697\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 1., 5.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [0., 3., 4., 3.]], device='cuda:0')\n",
      "tensor([[1., 2., 3., 5.],\n",
      "        [2., 5., 3., 2.],\n",
      "        [3., 3., 0., 2.],\n",
      "        [2., 0., 4., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.5206, device='cuda:0')\n",
      "tensor(0.5131, device='cuda:0')\n",
      "p0 reward: 0.6122013330459595\n",
      "p1 reward: 4.468562602996826\n",
      "p2 reward: -5.05485725402832\n",
      "p3 reward: -0.025907471776008606\n",
      "====================================================================================================\n",
      "tensor([[5., 3., 1., 1.],\n",
      "        [3., 1., 4., 2.],\n",
      "        [1., 4., 1., 4.],\n",
      "        [3., 2., 2., 3.]], device='cuda:0')\n",
      "tensor([[5., 5., 0., 1.],\n",
      "        [4., 1., 3., 1.],\n",
      "        [0., 3., 1., 4.],\n",
      "        [3., 1., 4., 4.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.5489, device='cuda:0')\n",
      "tensor(0.5266, device='cuda:0')\n",
      "p0 reward: 0.18859544396400452\n",
      "p1 reward: 4.875993728637695\n",
      "p2 reward: -4.8851189613342285\n",
      "p3 reward: -0.17947089672088623\n",
      "====================================================================================================\n",
      "tensor([[3., 3., 1., 3.],\n",
      "        [1., 3., 3., 3.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [1., 2., 3., 4.]], device='cuda:0')\n",
      "tensor([[3., 3., 1., 3.],\n",
      "        [2., 3., 3., 4.],\n",
      "        [2., 2., 3., 2.],\n",
      "        [1., 2., 3., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.5703, device='cuda:0')\n",
      "tensor(0.5579, device='cuda:0')\n",
      "p0 reward: 0.7941780090332031\n",
      "p1 reward: 2.356661796569824\n",
      "p2 reward: -3.556047201156616\n",
      "p3 reward: 0.40520742535591125\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 2., 3.],\n",
      "        [1., 1., 4., 4.],\n",
      "        [1., 4., 3., 2.],\n",
      "        [5., 1., 1., 3.]], device='cuda:0')\n",
      "tensor([[3., 3., 2., 3.],\n",
      "        [2., 1., 4., 6.],\n",
      "        [0., 4., 2., 1.],\n",
      "        [5., 0., 2., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.5829, device='cuda:0')\n",
      "tensor(0.5563, device='cuda:0')\n",
      "p0 reward: -0.16455306112766266\n",
      "p1 reward: 4.295565605163574\n",
      "p2 reward: -4.967263698577881\n",
      "p3 reward: 0.8362512588500977\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [2., 3., 1., 4.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [3., 2., 2., 3.]], device='cuda:0')\n",
      "tensor([[2., 2., 4., 3.],\n",
      "        [2., 2., 0., 4.],\n",
      "        [3., 4., 0., 2.],\n",
      "        [3., 2., 4., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.5915, device='cuda:0')\n",
      "tensor(0.5728, device='cuda:0')\n",
      "p0 reward: 0.16950254142284393\n",
      "p1 reward: 2.269192934036255\n",
      "p2 reward: -2.8024001121520996\n",
      "p3 reward: 0.36370447278022766\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 0., 5.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [1., 2., 5., 2.],\n",
      "        [2., 4., 4., 0.]], device='cuda:0')\n",
      "tensor([[2., 3., 0., 5.],\n",
      "        [6., 2., 3., 4.],\n",
      "        [0., 1., 5., 1.],\n",
      "        [0., 4., 4., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.6054, device='cuda:0')\n",
      "tensor(0.5953, device='cuda:0')\n",
      "p0 reward: -0.8491442799568176\n",
      "p1 reward: 3.5606632232666016\n",
      "p2 reward: -3.8380162715911865\n",
      "p3 reward: 1.1264971494674683\n",
      "====================================================================================================\n",
      "tensor([[0., 4., 3., 3.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [4., 1., 2., 3.],\n",
      "        [3., 1., 4., 2.]], device='cuda:0')\n",
      "tensor([[0., 4., 3., 2.],\n",
      "        [3., 3., 2., 3.],\n",
      "        [3., 0., 1., 3.],\n",
      "        [4., 1., 6., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.6330, device='cuda:0')\n",
      "tensor(0.6113, device='cuda:0')\n",
      "p0 reward: 0.3124205470085144\n",
      "p1 reward: 3.611629009246826\n",
      "p2 reward: -4.4761810302734375\n",
      "p3 reward: 0.5521311163902283\n",
      "====================================================================================================\n",
      "tensor([[4., 3., 2., 1.],\n",
      "        [1., 4., 4., 1.],\n",
      "        [2., 3., 1., 4.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[4., 3., 2., 0.],\n",
      "        [1., 4., 4., 2.],\n",
      "        [2., 3., 0., 3.],\n",
      "        [3., 2., 4., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(0.6480, device='cuda:0')\n",
      "tensor(0.6342, device='cuda:0')\n",
      "p0 reward: 0.8459032773971558\n",
      "p1 reward: 1.420068383216858\n",
      "p2 reward: -2.9429190158843994\n",
      "p3 reward: 0.6769469976425171\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 4., 2.],\n",
      "        [4., 3., 3., 0.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [3., 2., 2., 3.]], device='cuda:0')\n",
      "tensor([[3., 4., 3., 2.],\n",
      "        [5., 1., 3., 0.],\n",
      "        [1., 2., 2., 3.],\n",
      "        [1., 3., 4., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.6697, device='cuda:0')\n",
      "tensor(0.6644, device='cuda:0')\n",
      "p0 reward: 0.49824944138526917\n",
      "p1 reward: 2.336216926574707\n",
      "p2 reward: -3.214231014251709\n",
      "p3 reward: 0.3797646760940552\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 2., 3.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [4., 2., 2., 2.]], device='cuda:0')\n",
      "tensor([[5., 1., 2., 4.],\n",
      "        [6., 4., 3., 1.],\n",
      "        [0., 3., 2., 3.],\n",
      "        [1., 0., 3., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.6972, device='cuda:0')\n",
      "tensor(0.7024, device='cuda:0')\n",
      "p0 reward: -0.053108979016542435\n",
      "p1 reward: 3.96636700630188\n",
      "p2 reward: -2.755166530609131\n",
      "p3 reward: -1.1580917835235596\n",
      "====================================================================================================\n",
      "tensor([[5., 2., 2., 1.],\n",
      "        [2., 1., 3., 4.],\n",
      "        [1., 4., 1., 4.],\n",
      "        [4., 3., 2., 1.]], device='cuda:0')\n",
      "tensor([[5., 2., 2., 0.],\n",
      "        [0., 1., 0., 5.],\n",
      "        [1., 4., 3., 5.],\n",
      "        [6., 3., 3., 0.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.7316, device='cuda:0')\n",
      "tensor(0.6913, device='cuda:0')\n",
      "p0 reward: -0.3219868838787079\n",
      "p1 reward: 3.2590785026550293\n",
      "p2 reward: -3.5361013412475586\n",
      "p3 reward: 0.5990095734596252\n",
      "====================================================================================================\n",
      "tensor([[0., 3., 5., 2.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [3., 2., 2., 3.]], device='cuda:0')\n",
      "tensor([[0., 4., 6., 2.],\n",
      "        [2., 1., 2., 4.],\n",
      "        [5., 1., 2., 3.],\n",
      "        [3., 2., 2., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.7398, device='cuda:0')\n",
      "tensor(0.7096, device='cuda:0')\n",
      "p0 reward: 0.06139914318919182\n",
      "p1 reward: 2.018148183822632\n",
      "p2 reward: -2.297537088394165\n",
      "p3 reward: 0.21798944473266602\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 3., 2.],\n",
      "        [1., 4., 0., 5.],\n",
      "        [6., 3., 0., 1.],\n",
      "        [2., 1., 5., 2.]], device='cuda:0')\n",
      "tensor([[1., 5., 3., 2.],\n",
      "        [3., 4., 0., 5.],\n",
      "        [6., 2., 0., 1.],\n",
      "        [0., 1., 5., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(0.7507, device='cuda:0')\n",
      "tensor(0.7472, device='cuda:0')\n",
      "p0 reward: 0.48247620463371277\n",
      "p1 reward: 1.2321016788482666\n",
      "p2 reward: -2.3157222270965576\n",
      "p3 reward: 0.6011441946029663\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [4., 3., 1., 2.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [1., 5., 3., 1.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 3., 3.],\n",
      "        [4., 3., 1., 2.],\n",
      "        [4., 3., 3., 2.],\n",
      "        [2., 5., 3., 1.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(0.7766, device='cuda:0')\n",
      "tensor(0.7578, device='cuda:0')\n",
      "p0 reward: 0.6535454988479614\n",
      "p1 reward: 1.8488638401031494\n",
      "p2 reward: -3.0978825092315674\n",
      "p3 reward: 0.5954727530479431\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 4., 2.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [1., 2., 2., 5.],\n",
      "        [3., 3., 2., 2.]], device='cuda:0')\n",
      "tensor([[4., 2., 6., 4.],\n",
      "        [3., 1., 2., 0.],\n",
      "        [0., 3., 0., 6.],\n",
      "        [3., 2., 2., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.7731, device='cuda:0')\n",
      "tensor(0.7851, device='cuda:0')\n",
      "p0 reward: 1.0041126012802124\n",
      "p1 reward: 2.6152966022491455\n",
      "p2 reward: -3.3960490226745605\n",
      "p3 reward: -0.22336015105247498\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [2., 1., 3., 4.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [1., 4., 2., 3.]], device='cuda:0')\n",
      "tensor([[2., 4., 3., 2.],\n",
      "        [3., 1., 2., 6.],\n",
      "        [3., 0., 2., 1.],\n",
      "        [0., 5., 3., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8301, device='cuda:0')\n",
      "tensor(0.7802, device='cuda:0')\n",
      "p0 reward: 0.07840251177549362\n",
      "p1 reward: 2.5626230239868164\n",
      "p2 reward: -2.51982045173645\n",
      "p3 reward: -0.12120520323514938\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 3., 2.],\n",
      "        [1., 3., 1., 5.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[4., 0., 4., 1.],\n",
      "        [2., 3., 1., 6.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [2., 3., 2., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8384, device='cuda:0')\n",
      "tensor(0.7960, device='cuda:0')\n",
      "p0 reward: -0.13904905319213867\n",
      "p1 reward: 2.389861822128296\n",
      "p2 reward: -2.030876874923706\n",
      "p3 reward: -0.21993592381477356\n",
      "====================================================================================================\n",
      "tensor([[4., 3., 1., 2.],\n",
      "        [4., 2., 1., 3.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[5., 2., 2., 2.],\n",
      "        [4., 4., 0., 3.],\n",
      "        [2., 2., 2., 3.],\n",
      "        [1., 2., 4., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8430, device='cuda:0')\n",
      "tensor(0.8185, device='cuda:0')\n",
      "p0 reward: 0.8001778721809387\n",
      "p1 reward: 2.2276291847229004\n",
      "p2 reward: -3.0033116340637207\n",
      "p3 reward: -0.02449575997889042\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 2., 3.],\n",
      "        [1., 3., 2., 4.],\n",
      "        [2., 1., 3., 4.],\n",
      "        [4., 4., 1., 1.]], device='cuda:0')\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 3., 2., 5.],\n",
      "        [2., 1., 3., 4.],\n",
      "        [4., 4., 1., 1.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8771, device='cuda:0')\n",
      "tensor(0.8465, device='cuda:0')\n",
      "p0 reward: 0.24180445075035095\n",
      "p1 reward: 2.067349672317505\n",
      "p2 reward: -1.727223515510559\n",
      "p3 reward: -0.5819308161735535\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 3., 2.],\n",
      "        [4., 3., 2., 1.],\n",
      "        [0., 3., 2., 5.],\n",
      "        [4., 3., 1., 2.]], device='cuda:0')\n",
      "tensor([[4., 5., 3., 4.],\n",
      "        [4., 4., 4., 1.],\n",
      "        [0., 0., 0., 4.],\n",
      "        [4., 1., 1., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8797, device='cuda:0')\n",
      "tensor(0.8607, device='cuda:0')\n",
      "p0 reward: -0.707389771938324\n",
      "p1 reward: 1.590923547744751\n",
      "p2 reward: -1.2428532838821411\n",
      "p3 reward: 0.3593193590641022\n",
      "====================================================================================================\n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [3., 5., 0., 2.],\n",
      "        [4., 2., 1., 3.]], device='cuda:0')\n",
      "tensor([[3., 1., 3., 4.],\n",
      "        [3., 3., 3., 0.],\n",
      "        [2., 5., 1., 1.],\n",
      "        [4., 1., 1., 5.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8993, device='cuda:0')\n",
      "tensor(0.8797, device='cuda:0')\n",
      "p0 reward: 0.4135126769542694\n",
      "p1 reward: 2.106839895248413\n",
      "p2 reward: -2.9827027320861816\n",
      "p3 reward: 0.462350070476532\n",
      "====================================================================================================\n",
      "tensor([[4., 0., 4., 2.],\n",
      "        [0., 3., 4., 3.],\n",
      "        [3., 4., 2., 1.],\n",
      "        [3., 3., 2., 2.]], device='cuda:0')\n",
      "tensor([[4., 0., 4., 2.],\n",
      "        [0., 4., 4., 4.],\n",
      "        [2., 4., 1., 1.],\n",
      "        [4., 2., 3., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.9166, device='cuda:0')\n",
      "tensor(0.8964, device='cuda:0')\n",
      "p0 reward: -0.03725550323724747\n",
      "p1 reward: 3.2828927040100098\n",
      "p2 reward: -3.020493507385254\n",
      "p3 reward: -0.22514410316944122\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 2., 3.],\n",
      "        [1., 5., 3., 1.],\n",
      "        [2., 3., 1., 4.],\n",
      "        [3., 3., 2., 2.]], device='cuda:0')\n",
      "tensor([[4., 3., 2., 4.],\n",
      "        [1., 5., 4., 2.],\n",
      "        [1., 3., 0., 3.],\n",
      "        [4., 1., 2., 1.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(0.9236, device='cuda:0')\n",
      "tensor(0.9177, device='cuda:0')\n",
      "p0 reward: -0.37757888436317444\n",
      "p1 reward: 1.806831955909729\n",
      "p2 reward: -2.3068454265594482\n",
      "p3 reward: 0.8775922060012817\n",
      "====================================================================================================\n",
      "tensor([[3., 0., 5., 2.],\n",
      "        [2., 5., 2., 1.],\n",
      "        [5., 3., 0., 2.],\n",
      "        [0., 2., 5., 3.]], device='cuda:0')\n",
      "tensor([[4., 0., 5., 0.],\n",
      "        [1., 5., 2., 3.],\n",
      "        [5., 3., 0., 2.],\n",
      "        [0., 2., 5., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.9336, device='cuda:0')\n",
      "tensor(0.9217, device='cuda:0')\n",
      "p0 reward: -0.8882302641868591\n",
      "p1 reward: 1.8133518695831299\n",
      "p2 reward: -1.970885992050171\n",
      "p3 reward: 1.0457639694213867\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 5., 1.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [1., 3., 2., 4.],\n",
      "        [4., 2., 1., 3.]], device='cuda:0')\n",
      "tensor([[2., 0., 5., 1.],\n",
      "        [1., 2., 1., 4.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [4., 3., 1., 4.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.9576, device='cuda:0')\n",
      "tensor(0.9339, device='cuda:0')\n",
      "p0 reward: -2.1420750617980957\n",
      "p1 reward: 2.9209249019622803\n",
      "p2 reward: -1.0503416061401367\n",
      "p3 reward: 0.27149128913879395\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [3., 4., 2., 1.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [5., 1., 2., 2.]], device='cuda:0')\n",
      "tensor([[0., 0., 3., 4.],\n",
      "        [3., 7., 2., 1.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [6., 0., 2., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.9740, device='cuda:0')\n",
      "tensor(0.9538, device='cuda:0')\n",
      "p0 reward: 0.3030041754245758\n",
      "p1 reward: 0.9025511145591736\n",
      "p2 reward: -1.7624536752700806\n",
      "p3 reward: 0.5568983554840088\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 2., 2.],\n",
      "        [2., 5., 1., 2.],\n",
      "        [1., 4., 4., 1.],\n",
      "        [3., 1., 1., 5.]], device='cuda:0')\n",
      "tensor([[5., 0., 1., 2.],\n",
      "        [0., 6., 1., 5.],\n",
      "        [1., 4., 4., 0.],\n",
      "        [4., 2., 2., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0154, device='cuda:0')\n",
      "tensor(0.9947, device='cuda:0')\n",
      "p0 reward: 1.211545467376709\n",
      "p1 reward: 1.200721025466919\n",
      "p2 reward: -2.459627628326416\n",
      "p3 reward: 0.047360628843307495\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 4., 3.],\n",
      "        [3., 2., 1., 4.],\n",
      "        [2., 1., 4., 3.],\n",
      "        [2., 5., 1., 2.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 3., 4., 4.],\n",
      "        [3., 2., 1., 3.],\n",
      "        [0., 0., 4., 3.],\n",
      "        [3., 5., 1., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0428, device='cuda:0')\n",
      "tensor(1.0175, device='cuda:0')\n",
      "p0 reward: 0.9868881702423096\n",
      "p1 reward: 0.9250552654266357\n",
      "p2 reward: -1.6215802431106567\n",
      "p3 reward: -0.2903631925582886\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [3., 3., 3., 1.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [1., 3., 4., 2.]], device='cuda:0')\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [4., 5., 3., 1.],\n",
      "        [2., 2., 2., 3.],\n",
      "        [1., 1., 4., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0697, device='cuda:0')\n",
      "tensor(1.0549, device='cuda:0')\n",
      "p0 reward: 0.17776258289813995\n",
      "p1 reward: 2.227666139602661\n",
      "p2 reward: -1.4711635112762451\n",
      "p3 reward: -0.934265673160553\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 3., 2.],\n",
      "        [0., 6., 1., 3.],\n",
      "        [1., 4., 3., 2.],\n",
      "        [3., 1., 3., 3.]], device='cuda:0')\n",
      "tensor([[4., 2., 3., 3.],\n",
      "        [3., 7., 1., 3.],\n",
      "        [0., 2., 3., 2.],\n",
      "        [1., 1., 3., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0716, device='cuda:0')\n",
      "tensor(1.0714, device='cuda:0')\n",
      "p0 reward: 0.7092837691307068\n",
      "p1 reward: 0.8873305916786194\n",
      "p2 reward: -1.0933594703674316\n",
      "p3 reward: -0.5032552480697632\n",
      "====================================================================================================\n",
      "tensor([[0., 3., 4., 3.],\n",
      "        [2., 5., 2., 1.],\n",
      "        [4., 0., 2., 4.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[0., 2., 4., 3.],\n",
      "        [2., 6., 2., 0.],\n",
      "        [4., 0., 2., 4.],\n",
      "        [2., 2., 4., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.1143, device='cuda:0')\n",
      "tensor(1.0827, device='cuda:0')\n",
      "p0 reward: -0.4585922360420227\n",
      "p1 reward: 1.0470256805419922\n",
      "p2 reward: -0.8330749869346619\n",
      "p3 reward: 0.24464133381843567\n",
      "====================================================================================================\n",
      "tensor([[1., 1., 4., 4.],\n",
      "        [4., 2., 3., 1.],\n",
      "        [2., 5., 1., 2.],\n",
      "        [3., 2., 4., 1.]], device='cuda:0')\n",
      "tensor([[0., 3., 5., 2.],\n",
      "        [5., 2., 5., 3.],\n",
      "        [2., 5., 1., 1.],\n",
      "        [3., 0., 1., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.1286, device='cuda:0')\n",
      "tensor(1.0917, device='cuda:0')\n",
      "p0 reward: -0.5134164690971375\n",
      "p1 reward: 0.9894620776176453\n",
      "p2 reward: 0.19844114780426025\n",
      "p3 reward: -0.6744871139526367\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[3., 2., 2., 2.],\n",
      "        [2., 2., 3., 4.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.1532, device='cuda:0')\n",
      "tensor(1.1047, device='cuda:0')\n",
      "p0 reward: 0.656555712223053\n",
      "p1 reward: 0.6255750060081482\n",
      "p2 reward: -2.104620933532715\n",
      "p3 reward: 0.8224902749061584\n",
      "====================================================================================================\n",
      "tensor([[0., 4., 3., 3.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [6., 0., 1., 3.],\n",
      "        [2., 6., 1., 1.]], device='cuda:0')\n",
      "tensor([[0., 4., 3., 3.],\n",
      "        [3., 2., 2., 2.],\n",
      "        [6., 0., 1., 3.],\n",
      "        [1., 6., 2., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.1396, device='cuda:0')\n",
      "tensor(1.1403, device='cuda:0')\n",
      "p0 reward: 1.8902233839035034\n",
      "p1 reward: -0.02615542709827423\n",
      "p2 reward: -0.6059048771858215\n",
      "p3 reward: -1.2581629753112793\n",
      "====================================================================================================\n",
      "tensor([[0., 2., 4., 4.],\n",
      "        [5., 2., 1., 2.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [4., 3., 1., 2.]], device='cuda:0')\n",
      "tensor([[0., 3., 4., 5.],\n",
      "        [5., 1., 1., 3.],\n",
      "        [3., 2., 2., 1.],\n",
      "        [4., 4., 1., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.1699, device='cuda:0')\n",
      "tensor(1.1521, device='cuda:0')\n",
      "p0 reward: 1.206518292427063\n",
      "p1 reward: -0.7046516537666321\n",
      "p2 reward: -0.5441965460777283\n",
      "p3 reward: 0.04232977330684662\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 2., 3.],\n",
      "        [2., 3., 4., 1.],\n",
      "        [3., 3., 3., 1.],\n",
      "        [2., 4., 1., 3.]], device='cuda:0')\n",
      "tensor([[4., 2., 2., 3.],\n",
      "        [0., 3., 4., 1.],\n",
      "        [3., 2., 3., 1.],\n",
      "        [3., 5., 1., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2130, device='cuda:0')\n",
      "tensor(1.1957, device='cuda:0')\n",
      "p0 reward: -0.8317092657089233\n",
      "p1 reward: 1.6212538480758667\n",
      "p2 reward: -2.286593198776245\n",
      "p3 reward: 1.4970483779907227\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [2., 5., 0., 3.]], device='cuda:0')\n",
      "tensor([[3., 3., 2., 2.],\n",
      "        [4., 2., 3., 2.],\n",
      "        [1., 1., 2., 3.],\n",
      "        [2., 6., 1., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2501, device='cuda:0')\n",
      "tensor(1.2154, device='cuda:0')\n",
      "p0 reward: -1.519101858139038\n",
      "p1 reward: 0.8766834735870361\n",
      "p2 reward: -0.22689105570316315\n",
      "p3 reward: 0.8693094849586487\n",
      "====================================================================================================\n",
      "tensor([[3., 4., 0., 3.],\n",
      "        [3., 3., 3., 1.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [3., 1., 5., 1.]], device='cuda:0')\n",
      "tensor([[2., 4., 0., 4.],\n",
      "        [4., 4., 3., 1.],\n",
      "        [3., 1., 2., 3.],\n",
      "        [3., 1., 5., 0.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2870, device='cuda:0')\n",
      "tensor(1.2617, device='cuda:0')\n",
      "p0 reward: -0.8052554130554199\n",
      "p1 reward: 1.2351540327072144\n",
      "p2 reward: -1.4060649871826172\n",
      "p3 reward: 0.976166307926178\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 3., 3.],\n",
      "        [3., 3., 1., 3.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [1., 3., 3., 3.]], device='cuda:0')\n",
      "tensor([[0., 2., 4., 5.],\n",
      "        [3., 3., 0., 3.],\n",
      "        [5., 1., 4., 1.],\n",
      "        [0., 4., 2., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2964, device='cuda:0')\n",
      "tensor(1.2715, device='cuda:0')\n",
      "p0 reward: -0.26814353466033936\n",
      "p1 reward: 1.9403188228607178\n",
      "p2 reward: -1.185464859008789\n",
      "p3 reward: -0.4867105185985565\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 1., 4.],\n",
      "        [2., 2., 5., 1.],\n",
      "        [3., 4., 1., 2.],\n",
      "        [3., 3., 3., 1.]], device='cuda:0')\n",
      "tensor([[2., 3., 1., 4.],\n",
      "        [2., 3., 6., 1.],\n",
      "        [3., 4., 0., 2.],\n",
      "        [3., 2., 3., 1.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.3047, device='cuda:0')\n",
      "tensor(1.3030, device='cuda:0')\n",
      "p0 reward: -0.27749496698379517\n",
      "p1 reward: 2.1947126388549805\n",
      "p2 reward: -0.7636532187461853\n",
      "p3 reward: -1.1535645723342896\n",
      "====================================================================================================\n",
      "tensor([[2., 4., 2., 2.],\n",
      "        [3., 1., 4., 2.],\n",
      "        [1., 3., 3., 3.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[3., 4., 3., 1.],\n",
      "        [3., 1., 4., 2.],\n",
      "        [1., 3., 5., 4.],\n",
      "        [1., 2., 0., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.3291, device='cuda:0')\n",
      "tensor(1.3315, device='cuda:0')\n",
      "p0 reward: -0.9667418599128723\n",
      "p1 reward: 1.0463470220565796\n",
      "p2 reward: -0.48680025339126587\n",
      "p3 reward: 0.4071950912475586\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [2., 1., 4., 3.],\n",
      "        [2., 5., 2., 1.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 0., 4., 1.],\n",
      "        [3., 4., 5., 3.],\n",
      "        [2., 1., 0., 3.],\n",
      "        [2., 5., 3., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.3730, device='cuda:0')\n",
      "tensor(1.3406, device='cuda:0')\n",
      "p0 reward: -0.55738765001297\n",
      "p1 reward: 1.8275108337402344\n",
      "p2 reward: -1.3987829685211182\n",
      "p3 reward: 0.12865965068340302\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 2., 2.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [3., 2., 1., 4.],\n",
      "        [1., 3., 3., 3.]], device='cuda:0')\n",
      "tensor([[4., 1., 2., 2.],\n",
      "        [2., 3., 3., 3.],\n",
      "        [3., 3., 0., 4.],\n",
      "        [1., 3., 3., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.4167, device='cuda:0')\n",
      "tensor(1.3889, device='cuda:0')\n",
      "p0 reward: -0.411294162273407\n",
      "p1 reward: 1.6196173429489136\n",
      "p2 reward: -0.48219192028045654\n",
      "p3 reward: -0.7261316180229187\n",
      "====================================================================================================\n",
      "tensor([[3., 0., 4., 3.],\n",
      "        [5., 2., 2., 1.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [0., 6., 2., 2.]], device='cuda:0')\n",
      "tensor([[3., 0., 4., 3.],\n",
      "        [5., 1., 2., 0.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [0., 7., 2., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.4369, device='cuda:0')\n",
      "tensor(1.4351, device='cuda:0')\n",
      "p0 reward: -0.615504801273346\n",
      "p1 reward: 3.463242292404175\n",
      "p2 reward: -1.4645946025848389\n",
      "p3 reward: -1.3831428289413452\n",
      "====================================================================================================\n",
      "tensor([[3., 3., 1., 3.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [4., 2., 2., 2.]], device='cuda:0')\n",
      "tensor([[3., 2., 3., 3.],\n",
      "        [2., 5., 2., 3.],\n",
      "        [3., 3., 2., 1.],\n",
      "        [4., 0., 1., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.4597, device='cuda:0')\n",
      "tensor(1.4552, device='cuda:0')\n",
      "p0 reward: -0.21746985614299774\n",
      "p1 reward: 2.41226863861084\n",
      "p2 reward: -2.0860118865966797\n",
      "p3 reward: -0.10878679901361465\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 2., 4.],\n",
      "        [0., 2., 3., 5.],\n",
      "        [4., 4., 1., 1.],\n",
      "        [3., 1., 4., 2.]], device='cuda:0')\n",
      "tensor([[2., 3., 2., 4.],\n",
      "        [0., 1., 3., 5.],\n",
      "        [4., 4., 1., 1.],\n",
      "        [4., 0., 4., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.5002, device='cuda:0')\n",
      "tensor(1.5062, device='cuda:0')\n",
      "p0 reward: -1.256397008895874\n",
      "p1 reward: 2.8643295764923096\n",
      "p2 reward: -0.8728198409080505\n",
      "p3 reward: -0.7351128458976746\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 1., 4.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [1., 3., 3., 3.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[1., 3., 2., 4.],\n",
      "        [3., 2., 2., 2.],\n",
      "        [2., 3., 3., 3.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.4916, device='cuda:0')\n",
      "tensor(1.5390, device='cuda:0')\n",
      "p0 reward: 0.3244309425354004\n",
      "p1 reward: 1.6088119745254517\n",
      "p2 reward: -2.1740450859069824\n",
      "p3 reward: 0.24080173671245575\n",
      "====================================================================================================\n",
      "tensor([[2., 6., 0., 2.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [4., 2., 4., 0.],\n",
      "        [2., 2., 2., 4.]], device='cuda:0')\n",
      "tensor([[2., 7., 0., 2.],\n",
      "        [3., 1., 4., 1.],\n",
      "        [4., 3., 4., 0.],\n",
      "        [1., 1., 2., 5.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.5535, device='cuda:0')\n",
      "tensor(1.5243, device='cuda:0')\n",
      "p0 reward: -0.6595412492752075\n",
      "p1 reward: 2.0722296237945557\n",
      "p2 reward: -1.4591776132583618\n",
      "p3 reward: 0.04648895561695099\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 2., 3.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [3., 3., 1., 3.]], device='cuda:0')\n",
      "tensor([[1., 3., 3., 3.],\n",
      "        [3., 2., 1., 3.],\n",
      "        [3., 2., 3., 3.],\n",
      "        [3., 3., 1., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.5633, device='cuda:0')\n",
      "tensor(1.5611, device='cuda:0')\n",
      "p0 reward: 0.8875132203102112\n",
      "p1 reward: 1.1193535327911377\n",
      "p2 reward: -1.9469177722930908\n",
      "p3 reward: -0.05994934216141701\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 2., 4.],\n",
      "        [3., 4., 2., 1.],\n",
      "        [4., 1., 1., 4.],\n",
      "        [2., 0., 5., 3.]], device='cuda:0')\n",
      "tensor([[1., 3., 4., 4.],\n",
      "        [3., 4., 2., 1.],\n",
      "        [4., 1., 0., 4.],\n",
      "        [2., 0., 4., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.6086, device='cuda:0')\n",
      "tensor(1.6038, device='cuda:0')\n",
      "p0 reward: -0.3328360319137573\n",
      "p1 reward: 2.2146694660186768\n",
      "p2 reward: -1.9820621013641357\n",
      "p3 reward: 0.10022833943367004\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 2., 3.],\n",
      "        [1., 3., 4., 2.],\n",
      "        [2., 4., 2., 2.],\n",
      "        [1., 4., 2., 3.]], device='cuda:0')\n",
      "tensor([[5., 0., 1., 1.],\n",
      "        [2., 2., 5., 4.],\n",
      "        [1., 5., 0., 2.],\n",
      "        [0., 5., 4., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.5819, device='cuda:0')\n",
      "tensor(1.5950, device='cuda:0')\n",
      "p0 reward: 0.8068342208862305\n",
      "p1 reward: 1.1046425104141235\n",
      "p2 reward: -2.3847994804382324\n",
      "p3 reward: 0.4733227789402008\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 2., 3.],\n",
      "        [5., 2., 1., 2.],\n",
      "        [2., 4., 1., 3.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[1., 5., 2., 3.],\n",
      "        [4., 1., 1., 2.],\n",
      "        [2., 5., 1., 3.],\n",
      "        [3., 1., 4., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.6719, device='cuda:0')\n",
      "tensor(1.6246, device='cuda:0')\n",
      "p0 reward: -0.5934736728668213\n",
      "p1 reward: 1.8264479637145996\n",
      "p2 reward: -2.0046133995056152\n",
      "p3 reward: 0.7716390490531921\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 2., 3.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [4., 1., 4., 1.]], device='cuda:0')\n",
      "tensor([[2., 3., 3., 3.],\n",
      "        [1., 3., 3., 3.],\n",
      "        [3., 1., 2., 3.],\n",
      "        [4., 1., 4., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.6801, device='cuda:0')\n",
      "tensor(1.6528, device='cuda:0')\n",
      "p0 reward: -0.28917810320854187\n",
      "p1 reward: 1.4738175868988037\n",
      "p2 reward: -2.2026255130767822\n",
      "p3 reward: 1.0179858207702637\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 1., 4.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [1., 3., 4., 2.],\n",
      "        [3., 1., 3., 3.]], device='cuda:0')\n",
      "tensor([[1., 6., 0., 2.],\n",
      "        [3., 2., 5., 1.],\n",
      "        [1., 2., 3., 3.],\n",
      "        [3., 0., 4., 4.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7009, device='cuda:0')\n",
      "tensor(1.7142, device='cuda:0')\n",
      "p0 reward: 0.7211027145385742\n",
      "p1 reward: 0.3903020918369293\n",
      "p2 reward: -1.2407219409942627\n",
      "p3 reward: 0.12931717932224274\n",
      "====================================================================================================\n",
      "tensor([[3., 0., 4., 3.],\n",
      "        [4., 2., 3., 1.],\n",
      "        [2., 4., 3., 1.],\n",
      "        [1., 4., 2., 3.]], device='cuda:0')\n",
      "tensor([[3., 0., 4., 3.],\n",
      "        [5., 1., 5., 1.],\n",
      "        [2., 3., 3., 1.],\n",
      "        [0., 6., 0., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7165, device='cuda:0')\n",
      "tensor(1.6954, device='cuda:0')\n",
      "p0 reward: -0.0009676514309830964\n",
      "p1 reward: 1.907748818397522\n",
      "p2 reward: -2.1714634895324707\n",
      "p3 reward: 0.26468202471733093\n",
      "====================================================================================================\n",
      "tensor([[3., 3., 1., 3.],\n",
      "        [3., 4., 3., 0.],\n",
      "        [1., 3., 1., 5.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 1., 3.],\n",
      "        [3., 3., 2., 0.],\n",
      "        [2., 4., 2., 5.],\n",
      "        [3., 3., 3., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7533, device='cuda:0')\n",
      "tensor(1.7350, device='cuda:0')\n",
      "p0 reward: 0.895445704460144\n",
      "p1 reward: -0.15738624334335327\n",
      "p2 reward: -1.3747341632843018\n",
      "p3 reward: 0.6366743445396423\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 1., 4.],\n",
      "        [5., 1., 3., 1.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [3., 1., 4., 2.]], device='cuda:0')\n",
      "tensor([[0., 4., 1., 5.],\n",
      "        [5., 1., 3., 2.],\n",
      "        [4., 2., 2., 1.],\n",
      "        [3., 1., 4., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7312, device='cuda:0')\n",
      "tensor(1.7343, device='cuda:0')\n",
      "p0 reward: 0.6780855059623718\n",
      "p1 reward: 1.066667079925537\n",
      "p2 reward: -1.464143991470337\n",
      "p3 reward: -0.28060880303382874\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [0., 3., 2., 5.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [6., 2., 1., 1.]], device='cuda:0')\n",
      "tensor([[3., 2., 3., 3.],\n",
      "        [1., 4., 2., 6.],\n",
      "        [2., 4., 3., 2.],\n",
      "        [4., 0., 0., 1.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7536, device='cuda:0')\n",
      "tensor(1.7481, device='cuda:0')\n",
      "p0 reward: 0.5287041664123535\n",
      "p1 reward: 1.320893406867981\n",
      "p2 reward: -2.675010919570923\n",
      "p3 reward: 0.825413167476654\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 3., 3.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [2., 3., 4., 1.],\n",
      "        [3., 2., 1., 4.]], device='cuda:0')\n",
      "tensor([[1., 5., 3., 3.],\n",
      "        [2., 0., 4., 2.],\n",
      "        [2., 3., 4., 0.],\n",
      "        [3., 2., 1., 5.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7721, device='cuda:0')\n",
      "tensor(1.7676, device='cuda:0')\n",
      "p0 reward: 0.39700502157211304\n",
      "p1 reward: 1.3027981519699097\n",
      "p2 reward: -1.4726322889328003\n",
      "p3 reward: -0.2271709442138672\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 4., 2.],\n",
      "        [3., 4., 1., 2.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[3., 3., 5., 3.],\n",
      "        [2., 4., 1., 3.],\n",
      "        [3., 0., 1., 2.],\n",
      "        [2., 5., 3., 0.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.8142, device='cuda:0')\n",
      "tensor(1.7538, device='cuda:0')\n",
      "p0 reward: 1.5859438180923462\n",
      "p1 reward: 0.3053126335144043\n",
      "p2 reward: -1.566202998161316\n",
      "p3 reward: -0.32505354285240173\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 4., 1.],\n",
      "        [2., 0., 3., 5.],\n",
      "        [2., 5., 3., 0.],\n",
      "        [3., 1., 2., 4.]], device='cuda:0')\n",
      "tensor([[0., 4., 4., 2.],\n",
      "        [1., 0., 4., 4.],\n",
      "        [3., 6., 2., 0.],\n",
      "        [4., 0., 2., 4.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.8453, device='cuda:0')\n",
      "tensor(1.8105, device='cuda:0')\n",
      "p0 reward: 0.7464194297790527\n",
      "p1 reward: 0.7561786770820618\n",
      "p2 reward: -2.0774264335632324\n",
      "p3 reward: 0.5748282074928284\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 6., 0.],\n",
      "        [4., 2., 0., 4.],\n",
      "        [1., 3., 1., 5.],\n",
      "        [3., 3., 1., 3.]], device='cuda:0')\n",
      "tensor([[2., 2., 7., 0.],\n",
      "        [5., 2., 1., 4.],\n",
      "        [1., 3., 0., 5.],\n",
      "        [2., 3., 0., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.8557, device='cuda:0')\n",
      "tensor(1.8496, device='cuda:0')\n",
      "p0 reward: 0.5882946848869324\n",
      "p1 reward: 1.499588131904602\n",
      "p2 reward: -2.2487847805023193\n",
      "p3 reward: 0.16090188920497894\n",
      "====================================================================================================\n",
      "tensor([[2., 1., 1., 6.],\n",
      "        [4., 3., 3., 0.],\n",
      "        [0., 3., 6., 1.],\n",
      "        [4., 5., 0., 1.]], device='cuda:0')\n",
      "tensor([[2., 1., 0., 6.],\n",
      "        [4., 3., 3., 0.],\n",
      "        [0., 3., 6., 2.],\n",
      "        [4., 5., 1., 0.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.8786, device='cuda:0')\n",
      "tensor(1.8662, device='cuda:0')\n",
      "p0 reward: 1.2538825273513794\n",
      "p1 reward: 1.2921254634857178\n",
      "p2 reward: -2.2375123500823975\n",
      "p3 reward: -0.3084958493709564\n",
      "====================================================================================================\n",
      "tensor([[0., 4., 3., 3.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [3., 2., 1., 4.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[0., 3., 3., 5.],\n",
      "        [2., 2., 4., 1.],\n",
      "        [2., 2., 1., 4.],\n",
      "        [4., 3., 2., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9222, device='cuda:0')\n",
      "tensor(1.8842, device='cuda:0')\n",
      "p0 reward: 0.6866477131843567\n",
      "p1 reward: 1.692450761795044\n",
      "p2 reward: -2.922473669052124\n",
      "p3 reward: 0.5433750152587891\n",
      "====================================================================================================\n",
      "tensor([[2., 0., 2., 6.],\n",
      "        [3., 3., 4., 0.],\n",
      "        [2., 4., 1., 3.],\n",
      "        [1., 3., 3., 3.]], device='cuda:0')\n",
      "tensor([[2., 0., 2., 6.],\n",
      "        [3., 3., 4., 0.],\n",
      "        [2., 4., 2., 4.],\n",
      "        [1., 3., 2., 2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9465, device='cuda:0')\n",
      "tensor(1.9364, device='cuda:0')\n",
      "p0 reward: 0.4460943937301636\n",
      "p1 reward: 0.7544902563095093\n",
      "p2 reward: -1.9950040578842163\n",
      "p3 reward: 0.7944190502166748\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 4., 2.],\n",
      "        [5., 2., 2., 1.],\n",
      "        [1., 3., 5., 1.],\n",
      "        [2., 1., 1., 6.]], device='cuda:0')\n",
      "tensor([[3., 1., 1., 1.],\n",
      "        [5., 4., 3., 0.],\n",
      "        [1., 3., 4., 3.],\n",
      "        [1., 0., 4., 6.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9603, device='cuda:0')\n",
      "tensor(1.9643, device='cuda:0')\n",
      "p0 reward: -0.053364429622888565\n",
      "p1 reward: 0.3866819739341736\n",
      "p2 reward: -1.5339230298995972\n",
      "p3 reward: 1.2006053924560547\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [3., 4., 1., 2.],\n",
      "        [1., 4., 3., 2.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[0., 1., 3., 4.],\n",
      "        [2., 4., 1., 3.],\n",
      "        [4., 6., 3., 2.],\n",
      "        [2., 1., 3., 1.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9964, device='cuda:0')\n",
      "tensor(1.9529, device='cuda:0')\n",
      "p0 reward: -0.9289342164993286\n",
      "p1 reward: 2.4890921115875244\n",
      "p2 reward: -2.611898899078369\n",
      "p3 reward: 1.0517410039901733\n",
      "====================================================================================================\n",
      "tensor([[3., 4., 3., 0.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[3., 5., 2., 0.],\n",
      "        [2., 2., 3., 5.],\n",
      "        [1., 3., 3., 2.],\n",
      "        [2., 0., 4., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.0207, device='cuda:0')\n",
      "tensor(2.0213, device='cuda:0')\n",
      "p0 reward: 0.048678331077098846\n",
      "p1 reward: 2.5967087745666504\n",
      "p2 reward: -2.674107074737549\n",
      "p3 reward: 0.02871968224644661\n",
      "====================================================================================================\n",
      "tensor([[2., 4., 1., 3.],\n",
      "        [2., 2., 5., 1.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[2., 4., 1., 3.],\n",
      "        [2., 2., 5., 1.],\n",
      "        [3., 2., 3., 0.],\n",
      "        [3., 2., 3., 4.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.0648, device='cuda:0')\n",
      "tensor(2.0635, device='cuda:0')\n",
      "p0 reward: -0.4201352000236511\n",
      "p1 reward: 1.4596543312072754\n",
      "p2 reward: -2.2377822399139404\n",
      "p3 reward: 1.198263168334961\n",
      "====================================================================================================\n",
      "tensor([[2., 4., 1., 3.],\n",
      "        [4., 2., 1., 3.],\n",
      "        [4., 1., 3., 2.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4., 1., 3.],\n",
      "        [1., 2., 1., 3.],\n",
      "        [4., 1., 3., 2.],\n",
      "        [5., 3., 3., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.1105, device='cuda:0')\n",
      "tensor(2.1021, device='cuda:0')\n",
      "p0 reward: -0.21013520658016205\n",
      "p1 reward: 1.77757728099823\n",
      "p2 reward: -1.737836480140686\n",
      "p3 reward: 0.17039427161216736\n",
      "====================================================================================================\n",
      "tensor([[4., 0., 3., 3.],\n",
      "        [0., 2., 4., 4.],\n",
      "        [4., 4., 0., 2.],\n",
      "        [4., 2., 3., 1.]], device='cuda:0')\n",
      "tensor([[3., 1., 4., 3.],\n",
      "        [1., 2., 4., 4.],\n",
      "        [4., 3., 1., 2.],\n",
      "        [4., 2., 1., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.1495, device='cuda:0')\n",
      "tensor(2.1300, device='cuda:0')\n",
      "p0 reward: -0.500583291053772\n",
      "p1 reward: 2.0872530937194824\n",
      "p2 reward: -2.53301739692688\n",
      "p3 reward: 0.9463474154472351\n",
      "====================================================================================================\n",
      "tensor([[3., 3., 1., 3.],\n",
      "        [4., 3., 3., 0.],\n",
      "        [4., 1., 3., 2.],\n",
      "        [1., 3., 3., 3.]], device='cuda:0')\n",
      "tensor([[3., 3., 2., 3.],\n",
      "        [3., 4., 0., 2.],\n",
      "        [5., 0., 3., 1.],\n",
      "        [1., 3., 5., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.1872, device='cuda:0')\n",
      "tensor(2.1709, device='cuda:0')\n",
      "p0 reward: 0.31911638379096985\n",
      "p1 reward: 1.6442909240722656\n",
      "p2 reward: -2.4488189220428467\n",
      "p3 reward: 0.4854115843772888\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 3., 4., 2.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [5., 2., 2., 1.]], device='cuda:0')\n",
      "tensor([[3., 3., 1., 6.],\n",
      "        [0., 3., 6., 1.],\n",
      "        [1., 1., 3., 1.],\n",
      "        [6., 1., 2., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2112, device='cuda:0')\n",
      "tensor(2.2341, device='cuda:0')\n",
      "p0 reward: -0.4503076672554016\n",
      "p1 reward: 1.5333256721496582\n",
      "p2 reward: -1.6998718976974487\n",
      "p3 reward: 0.6168540120124817\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 1., 5.],\n",
      "        [5., 2., 2., 1.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[0., 3., 0., 6.],\n",
      "        [4., 1., 2., 1.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [6., 2., 4., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2403, device='cuda:0')\n",
      "tensor(2.2567, device='cuda:0')\n",
      "p0 reward: -0.3304083049297333\n",
      "p1 reward: 0.6355059146881104\n",
      "p2 reward: -0.8012704253196716\n",
      "p3 reward: 0.4961729645729065\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 3., 1.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [2., 2., 0., 6.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[4., 4., 3., 1.],\n",
      "        [2., 2., 3., 4.],\n",
      "        [2., 2., 0., 6.],\n",
      "        [2., 0., 4., 1.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2467, device='cuda:0')\n",
      "tensor(2.2823, device='cuda:0')\n",
      "p0 reward: -1.7052052021026611\n",
      "p1 reward: 2.0026824474334717\n",
      "p2 reward: -1.43129563331604\n",
      "p3 reward: 1.1338181495666504\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 3., 1.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [0., 3., 3., 4.]], device='cuda:0')\n",
      "tensor([[4., 2., 3., 1.],\n",
      "        [3., 4., 2., 2.],\n",
      "        [2., 0., 3., 0.],\n",
      "        [1., 4., 4., 5.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2968, device='cuda:0')\n",
      "tensor(2.3286, device='cuda:0')\n",
      "p0 reward: -0.5426797866821289\n",
      "p1 reward: 0.9109838604927063\n",
      "p2 reward: -0.9589069485664368\n",
      "p3 reward: 0.5906028747558594\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [4., 2., 1., 3.],\n",
      "        [3., 4., 2., 1.],\n",
      "        [1., 4., 3., 2.]], device='cuda:0')\n",
      "tensor([[3., 5., 2., 4.],\n",
      "        [3., 0., 1., 1.],\n",
      "        [3., 0., 2., 2.],\n",
      "        [1., 7., 3., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.3677, device='cuda:0')\n",
      "tensor(2.3324, device='cuda:0')\n",
      "p0 reward: -0.3147655427455902\n",
      "p1 reward: 0.8760409951210022\n",
      "p2 reward: -1.198352336883545\n",
      "p3 reward: 0.6370767951011658\n",
      "====================================================================================================\n",
      "tensor([[4., 0., 4., 2.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [2., 6., 1., 1.],\n",
      "        [3., 4., 2., 1.]], device='cuda:0')\n",
      "tensor([[3., 0., 4., 2.],\n",
      "        [3., 3., 4., 5.],\n",
      "        [2., 6., 0., 1.],\n",
      "        [2., 3., 2., 0.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.3897, device='cuda:0')\n",
      "tensor(2.3440, device='cuda:0')\n",
      "p0 reward: -0.8496843576431274\n",
      "p1 reward: 1.6730587482452393\n",
      "p2 reward: -1.3652232885360718\n",
      "p3 reward: 0.5418487191200256\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 3., 2.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [1., 4., 2., 3.],\n",
      "        [4., 2., 3., 1.]], device='cuda:0')\n",
      "tensor([[2., 4., 2., 3.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [0., 4., 2., 3.],\n",
      "        [4., 2., 4., 0.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4111, device='cuda:0')\n",
      "tensor(2.4072, device='cuda:0')\n",
      "p0 reward: -0.7157989740371704\n",
      "p1 reward: 0.8337742686271667\n",
      "p2 reward: -0.7079410552978516\n",
      "p3 reward: 0.5899658203125\n",
      "====================================================================================================\n",
      "tensor([[2., 4., 2., 2.],\n",
      "        [5., 0., 2., 3.],\n",
      "        [1., 1., 5., 3.],\n",
      "        [4., 3., 1., 2.]], device='cuda:0')\n",
      "tensor([[2., 4., 1., 2.],\n",
      "        [4., 0., 3., 2.],\n",
      "        [1., 0., 4., 4.],\n",
      "        [5., 4., 2., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4368, device='cuda:0')\n",
      "tensor(2.4623, device='cuda:0')\n",
      "p0 reward: -1.6488113403320312\n",
      "p1 reward: 2.0016021728515625\n",
      "p2 reward: -0.25549978017807007\n",
      "p3 reward: -0.09729111194610596\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [3., 4., 3., 0.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[0., 1., 5., 4.],\n",
      "        [6., 1., 2., 2.],\n",
      "        [1., 6., 0., 0.],\n",
      "        [3., 4., 3., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4781, device='cuda:0')\n",
      "tensor(2.4540, device='cuda:0')\n",
      "p0 reward: -0.4010578691959381\n",
      "p1 reward: 1.358402132987976\n",
      "p2 reward: -1.2752853631973267\n",
      "p3 reward: 0.31794100999832153\n",
      "====================================================================================================\n",
      "tensor([[2., 4., 2., 2.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[2., 5., 3., 3.],\n",
      "        [2., 3., 5., 3.],\n",
      "        [2., 0., 0., 2.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4681, device='cuda:0')\n",
      "tensor(2.4833, device='cuda:0')\n",
      "p0 reward: -0.15425309538841248\n",
      "p1 reward: 0.30075356364250183\n",
      "p2 reward: -0.9347855448722839\n",
      "p3 reward: 0.7882851958274841\n",
      "====================================================================================================\n",
      "tensor([[3., 3., 2., 2.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [5., 3., 0., 2.],\n",
      "        [0., 0., 6., 4.]], device='cuda:0')\n",
      "tensor([[3., 2., 1., 2.],\n",
      "        [2., 1., 4., 1.],\n",
      "        [5., 4., 0., 3.],\n",
      "        [0., 1., 7., 4.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5039, device='cuda:0')\n",
      "tensor(2.4941, device='cuda:0')\n",
      "p0 reward: -0.5253339409828186\n",
      "p1 reward: 0.5452152490615845\n",
      "p2 reward: -0.8938156962394714\n",
      "p3 reward: 0.8739345073699951\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [3., 2., 0., 5.],\n",
      "        [5., 4., 1., 0.],\n",
      "        [0., 2., 5., 3.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2., 5.],\n",
      "        [2., 3., 1., 3.],\n",
      "        [7., 3., 1., 0.],\n",
      "        [1., 3., 4., 4.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5326, device='cuda:0')\n",
      "tensor(2.4897, device='cuda:0')\n",
      "p0 reward: -0.6204812526702881\n",
      "p1 reward: 2.486571788787842\n",
      "p2 reward: -1.5231965780258179\n",
      "p3 reward: -0.3428940773010254\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [1., 3., 4., 2.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [2., 3., 2., 3.]], device='cuda:0')\n",
      "tensor([[4., 2., 4., 4.],\n",
      "        [0., 3., 4., 2.],\n",
      "        [2., 2., 1., 1.],\n",
      "        [2., 3., 3., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5159, device='cuda:0')\n",
      "tensor(2.5062, device='cuda:0')\n",
      "p0 reward: -0.2860777676105499\n",
      "p1 reward: 1.1071772575378418\n",
      "p2 reward: -1.3651663064956665\n",
      "p3 reward: 0.5440667867660522\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 3., 3.],\n",
      "        [2., 4., 4., 0.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[2., 1., 5., 1.],\n",
      "        [3., 4., 4., 2.],\n",
      "        [0., 1., 2., 2.],\n",
      "        [5., 4., 1., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5057, device='cuda:0')\n",
      "tensor(2.5235, device='cuda:0')\n",
      "p0 reward: 0.1796344518661499\n",
      "p1 reward: 1.1352463960647583\n",
      "p2 reward: -2.136894702911377\n",
      "p3 reward: 0.8220137357711792\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 3., 1.],\n",
      "        [4., 3., 2., 1.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [2., 2., 2., 4.]], device='cuda:0')\n",
      "tensor([[4., 2., 3., 3.],\n",
      "        [5., 3., 2., 3.],\n",
      "        [1., 0., 3., 0.],\n",
      "        [2., 5., 2., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5249, device='cuda:0')\n",
      "tensor(2.4956, device='cuda:0')\n",
      "p0 reward: -0.21101631224155426\n",
      "p1 reward: 0.43437397480010986\n",
      "p2 reward: -1.4959954023361206\n",
      "p3 reward: 1.2726376056671143\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 4., 2.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [4., 0., 1., 5.],\n",
      "        [2., 3., 4., 1.]], device='cuda:0')\n",
      "tensor([[2., 1., 3., 1.],\n",
      "        [2., 4., 4., 5.],\n",
      "        [1., 0., 1., 3.],\n",
      "        [5., 3., 4., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5723, device='cuda:0')\n",
      "tensor(2.5304, device='cuda:0')\n",
      "p0 reward: 0.692308783531189\n",
      "p1 reward: 1.4259309768676758\n",
      "p2 reward: -1.9552441835403442\n",
      "p3 reward: -0.1629956066608429\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 4., 2.],\n",
      "        [4., 2., 3., 1.],\n",
      "        [3., 3., 0., 4.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[6., 4., 6., 2.],\n",
      "        [2., 2., 0., 1.],\n",
      "        [2., 1., 3., 3.],\n",
      "        [2., 1., 1., 4.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5281, device='cuda:0')\n",
      "tensor(2.5149, device='cuda:0')\n",
      "p0 reward: -0.05835932493209839\n",
      "p1 reward: 0.5707694292068481\n",
      "p2 reward: -2.3425679206848145\n",
      "p3 reward: 1.8301576375961304\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [5., 1., 1., 3.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [2., 4., 3., 1.]], device='cuda:0')\n",
      "tensor([[1., 3., 2., 4.],\n",
      "        [6., 0., 2., 3.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [2., 4., 2., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5502, device='cuda:0')\n",
      "tensor(2.5239, device='cuda:0')\n",
      "p0 reward: -1.5246626138687134\n",
      "p1 reward: 0.759523868560791\n",
      "p2 reward: -0.5152464509010315\n",
      "p3 reward: 1.2803852558135986\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 1., 4.],\n",
      "        [4., 2., 1., 3.],\n",
      "        [1., 5., 2., 2.],\n",
      "        [4., 1., 4., 1.]], device='cuda:0')\n",
      "tensor([[5., 1., 1., 4.],\n",
      "        [0., 2., 2., 3.],\n",
      "        [1., 7., 2., 2.],\n",
      "        [6., 0., 3., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.5220, device='cuda:0')\n",
      "tensor(2.4790, device='cuda:0')\n",
      "p0 reward: -0.8465858101844788\n",
      "p1 reward: 1.3596826791763306\n",
      "p2 reward: -1.8886771202087402\n",
      "p3 reward: 1.375580072402954\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 2., 3.],\n",
      "        [1., 1., 4., 4.],\n",
      "        [3., 3., 3., 1.],\n",
      "        [5., 2., 1., 2.]], device='cuda:0')\n",
      "tensor([[0., 2., 4., 2.],\n",
      "        [6., 1., 5., 3.],\n",
      "        [0., 3., 1., 0.],\n",
      "        [6., 2., 0., 5.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4859, device='cuda:0')\n",
      "tensor(2.5322, device='cuda:0')\n",
      "p0 reward: -0.12625247240066528\n",
      "p1 reward: 0.7211022973060608\n",
      "p2 reward: -1.0938040018081665\n",
      "p3 reward: 0.49895399808883667\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 5., 2.],\n",
      "        [1., 3., 2., 4.],\n",
      "        [4., 1., 2., 3.],\n",
      "        [4., 2., 1., 3.]], device='cuda:0')\n",
      "tensor([[1., 3., 5., 2.],\n",
      "        [0., 3., 3., 5.],\n",
      "        [5., 1., 0., 2.],\n",
      "        [4., 1., 2., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4925, device='cuda:0')\n",
      "tensor(2.4875, device='cuda:0')\n",
      "p0 reward: -0.08546086400747299\n",
      "p1 reward: -0.03551873937249184\n",
      "p2 reward: -0.7710844874382019\n",
      "p3 reward: 0.8920638561248779\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[0., 2., 1., 5.],\n",
      "        [0., 1., 1., 4.],\n",
      "        [5., 3., 2., 0.],\n",
      "        [5., 2., 8., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4941, device='cuda:0')\n",
      "tensor(2.4835, device='cuda:0')\n",
      "p0 reward: -0.27687719464302063\n",
      "p1 reward: -0.7883406281471252\n",
      "p2 reward: -1.5919269323349\n",
      "p3 reward: 2.657144546508789\n",
      "====================================================================================================\n",
      "tensor([[2., 0., 5., 3.],\n",
      "        [4., 3., 1., 2.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [2., 3., 2., 3.]], device='cuda:0')\n",
      "tensor([[0., 0., 5., 1.],\n",
      "        [1., 4., 2., 4.],\n",
      "        [7., 3., 1., 4.],\n",
      "        [4., 1., 2., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4750, device='cuda:0')\n",
      "tensor(2.5233, device='cuda:0')\n",
      "p0 reward: 0.4715770483016968\n",
      "p1 reward: 0.4002753496170044\n",
      "p2 reward: -1.052851676940918\n",
      "p3 reward: 0.18099915981292725\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 5., 2.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [3., 2., 1., 4.],\n",
      "        [4., 2., 2., 2.]], device='cuda:0')\n",
      "tensor([[0., 3., 6., 1.],\n",
      "        [3., 3., 4., 4.],\n",
      "        [0., 2., 0., 5.],\n",
      "        [7., 0., 2., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4745, device='cuda:0')\n",
      "tensor(2.5258, device='cuda:0')\n",
      "p0 reward: 0.4884612560272217\n",
      "p1 reward: 0.6420537233352661\n",
      "p2 reward: -1.8977876901626587\n",
      "p3 reward: 0.7672726511955261\n",
      "====================================================================================================\n",
      "tensor([[6., 0., 2., 2.],\n",
      "        [2., 3., 4., 1.],\n",
      "        [2., 4., 1., 3.],\n",
      "        [0., 3., 1., 6.]], device='cuda:0')\n",
      "tensor([[4., 0., 0., 3.],\n",
      "        [0., 2., 4., 1.],\n",
      "        [4., 8., 1., 4.],\n",
      "        [2., 0., 3., 4.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4931, device='cuda:0')\n",
      "tensor(2.4446, device='cuda:0')\n",
      "p0 reward: -0.23694656789302826\n",
      "p1 reward: 0.0064597781747579575\n",
      "p2 reward: -1.1581984758377075\n",
      "p3 reward: 1.3886851072311401\n",
      "====================================================================================================\n",
      "tensor([[1., 5., 3., 1.],\n",
      "        [5., 1., 1., 3.],\n",
      "        [1., 2., 6., 1.],\n",
      "        [3., 4., 0., 3.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 9., 1., 5.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [3., 1., 7., 0.],\n",
      "        [6., 2., 2., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4965, device='cuda:0')\n",
      "tensor(2.4583, device='cuda:0')\n",
      "p0 reward: 0.5067538619041443\n",
      "p1 reward: 1.0010212659835815\n",
      "p2 reward: -1.2180039882659912\n",
      "p3 reward: -0.28977134823799133\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 4., 1.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [2., 1., 4., 3.]], device='cuda:0')\n",
      "tensor([[2., 3., 5., 2.],\n",
      "        [2., 1., 4., 3.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [3., 1., 1., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4537, device='cuda:0')\n",
      "tensor(2.4328, device='cuda:0')\n",
      "p0 reward: -0.778581440448761\n",
      "p1 reward: 0.11368849873542786\n",
      "p2 reward: -1.0333839654922485\n",
      "p3 reward: 1.6982765197753906\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 4., 2.],\n",
      "        [5., 3., 0., 2.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [3., 1., 2., 4.]], device='cuda:0')\n",
      "tensor([[3., 3., 4., 1.],\n",
      "        [6., 4., 0., 2.],\n",
      "        [0., 2., 2., 2.],\n",
      "        [3., 1., 2., 5.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4234, device='cuda:0')\n",
      "tensor(2.4316, device='cuda:0')\n",
      "p0 reward: -0.46469321846961975\n",
      "p1 reward: 0.5689328908920288\n",
      "p2 reward: -0.38057655096054077\n",
      "p3 reward: 0.276336669921875\n",
      "====================================================================================================\n",
      "tensor([[1., 2., 2., 5.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[1., 2., 1., 7.],\n",
      "        [1., 3., 5., 2.],\n",
      "        [1., 5., 2., 3.],\n",
      "        [5., 0., 2., 0.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.4120, device='cuda:0')\n",
      "tensor(2.4002, device='cuda:0')\n",
      "p0 reward: -0.401709645986557\n",
      "p1 reward: 0.9002079367637634\n",
      "p2 reward: -0.6482082605361938\n",
      "p3 reward: 0.14971011877059937\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 3., 2.],\n",
      "        [3., 3., 1., 3.],\n",
      "        [2., 1., 3., 4.],\n",
      "        [3., 1., 3., 3.]], device='cuda:0')\n",
      "tensor([[2., 1., 3., 2.],\n",
      "        [2., 6., 0., 3.],\n",
      "        [1., 1., 4., 4.],\n",
      "        [5., 0., 3., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.3749, device='cuda:0')\n",
      "tensor(2.3945, device='cuda:0')\n",
      "p0 reward: -0.8950095772743225\n",
      "p1 reward: -0.12092975527048111\n",
      "p2 reward: 0.054436832666397095\n",
      "p3 reward: 0.9615024924278259\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 4., 0.],\n",
      "        [1., 5., 1., 3.],\n",
      "        [1., 3., 2., 4.],\n",
      "        [4., 2., 1., 3.]], device='cuda:0')\n",
      "tensor([[2., 3., 4., 0.],\n",
      "        [2., 6., 0., 4.],\n",
      "        [0., 3., 1., 0.],\n",
      "        [6., 0., 3., 6.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.3422, device='cuda:0')\n",
      "tensor(2.3596, device='cuda:0')\n",
      "p0 reward: -0.8581030964851379\n",
      "p1 reward: -0.7524877190589905\n",
      "p2 reward: -0.5381329655647278\n",
      "p3 reward: 2.148723840713501\n",
      "====================================================================================================\n",
      "tensor([[6., 1., 2., 1.],\n",
      "        [1., 1., 2., 6.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [2., 3., 4., 1.]], device='cuda:0')\n",
      "tensor([[3., 4., 2., 3.],\n",
      "        [3., 3., 4., 6.],\n",
      "        [3., 1., 0., 0.],\n",
      "        [3., 0., 4., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.3265, device='cuda:0')\n",
      "tensor(2.3033, device='cuda:0')\n",
      "p0 reward: 0.18410176038742065\n",
      "p1 reward: 0.06933826208114624\n",
      "p2 reward: -0.5867853164672852\n",
      "p3 reward: 0.33334529399871826\n",
      "====================================================================================================\n",
      "tensor([[4., 0., 4., 2.],\n",
      "        [4., 3., 2., 1.],\n",
      "        [2., 3., 4., 1.],\n",
      "        [2., 2., 0., 6.]], device='cuda:0')\n",
      "tensor([[5., 1., 3., 0.],\n",
      "        [2., 4., 1., 1.],\n",
      "        [0., 3., 2., 0.],\n",
      "        [5., 0., 4., 9.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.3131, device='cuda:0')\n",
      "tensor(2.3367, device='cuda:0')\n",
      "p0 reward: 0.3345191776752472\n",
      "p1 reward: 0.6173282861709595\n",
      "p2 reward: -2.142092227935791\n",
      "p3 reward: 1.1902445554733276\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 3., 2.],\n",
      "        [4., 4., 1., 1.],\n",
      "        [1., 1., 5., 3.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[3., 0., 3., 2.],\n",
      "        [0., 7., 3., 0.],\n",
      "        [1., 1., 4., 5.],\n",
      "        [6., 2., 2., 1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.3311, device='cuda:0')\n",
      "tensor(2.3097, device='cuda:0')\n",
      "p0 reward: -0.4826630651950836\n",
      "p1 reward: -0.11399441957473755\n",
      "p2 reward: -0.7684370875358582\n",
      "p3 reward: 1.365094542503357\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 3., 3.],\n",
      "        [1., 2., 4., 3.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [3., 3., 1., 3.]], device='cuda:0')\n",
      "tensor([[0., 0., 1., 2.],\n",
      "        [0., 2., 6., 4.],\n",
      "        [4., 2., 2., 1.],\n",
      "        [6., 4., 3., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2991, device='cuda:0')\n",
      "tensor(2.2903, device='cuda:0')\n",
      "p0 reward: 0.02669430710375309\n",
      "p1 reward: -1.4210234880447388\n",
      "p2 reward: -0.27551379799842834\n",
      "p3 reward: 1.669843077659607\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 0., 6.],\n",
      "        [1., 2., 4., 3.],\n",
      "        [2., 4., 3., 1.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[1., 2., 1., 6.],\n",
      "        [0., 3., 3., 5.],\n",
      "        [3., 2., 0., 0.],\n",
      "        [4., 3., 6., 1.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2593, device='cuda:0')\n",
      "tensor(2.2643, device='cuda:0')\n",
      "p0 reward: 0.3416863679885864\n",
      "p1 reward: -0.3296165466308594\n",
      "p2 reward: -0.8107220530509949\n",
      "p3 reward: 0.7986522316932678\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [1., 4., 4., 1.],\n",
      "        [3., 3., 1., 3.]], device='cuda:0')\n",
      "tensor([[1., 3., 3., 5.],\n",
      "        [5., 5., 5., 0.],\n",
      "        [0., 4., 0., 0.],\n",
      "        [2., 0., 2., 5.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2547, device='cuda:0')\n",
      "tensor(2.2560, device='cuda:0')\n",
      "p0 reward: -0.20397816598415375\n",
      "p1 reward: -0.7519404888153076\n",
      "p2 reward: -0.23384150862693787\n",
      "p3 reward: 1.1897598505020142\n",
      "====================================================================================================\n",
      "tensor([[0., 4., 2., 4.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [3., 2., 2., 3.]], device='cuda:0')\n",
      "tensor([[2., 0., 1., 5.],\n",
      "        [0., 4., 2., 2.],\n",
      "        [3., 2., 3., 1.],\n",
      "        [3., 4., 4., 4.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2335, device='cuda:0')\n",
      "tensor(2.2214, device='cuda:0')\n",
      "p0 reward: -0.18398094177246094\n",
      "p1 reward: -0.9520580172538757\n",
      "p2 reward: 0.2919274568557739\n",
      "p3 reward: 0.8441115617752075\n",
      "====================================================================================================\n",
      "tensor([[3., 3., 2., 2.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [1., 4., 1., 4.]], device='cuda:0')\n",
      "tensor([[1., 2., 2., 3.],\n",
      "        [5., 6., 3., 4.],\n",
      "        [2., 0., 1., 0.],\n",
      "        [2., 4., 2., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2397, device='cuda:0')\n",
      "tensor(2.1925, device='cuda:0')\n",
      "p0 reward: -0.8372830152511597\n",
      "p1 reward: -0.491379052400589\n",
      "p2 reward: 0.3363342583179474\n",
      "p3 reward: 0.9923280477523804\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 3., 1.],\n",
      "        [6., 2., 1., 1.],\n",
      "        [1., 1., 2., 6.],\n",
      "        [1., 3., 4., 2.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 5., 2., 0.],\n",
      "        [9., 2., 8., 1.],\n",
      "        [0., 1., 0., 6.],\n",
      "        [0., 0., 0., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.1932, device='cuda:0')\n",
      "tensor(2.1730, device='cuda:0')\n",
      "p0 reward: -0.8396992683410645\n",
      "p1 reward: 0.10030237585306168\n",
      "p2 reward: -0.04020575061440468\n",
      "p3 reward: 0.7796024680137634\n",
      "====================================================================================================\n",
      "tensor([[2., 1., 4., 3.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [3., 1., 4., 2.],\n",
      "        [1., 6., 2., 1.]], device='cuda:0')\n",
      "tensor([[1., 0., 2., 0.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [3., 6., 8., 6.],\n",
      "        [2., 2., 0., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.2121, device='cuda:0')\n",
      "tensor(2.1914, device='cuda:0')\n",
      "p0 reward: -0.2398170530796051\n",
      "p1 reward: -0.425430566072464\n",
      "p2 reward: 0.10120758414268494\n",
      "p3 reward: 0.5640401840209961\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 1., 4.],\n",
      "        [2., 6., 1., 1.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [2., 1., 4., 3.]], device='cuda:0')\n",
      "tensor([[ 0.,  0.,  0.,  6.],\n",
      "        [ 2., 11.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  0.],\n",
      "        [ 7.,  0.,  6.,  3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(2.1947, device='cuda:0')\n",
      "tensor(2.1717, device='cuda:0')\n",
      "p0 reward: 0.8917691707611084\n",
      "p1 reward: -0.5062952637672424\n",
      "p2 reward: -0.2927408516407013\n",
      "p3 reward: -0.09273315966129303\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 2., 4.],\n",
      "        [4., 4., 1., 1.],\n",
      "        [1., 2., 4., 3.],\n",
      "        [2., 1., 5., 2.]], device='cuda:0')\n",
      "tensor([[2., 2., 1., 3.],\n",
      "        [0., 5., 4., 0.],\n",
      "        [2., 0., 2., 3.],\n",
      "        [6., 1., 5., 4.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.1552, device='cuda:0')\n",
      "tensor(2.1510, device='cuda:0')\n",
      "p0 reward: -0.44862988591194153\n",
      "p1 reward: -0.8780443072319031\n",
      "p2 reward: 0.04681442305445671\n",
      "p3 reward: 1.2798596620559692\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 4., 1.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [2., 5., 1., 2.],\n",
      "        [3., 2., 2., 3.]], device='cuda:0')\n",
      "tensor([[4., 1., 5., 2.],\n",
      "        [0., 5., 3., 2.],\n",
      "        [3., 1., 0., 0.],\n",
      "        [5., 3., 2., 4.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(2.0863, device='cuda:0')\n",
      "tensor(2.1233, device='cuda:0')\n",
      "p0 reward: 0.2796649634838104\n",
      "p1 reward: -1.0549125671386719\n",
      "p2 reward: -0.9934359192848206\n",
      "p3 reward: 1.7686835527420044\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 2., 3.],\n",
      "        [5., 2., 1., 2.],\n",
      "        [1., 2., 4., 3.],\n",
      "        [1., 2., 5., 2.]], device='cuda:0')\n",
      "tensor([[0., 6., 2., 5.],\n",
      "        [2., 0., 0., 3.],\n",
      "        [5., 2., 1., 0.],\n",
      "        [1., 2., 9., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(2.0613, device='cuda:0')\n",
      "tensor(2.1096, device='cuda:0')\n",
      "p0 reward: -0.37445950508117676\n",
      "p1 reward: -0.8337533473968506\n",
      "p2 reward: -0.8568528890609741\n",
      "p3 reward: 2.065065622329712\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 3., 3.],\n",
      "        [2., 3., 4., 1.],\n",
      "        [4., 1., 1., 4.],\n",
      "        [3., 1., 2., 4.]], device='cuda:0')\n",
      "tensor([[ 1.,  4.,  0.,  4.],\n",
      "        [ 1.,  3.,  0.,  0.],\n",
      "        [ 1.,  1.,  0.,  6.],\n",
      "        [ 7.,  0., 10.,  2.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(2.0578, device='cuda:0')\n",
      "tensor(2.0684, device='cuda:0')\n",
      "p0 reward: -1.1130907535552979\n",
      "p1 reward: -1.0849671363830566\n",
      "p2 reward: -0.7069031596183777\n",
      "p3 reward: 2.904961109161377\n",
      "====================================================================================================\n",
      "tensor([[5., 2., 2., 1.],\n",
      "        [2., 4., 4., 0.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[2., 3., 0., 0.],\n",
      "        [2., 4., 7., 0.],\n",
      "        [2., 2., 0., 3.],\n",
      "        [4., 1., 5., 5.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9971, device='cuda:0')\n",
      "tensor(2.0202, device='cuda:0')\n",
      "p0 reward: -0.36118030548095703\n",
      "p1 reward: -0.2224525362253189\n",
      "p2 reward: -0.1161523088812828\n",
      "p3 reward: 0.6997854113578796\n",
      "====================================================================================================\n",
      "tensor([[0., 1., 4., 5.],\n",
      "        [5., 1., 2., 2.],\n",
      "        [2., 3., 4., 1.],\n",
      "        [3., 3., 2., 2.]], device='cuda:0')\n",
      "tensor([[0., 1., 1., 6.],\n",
      "        [5., 5., 5., 2.],\n",
      "        [2., 2., 2., 0.],\n",
      "        [3., 0., 4., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9828, device='cuda:0')\n",
      "tensor(1.9947, device='cuda:0')\n",
      "p0 reward: 0.3111139237880707\n",
      "p1 reward: -0.29942837357521057\n",
      "p2 reward: -1.487585186958313\n",
      "p3 reward: 1.475899577140808\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 2., 4.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [3., 1., 3., 3.],\n",
      "        [3., 3., 4., 0.]], device='cuda:0')\n",
      "tensor([[0., 6., 0., 9.],\n",
      "        [4., 0., 8., 1.],\n",
      "        [2., 1., 0., 0.],\n",
      "        [4., 1., 4., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9376, device='cuda:0')\n",
      "tensor(1.8978, device='cuda:0')\n",
      "p0 reward: -0.10973727703094482\n",
      "p1 reward: 0.1249791607260704\n",
      "p2 reward: -1.144734263420105\n",
      "p3 reward: 1.1294925212860107\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 4., 2.],\n",
      "        [4., 3., 2., 1.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [2., 1., 2., 5.]], device='cuda:0')\n",
      "tensor([[3., 1., 0., 1.],\n",
      "        [1., 1., 7., 1.],\n",
      "        [5., 4., 0., 2.],\n",
      "        [3., 2., 3., 6.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.9171, device='cuda:0')\n",
      "tensor(1.8856, device='cuda:0')\n",
      "p0 reward: 0.5935326218605042\n",
      "p1 reward: -0.792300283908844\n",
      "p2 reward: -0.9195795655250549\n",
      "p3 reward: 1.11834716796875\n",
      "====================================================================================================\n",
      "tensor([[6., 0., 1., 3.],\n",
      "        [4., 2., 2., 2.],\n",
      "        [1., 5., 2., 2.],\n",
      "        [1., 3., 3., 3.]], device='cuda:0')\n",
      "tensor([[5., 1., 0., 7.],\n",
      "        [0., 3., 5., 0.],\n",
      "        [4., 5., 0., 2.],\n",
      "        [3., 1., 3., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.8404, device='cuda:0')\n",
      "tensor(1.8103, device='cuda:0')\n",
      "p0 reward: -0.772705614566803\n",
      "p1 reward: -0.3499442934989929\n",
      "p2 reward: -0.9817558526992798\n",
      "p3 reward: 2.104405641555786\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 5., 1.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [3., 2., 1., 4.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[0., 4., 0., 8.],\n",
      "        [1., 4., 4., 0.],\n",
      "        [4., 2., 0., 0.],\n",
      "        [3., 0., 8., 2.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.8145, device='cuda:0')\n",
      "tensor(1.7504, device='cuda:0')\n",
      "p0 reward: -1.109162449836731\n",
      "p1 reward: 0.18931616842746735\n",
      "p2 reward: -1.1133360862731934\n",
      "p3 reward: 2.033182144165039\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 4., 1.],\n",
      "        [2., 2., 5., 1.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [2., 3., 0., 5.]], device='cuda:0')\n",
      "tensor([[3., 7., 1., 2.],\n",
      "        [0., 3., 8., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [5., 0., 3., 8.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7433, device='cuda:0')\n",
      "tensor(1.7659, device='cuda:0')\n",
      "p0 reward: -1.285563349723816\n",
      "p1 reward: 0.638526201248169\n",
      "p2 reward: -0.14586761593818665\n",
      "p3 reward: 0.7929046154022217\n",
      "====================================================================================================\n",
      "tensor([[2., 0., 3., 5.],\n",
      "        [0., 3., 4., 3.],\n",
      "        [5., 3., 1., 1.],\n",
      "        [3., 2., 2., 3.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 3., 8., 4.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [5., 4., 1., 1.],\n",
      "        [4., 0., 0., 7.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.7237, device='cuda:0')\n",
      "tensor(1.6790, device='cuda:0')\n",
      "p0 reward: -1.5183123350143433\n",
      "p1 reward: 0.3533429205417633\n",
      "p2 reward: -0.10037105530500412\n",
      "p3 reward: 1.2653405666351318\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 2., 3.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [4., 2., 1., 3.]], device='cuda:0')\n",
      "tensor([[0., 2., 3., 4.],\n",
      "        [2., 6., 6., 0.],\n",
      "        [2., 0., 0., 3.],\n",
      "        [8., 0., 1., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.6887, device='cuda:0')\n",
      "tensor(1.7068, device='cuda:0')\n",
      "p0 reward: -2.397963523864746\n",
      "p1 reward: -0.40733078122138977\n",
      "p2 reward: -0.7241709232330322\n",
      "p3 reward: 3.529465436935425\n",
      "====================================================================================================\n",
      "tensor([[4., 3., 2., 1.],\n",
      "        [2., 2., 3., 3.],\n",
      "        [1., 4., 2., 3.],\n",
      "        [3., 1., 1., 5.]], device='cuda:0')\n",
      "tensor([[0., 3., 0., 2.],\n",
      "        [4., 1., 0., 3.],\n",
      "        [0., 6., 0., 0.],\n",
      "        [6., 0., 8., 7.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.6742, device='cuda:0')\n",
      "tensor(1.6428, device='cuda:0')\n",
      "p0 reward: -2.4032442569732666\n",
      "p1 reward: 0.5710827112197876\n",
      "p2 reward: -0.1437370479106903\n",
      "p3 reward: 1.9758983850479126\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [2., 4., 0., 4.],\n",
      "        [3., 3., 2., 2.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[1., 6., 5., 7.],\n",
      "        [1., 6., 0., 1.],\n",
      "        [3., 0., 0., 0.],\n",
      "        [5., 0., 3., 2.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.6476, device='cuda:0')\n",
      "tensor(1.6218, device='cuda:0')\n",
      "p0 reward: -0.714526355266571\n",
      "p1 reward: -0.6993745565414429\n",
      "p2 reward: -0.626972496509552\n",
      "p3 reward: 2.0408732891082764\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [2., 2., 2., 4.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[6., 2., 2., 2.],\n",
      "        [0., 1., 3., 1.],\n",
      "        [1., 1., 0., 1.],\n",
      "        [3., 4., 7., 6.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.5510, device='cuda:0')\n",
      "tensor(1.5498, device='cuda:0')\n",
      "p0 reward: -0.9649944305419922\n",
      "p1 reward: 0.38317009806632996\n",
      "p2 reward: -0.776108980178833\n",
      "p3 reward: 1.3579334020614624\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [3., 3., 1., 3.],\n",
      "        [2., 0., 6., 2.],\n",
      "        [4., 3., 0., 3.]], device='cuda:0')\n",
      "tensor([[0., 4., 5., 2.],\n",
      "        [4., 1., 0., 3.],\n",
      "        [2., 3., 4., 2.],\n",
      "        [6., 0., 1., 3.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.5552, device='cuda:0')\n",
      "tensor(1.5264, device='cuda:0')\n",
      "p0 reward: -2.57129168510437\n",
      "p1 reward: -0.28095880150794983\n",
      "p2 reward: 0.6086764931678772\n",
      "p3 reward: 2.243574380874634\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 3., 3.],\n",
      "        [2., 1., 4., 3.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [7., 1., 1., 1.]], device='cuda:0')\n",
      "tensor([[0., 4., 0., 6.],\n",
      "        [0., 0., 5., 3.],\n",
      "        [3., 3., 2., 0.],\n",
      "        [9., 1., 3., 1.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.5068, device='cuda:0')\n",
      "tensor(1.5079, device='cuda:0')\n",
      "p0 reward: -2.094343900680542\n",
      "p1 reward: -0.1685285121202469\n",
      "p2 reward: 0.4912102520465851\n",
      "p3 reward: 1.7716622352600098\n",
      "====================================================================================================\n",
      "tensor([[3., 5., 1., 1.],\n",
      "        [2., 1., 3., 4.],\n",
      "        [1., 1., 6., 2.],\n",
      "        [2., 3., 2., 3.]], device='cuda:0')\n",
      "tensor([[0., 5., 3., 1.],\n",
      "        [0., 1., 4., 0.],\n",
      "        [4., 2., 1., 6.],\n",
      "        [4., 2., 4., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.4336, device='cuda:0')\n",
      "tensor(1.4327, device='cuda:0')\n",
      "p0 reward: -1.6552319526672363\n",
      "p1 reward: 0.6752021908760071\n",
      "p2 reward: -0.07031950354576111\n",
      "p3 reward: 1.0503491163253784\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [1., 6., 2., 1.],\n",
      "        [4., 3., 1., 2.],\n",
      "        [1., 1., 4., 4.]], device='cuda:0')\n",
      "tensor([[1., 0., 3., 3.],\n",
      "        [0., 4., 2., 0.],\n",
      "        [3., 4., 0., 0.],\n",
      "        [4., 4., 5., 7.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.4439, device='cuda:0')\n",
      "tensor(1.3933, device='cuda:0')\n",
      "p0 reward: -2.4132838249206543\n",
      "p1 reward: 0.09150458127260208\n",
      "p2 reward: 0.2622944712638855\n",
      "p3 reward: 2.0594844818115234\n",
      "====================================================================================================\n",
      "tensor([[3., 5., 1., 1.],\n",
      "        [1., 3., 4., 2.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [4., 1., 2., 3.]], device='cuda:0')\n",
      "tensor([[1., 6., 4., 5.],\n",
      "        [0., 5., 4., 0.],\n",
      "        [2., 0., 0., 0.],\n",
      "        [7., 1., 2., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.3518, device='cuda:0')\n",
      "tensor(1.3249, device='cuda:0')\n",
      "p0 reward: -2.019326686859131\n",
      "p1 reward: -0.044716622680425644\n",
      "p2 reward: -0.20070098340511322\n",
      "p3 reward: 2.264744281768799\n",
      "====================================================================================================\n",
      "tensor([[2., 0., 5., 3.],\n",
      "        [3., 4., 2., 1.],\n",
      "        [1., 4., 1., 4.],\n",
      "        [2., 4., 2., 2.]], device='cuda:0')\n",
      "tensor([[0., 1., 5., 3.],\n",
      "        [5., 4., 0., 2.],\n",
      "        [1., 3., 0., 0.],\n",
      "        [2., 4., 5., 5.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.3532, device='cuda:0')\n",
      "tensor(1.3435, device='cuda:0')\n",
      "p0 reward: -1.9642493724822998\n",
      "p1 reward: 0.26002025604248047\n",
      "p2 reward: -0.19218149781227112\n",
      "p3 reward: 1.8964108228683472\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 2., 4.],\n",
      "        [5., 4., 1., 0.],\n",
      "        [2., 2., 4., 2.],\n",
      "        [3., 2., 3., 2.]], device='cuda:0')\n",
      "tensor([[0., 0., 2., 1.],\n",
      "        [4., 3., 1., 3.],\n",
      "        [5., 4., 0., 2.],\n",
      "        [3., 3., 7., 2.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.3250, device='cuda:0')\n",
      "tensor(1.3476, device='cuda:0')\n",
      "p0 reward: -3.2184598445892334\n",
      "p1 reward: 2.0107877254486084\n",
      "p2 reward: -0.43643635511398315\n",
      "p3 reward: 1.6441082954406738\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [3., 2., 2., 3.],\n",
      "        [3., 2., 3., 2.],\n",
      "        [2., 2., 4., 2.]], device='cuda:0')\n",
      "tensor([[1., 1., 0., 6.],\n",
      "        [0., 2., 8., 0.],\n",
      "        [0., 3., 0., 0.],\n",
      "        [9., 2., 4., 4.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.3085, device='cuda:0')\n",
      "tensor(1.3222, device='cuda:0')\n",
      "p0 reward: -1.5213961601257324\n",
      "p1 reward: 0.07966461777687073\n",
      "p2 reward: -0.1754560023546219\n",
      "p3 reward: 1.6171875\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 3., 1.],\n",
      "        [2., 4., 2., 2.],\n",
      "        [3., 2., 1., 4.],\n",
      "        [1., 4., 4., 1.]], device='cuda:0')\n",
      "tensor([[3., 2., 2., 1.],\n",
      "        [3., 5., 6., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [3., 5., 2., 7.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2974, device='cuda:0')\n",
      "tensor(1.2521, device='cuda:0')\n",
      "p0 reward: -3.847242832183838\n",
      "p1 reward: 1.1561317443847656\n",
      "p2 reward: -1.1415493488311768\n",
      "p3 reward: 3.83266019821167\n",
      "====================================================================================================\n",
      "tensor([[2., 3., 1., 4.],\n",
      "        [5., 0., 3., 2.],\n",
      "        [1., 4., 3., 2.],\n",
      "        [2., 1., 5., 2.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 4.],\n",
      "        [5., 0., 1., 1.],\n",
      "        [1., 0., 1., 2.],\n",
      "        [4., 7., 9., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2206, device='cuda:0')\n",
      "tensor(1.2548, device='cuda:0')\n",
      "p0 reward: -1.8518363237380981\n",
      "p1 reward: 0.33406785130500793\n",
      "p2 reward: -0.29141542315483093\n",
      "p3 reward: 1.8091837167739868\n",
      "====================================================================================================\n",
      "tensor([[4., 2., 2., 2.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [2., 1., 4., 3.],\n",
      "        [3., 3., 3., 1.]], device='cuda:0')\n",
      "tensor([[0., 1., 4., 1.],\n",
      "        [0., 0., 4., 3.],\n",
      "        [4., 0., 0., 1.],\n",
      "        [6., 7., 4., 5.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2031, device='cuda:0')\n",
      "tensor(1.2132, device='cuda:0')\n",
      "p0 reward: -3.1230766773223877\n",
      "p1 reward: 1.4156428575515747\n",
      "p2 reward: 0.6442655920982361\n",
      "p3 reward: 1.0631680488586426\n",
      "====================================================================================================\n",
      "tensor([[4., 4., 1., 1.],\n",
      "        [5., 0., 2., 3.],\n",
      "        [0., 2., 2., 6.],\n",
      "        [1., 4., 3., 2.]], device='cuda:0')\n",
      "tensor([[0., 4., 2., 4.],\n",
      "        [3., 4., 4., 0.],\n",
      "        [3., 2., 0., 4.],\n",
      "        [4., 0., 2., 4.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.2066, device='cuda:0')\n",
      "tensor(1.1461, device='cuda:0')\n",
      "p0 reward: -1.9934444427490234\n",
      "p1 reward: 0.6448596715927124\n",
      "p2 reward: 0.16348761320114136\n",
      "p3 reward: 1.1850969791412354\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 1., 5.],\n",
      "        [0., 2., 5., 3.],\n",
      "        [4., 3., 1., 2.],\n",
      "        [1., 4., 3., 2.]], device='cuda:0')\n",
      "tensor([[4., 0., 0., 7.],\n",
      "        [0., 3., 5., 0.],\n",
      "        [4., 3., 0., 2.],\n",
      "        [0., 4., 5., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.1556, device='cuda:0')\n",
      "tensor(1.1355, device='cuda:0')\n",
      "p0 reward: -2.3855040073394775\n",
      "p1 reward: 1.7905902862548828\n",
      "p2 reward: 0.3808934986591339\n",
      "p3 reward: 0.2140202671289444\n",
      "====================================================================================================\n",
      "tensor([[5., 2., 3., 0.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [2., 4., 0., 4.],\n",
      "        [3., 1., 2., 4.]], device='cuda:0')\n",
      "tensor([[2., 2., 3., 0.],\n",
      "        [5., 3., 4., 0.],\n",
      "        [1., 3., 0., 4.],\n",
      "        [4., 2., 1., 6.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(1.1623, device='cuda:0')\n",
      "tensor(1.0996, device='cuda:0')\n",
      "p0 reward: -2.1793746948242188\n",
      "p1 reward: 0.3881324529647827\n",
      "p2 reward: 0.22243766486644745\n",
      "p3 reward: 1.5688045024871826\n",
      "====================================================================================================\n",
      "tensor([[2., 2., 3., 3.],\n",
      "        [3., 1., 2., 4.],\n",
      "        [4., 3., 2., 1.],\n",
      "        [1., 2., 5., 2.]], device='cuda:0')\n",
      "tensor([[ 2.,  3.,  0.,  5.],\n",
      "        [ 0.,  2.,  0.,  0.],\n",
      "        [ 5.,  2.,  0.,  4.],\n",
      "        [ 3.,  1., 12.,  1.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0755, device='cuda:0')\n",
      "tensor(1.1066, device='cuda:0')\n",
      "p0 reward: -2.765841245651245\n",
      "p1 reward: 2.1839420795440674\n",
      "p2 reward: 0.2820608615875244\n",
      "p3 reward: 0.2998379170894623\n",
      "====================================================================================================\n",
      "tensor([[5., 2., 2., 1.],\n",
      "        [1., 3., 2., 4.],\n",
      "        [1., 4., 2., 3.],\n",
      "        [3., 3., 4., 0.]], device='cuda:0')\n",
      "tensor([[7., 2., 1., 4.],\n",
      "        [3., 5., 3., 0.],\n",
      "        [0., 5., 0., 0.],\n",
      "        [0., 0., 6., 4.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0471, device='cuda:0')\n",
      "tensor(1.0681, device='cuda:0')\n",
      "p0 reward: -1.824989914894104\n",
      "p1 reward: 1.2258447408676147\n",
      "p2 reward: -0.4808584749698639\n",
      "p3 reward: 1.0800037384033203\n",
      "====================================================================================================\n",
      "tensor([[1., 3., 3., 3.],\n",
      "        [3., 1., 1., 5.],\n",
      "        [2., 3., 3., 2.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[0., 4., 3., 3.],\n",
      "        [1., 3., 1., 6.],\n",
      "        [1., 3., 2., 0.],\n",
      "        [6., 0., 4., 3.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0612, device='cuda:0')\n",
      "tensor(1.0810, device='cuda:0')\n",
      "p0 reward: -1.585650086402893\n",
      "p1 reward: 0.3591862618923187\n",
      "p2 reward: 0.7010658383369446\n",
      "p3 reward: 0.5253979563713074\n",
      "====================================================================================================\n",
      "tensor([[3., 1., 3., 3.],\n",
      "        [2., 3., 2., 3.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [2., 4., 3., 1.]], device='cuda:0')\n",
      "tensor([[0., 1., 3., 6.],\n",
      "        [0., 0., 3., 0.],\n",
      "        [4., 9., 0., 2.],\n",
      "        [6., 0., 6., 0.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(1.0256, device='cuda:0')\n",
      "tensor(0.9477, device='cuda:0')\n",
      "p0 reward: 1.3897757530212402\n",
      "p1 reward: -0.42543935775756836\n",
      "p2 reward: -0.9139723181724548\n",
      "p3 reward: -0.050363894551992416\n",
      "====================================================================================================\n",
      "tensor([[4., 1., 3., 2.],\n",
      "        [3., 3., 0., 4.],\n",
      "        [1., 5., 2., 2.],\n",
      "        [2., 3., 3., 2.]], device='cuda:0')\n",
      "tensor([[1., 5., 5., 7.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [7., 6., 0., 0.],\n",
      "        [2., 0., 3., 3.]], device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "*****\n",
      "tensor(0.9881, device='cuda:0')\n",
      "tensor(0.9918, device='cuda:0')\n",
      "p0 reward: -2.2968251705169678\n",
      "p1 reward: 1.1477413177490234\n",
      "p2 reward: -0.6411746144294739\n",
      "p3 reward: 1.7902581691741943\n",
      "====================================================================================================\n",
      "tensor([[3., 2., 3., 2.],\n",
      "        [3., 3., 1., 3.],\n",
      "        [3., 2., 4., 1.],\n",
      "        [3., 1., 2., 4.]], device='cuda:0')\n",
      "tensor([[ 0.,  6.,  3.,  2.],\n",
      "        [ 0.,  0.,  0.,  0.],\n",
      "        [ 2.,  0.,  1.,  1.],\n",
      "        [10.,  2.,  6.,  7.]], device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "*****\n",
      "tensor(0.9675, device='cuda:0')\n",
      "tensor(0.9349, device='cuda:0')\n",
      "p0 reward: -0.7909664511680603\n",
      "p1 reward: 0.23218940198421478\n",
      "p2 reward: 0.42489364743232727\n",
      "p3 reward: 0.13388341665267944\n",
      "====================================================================================================\n",
      "tensor([[5., 2., 2., 1.],\n",
      "        [1., 4., 2., 3.],\n",
      "        [2., 2., 1., 5.],\n",
      "        [2., 2., 3., 3.]], device='cuda:0')\n",
      "tensor([[3., 4., 0., 7.],\n",
      "        [1., 5., 4., 0.],\n",
      "        [1., 0., 0., 1.],\n",
      "        [5., 1., 4., 4.]], device='cuda:0')\n",
      "tensor(1, device='cuda:0')\n",
      "tensor(3, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8916, device='cuda:0')\n",
      "tensor(0.9037, device='cuda:0')\n",
      "p0 reward: -1.1604022979736328\n",
      "p1 reward: -0.9912593960762024\n",
      "p2 reward: -0.40931665897369385\n",
      "p3 reward: 2.5609781742095947\n",
      "====================================================================================================\n",
      "tensor([[1., 4., 1., 4.],\n",
      "        [1., 4., 5., 0.],\n",
      "        [3., 1., 4., 2.],\n",
      "        [3., 1., 2., 4.]], device='cuda:0')\n",
      "tensor([[0., 6., 0., 7.],\n",
      "        [0., 4., 2., 0.],\n",
      "        [7., 0., 1., 0.],\n",
      "        [1., 0., 9., 3.]], device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n",
      "tensor(2, device='cuda:0')\n",
      "*****\n",
      "tensor(0.8662, device='cuda:0')\n",
      "tensor(0.8672, device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1333777/957506036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mppo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mmemory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1333777/1372387200.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;31m# take gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################\n",
    "\n",
    "num_assets=4\n",
    "num_agents=4\n",
    "num_steps=16\n",
    "\n",
    "state_dim = num_assets * num_agents * 4 + num_assets + 1\n",
    "# FLATTENED ORDER BOOK + CURRENT ALLOC + CASH\n",
    "\n",
    "action_dim = num_assets * 4\n",
    "# ORDER BOOK\n",
    "\n",
    "action_std = 0.5 # constant std for action distribution (Multivariate Normal)\n",
    "# K_epochs = 80               # update policy for K epochs\n",
    "K_epochs = 4               # update policy for K epochs\n",
    "\n",
    "eps_clip = 0.2              # clip parameter for PPO\n",
    "gamma = 0.95                # discount factor\n",
    "\n",
    "lr = 0.0002                 # parameters for Adam optimizer\n",
    "betas = (0.9, 0.999)\n",
    "\n",
    "\n",
    "max_episodes = 10000\n",
    "batch_size = 8000\n",
    "random_seed = None\n",
    "# num_steps = 100\n",
    "#############################################\n",
    "\n",
    "# creating environment\n",
    "env = FiggieBatched(batch_size, num_steps)\n",
    "\n",
    "memory = [Memory() for _ in range(num_agents)]\n",
    "ppo = [PPO(state_dim, action_dim, action_std, lr, betas, gamma, K_epochs, eps_clip) for _ in range(num_agents)]\n",
    "\n",
    "print(lr,betas)\n",
    "\n",
    "# training loop\n",
    "\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    state = env.reset()\n",
    "    print(env.cur_alloc[0])\n",
    "\n",
    "    running_rewards = torch.zeros(batch_size, num_agents).to(device)\n",
    "    \n",
    "    last_reward = 0\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "\n",
    "        # Running policy_old:\n",
    "        asdf = []\n",
    "        for i in range(num_agents):\n",
    "            asdf.append(ppo[i].policy_old.act(state[i], memory[i]))\n",
    "        \n",
    "\n",
    "        actions = torch.stack([torch.stack(a.split(num_assets, dim=-1),dim=-1) for a in asdf], dim=1)\n",
    "        state, reward, _ = env.step(actions)\n",
    "        running_rewards += reward\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            memory[i].rewards.append(reward[:,i])\n",
    "\n",
    "    print(env.cur_alloc[0])\n",
    "    print(env.goal_suit[0])\n",
    "    print(env.suit_shuff[0,0])\n",
    "    print(\"*\"*5)\n",
    "    print(torch.gather(env.order_book, 2, env.goal_suit[:,None,None,None].repeat(1,4,1,4)).squeeze(2)[:,:,0].mean())\n",
    "    print(torch.gather(env.order_book, 2, env.suit_shuff[:,0:1,None,None].repeat(1,4,1,4)).squeeze(2)[:,:,0].mean())\n",
    "\n",
    "#     print(env.order_book[:,:,env.goal_suit,0].mean())\n",
    "#     print(env.order_book[:,:,env.suit_shuff[:,0],0].mean())\n",
    "\n",
    "#     print((env.order_book[0,:,env.goal_suit[0].item(),:].mean(dim=-1))\n",
    "#     print(env.order_book[0,:,env.goal_suit[0].item(),:])\n",
    "#     print(env.order_book[0,:,env.suit_shuff[0,0].item(),:])\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        ppo[i].update(memory[i])\n",
    "        memory[i].clear_memory()\n",
    "        \n",
    "    for i in range(num_agents):\n",
    "        print(f\"p{i} reward: {running_rewards[:,i].mean()}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65d46459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 4, 8000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.order_book[:,:,env.goal_suit,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2ffdbdce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9656, device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(env.order_book, 2, env.goal_suit[:,None,None,None].repeat(1,4,1,4)).squeeze(2)[:,:,0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d84aeafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0738,  1.7250, -1.1109, -1.0061],\n",
       "        [ 1.4805, -0.2583, -1.0393, -1.1753],\n",
       "        [-1.5975,  0.6800,  0.2453, -2.4835],\n",
       "        [ 4.6622, -0.2883, -1.0331, -3.1572]], device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.order_book[0,:,env.goal_suit[0],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14f4f9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8000, 4, 4, 4])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.order_book.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c590a800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 3,  ..., 2, 1, 3], device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.goal_suit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c798620f",
   "metadata": {},
   "source": [
    "# COMPARE TO PYTHON CPU IMPLEMENTATION AND SORT IMPLEMNTATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95befb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TradingBatchedSort:\n",
    "    \"\"\"\n",
    "    THIS SEEMS SLOWER ON ALL N AND HAS SAME SPEEDS ON PLOTS, BUT IN THEORY HAS BETTER ASYMPTOTICS.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, num_assets, num_agents, num_steps):\n",
    "        self.b = batch_size\n",
    "        self.num_assets = num_assets\n",
    "        self.num_agents = num_agents\n",
    "        self.num_steps = num_steps\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.true_values = 5.0 * torch.normal(mean=0, std=1, size=(self.b, self.num_assets)).to(device) # N(0, 1)\n",
    "        self.stds = 3.0 * torch.sigmoid(torch.normal(mean=0, std=1, size=(self.b, self.num_agents, self.num_assets))).to(device) # N(0, 1)\n",
    "        self.estimates = torch.normal(mean=self.true_values.unsqueeze(1), std=self.stds).to(device)\n",
    "        \n",
    "        self.cash = torch.zeros(self.b, self.num_agents).to(device)\n",
    "        self.cur_alloc = torch.ones(self.b, self.num_agents, self.num_assets).to(device) * 10000 # everyone just starts w/ one of everythign\n",
    "        \n",
    "        self.order_book = torch.zeros(self.b, self.num_agents, self.num_assets, 4).to(device) # TODO: Attention over assets?\n",
    "        self.t = 0\n",
    "        \n",
    "        return self._naive_states()\n",
    "    \n",
    "    def _naive_states(self):\n",
    "        states = []\n",
    "        for i in range(self.num_agents):\n",
    "#             order_book = self.order_book.clone()\n",
    "#             order_book[:,0], order_book[:,i] = order_book[:,i], order_book[:,0] # AGENT ALWAYS FIRST\n",
    "            states.append(torch.cat([\n",
    "                self.order_book.flatten(start_dim=1),\n",
    "                self.cash[:,i].unsqueeze(-1),\n",
    "                self.cur_alloc[:,i],\n",
    "                self.estimates[:,i],\n",
    "                self.stds[:,i],\n",
    "            ], dim=-1))\n",
    "\n",
    "        return states\n",
    "    \n",
    "    def step(self, actions_bna4, debug=False):\n",
    "        \n",
    "        self.order_book = actions_bna4.clone()\n",
    "        \n",
    "        # TODO: REFACTOR WHERE YOU SPLIT BY COLUMNS. EASIER TO READ, MAYBE FASTER.\n",
    "        \n",
    "        # PRIORIY BASED ON BIDDING\n",
    "        curr_order_book = actions_bna4.clone()\n",
    "        \n",
    "        # CANT SELL MORE THAN YOU HAVE. I DONT CARE IF YOU GET MORE OVER THE COURSE OF TRADING.\n",
    "        curr_order_book[:,:,:,3] = torch.min(curr_order_book[:,:,:,3], self.cur_alloc)\n",
    "        # LATER THIS SHOULDNT BE NEEDED\n",
    "        curr_order_book[:,:,:,2:] = torch.round(curr_order_book[:,:,:,2:])\n",
    "\n",
    "        info = []\n",
    "        agent_indices = torch.arange(self.num_agents).repeat_interleave(self.num_assets).view(1,self.num_agents,self.num_assets,1).repeat(self.b,1,1,1).to(device)\n",
    "        curr_order_book = torch.cat([curr_order_book, agent_indices], dim=-1).transpose(1,2)\n",
    "        \n",
    "        bid_indices = torch.argsort(curr_order_book[:,:,:,0], dim=-1)\n",
    "        ask_indices = torch.argsort(curr_order_book[:,:,:,1], dim=-1)\n",
    "\n",
    "        bid_sort = torch.gather(curr_order_book, 2, bid_indices.unsqueeze(-1).repeat(1,1,1,5))\n",
    "        ask_sort = torch.gather(curr_order_book, 2, ask_indices.unsqueeze(-1).repeat(1,1,1,5))\n",
    "\n",
    "        bid_ptr = torch.zeros(self.b,self.num_assets).long().to(device) + self.num_agents - 1\n",
    "        ask_ptr = torch.zeros(self.b,self.num_assets).long().to(device) \n",
    "\n",
    "        while True:\n",
    "            highest_row = torch.gather(bid_sort, 2, bid_ptr.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,5)).squeeze(2)\n",
    "            inc = torch.logical_and(highest_row[:,:,2] == 0, bid_ptr > 0)\n",
    "            # Skip 0 volume bids\n",
    "            while torch.any(inc):\n",
    "                bid_ptr -= inc.long()\n",
    "                highest_row = torch.gather(bid_sort, 2, bid_ptr.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,5)).squeeze(2)\n",
    "                inc = torch.logical_and(highest_row[:,:,2] == 0, bid_ptr > 0)\n",
    "            highest_bids_ba = highest_row[:,:,0]\n",
    "            highest_bidders = highest_row[:,:,-1].long()\n",
    "\n",
    "            lowest_row = torch.gather(ask_sort, 2, ask_ptr.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,5)).squeeze(2)\n",
    "            dec = torch.logical_and(lowest_row[:,:,3] == 0, ask_ptr < self.num_agents - 1)\n",
    "            # Skip 0 volume asks\n",
    "            while torch.any(dec):\n",
    "                ask_ptr += dec.long()\n",
    "                lowest_row = torch.gather(ask_sort, 2, ask_ptr.unsqueeze(-1).unsqueeze(-1).repeat(1,1,1,5)).squeeze(2)\n",
    "                dec = torch.logical_and(lowest_row[:,:,3] == 0, ask_ptr < self.num_agents - 1)\n",
    "            lowest_asks_ba = lowest_row[:,:,1]\n",
    "            lowest_askers = lowest_row[:,:,-1].long()            \n",
    "\n",
    "            # Break if all lowest asks are higher than all highest bids across all batches and assets\n",
    "            if torch.all(torch.logical_or(highest_bids_ba < lowest_asks_ba, torch.logical_or(lowest_row[:,:,3] <= 0, highest_row[:,:,2] <= 0))):\n",
    "                break\n",
    "            \n",
    "            num_trades = torch.where(\n",
    "                lowest_asks_ba < highest_bids_ba, \n",
    "                torch.min(highest_row[:,:,2], lowest_row[:,:,3]),\n",
    "                torch.zeros_like(highest_row[:,:,2])\n",
    "            )\n",
    "\n",
    "            cash_diff = (num_trades * highest_bids_ba).unsqueeze(-1)\n",
    "\n",
    "            highest_cash_diff = torch.zeros_like(self.cash.unsqueeze(1).repeat(1,self.num_assets,1)).scatter_(-1,highest_bidders.unsqueeze(-1),cash_diff).sum(1)\n",
    "            lowest_cash_diff = torch.zeros_like(self.cash.unsqueeze(1).repeat(1,self.num_assets,1)).scatter_(-1,lowest_askers.unsqueeze(-1),cash_diff).sum(1)\n",
    "\n",
    "            self.cash -= highest_cash_diff\n",
    "            self.cash += lowest_cash_diff\n",
    "            \n",
    "            # TODO: CLEAN\n",
    "            highest_alloc_diff = torch.zeros_like(self.cur_alloc).scatter_(1, highest_bidders.unsqueeze(1).repeat(1,self.num_agents,1), num_trades.unsqueeze(1).repeat(1,self.num_agents,1))\n",
    "            lowest_alloc_diff = torch.zeros_like(self.cur_alloc).scatter_(1, lowest_askers.unsqueeze(1).repeat(1,self.num_agents,1), num_trades.unsqueeze(1).repeat(1,self.num_agents,1))\n",
    "            self.cur_alloc += highest_alloc_diff\n",
    "            self.cur_alloc -= lowest_alloc_diff\n",
    "            \n",
    "            bid_sort[:,:,:,2].scatter_(-1, bid_ptr.unsqueeze(-1), -num_trades.unsqueeze(-1), reduce=\"add\")\n",
    "            ask_sort[:,:,:,3].scatter_(-1, ask_ptr.unsqueeze(-1), -num_trades.unsqueeze(-1), reduce=\"add\")\n",
    "            if debug:\n",
    "                info.append(\n",
    "                    {\n",
    "                        \"num_trades\": num_trades,\n",
    "                        \"lowest_asks\": lowest_asks_ba,\n",
    "                        \"cash_diff\": cash_diff,\n",
    "                        \"highest_alloc_diff\": highest_alloc_diff,\n",
    "                        \"lowest_alloc_diff\": lowest_alloc_diff,\n",
    "                        \"curr_order_book\": curr_order_book.clone()\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        self.t += 1\n",
    "        if self.t == self.num_steps:\n",
    "            reward = (self.true_values.unsqueeze(1) * self.cur_alloc).sum(dim=-1) + self.cash\n",
    "        else:\n",
    "            reward = torch.zeros(self.b, self.num_agents).to(device)\n",
    "        \n",
    "        return self._naive_states(), reward, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbef85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd8c348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f36629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "22c0978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trades(a, s_id, d_id, vol, price, trades, type):\n",
    "    trade_s = {\n",
    "        'id': s_id,\n",
    "        'price': price,\n",
    "        'asset': a,\n",
    "        'cleared_vol': -1.0 * vol,\n",
    "        'cleared_cash': 1.0 * price * vol,\n",
    "        'instr': type,\n",
    "    }\n",
    "    trade_d = {\n",
    "        'id': d_id,\n",
    "        'price': price,\n",
    "        'asset': a,\n",
    "        'cleared_vol': 1.0 * vol,\n",
    "        'cleared_cash': -1.0 * price * vol,\n",
    "        'instr': type,\n",
    "    }\n",
    "    trades.append(trade_s)\n",
    "    trades.append(trade_d)\n",
    "    return trades\n",
    "\n",
    "def clear_orders(order_book):\n",
    "    # Getting rid of non-existent orders\n",
    "    for o in order_book[:]:\n",
    "        if isinstance(o, type(None)):\n",
    "            order_book.remove(o)\n",
    "    # Getting rid of orders with price and/or vol equal to 0\n",
    "    for o in order_book[:]:\n",
    "        if abs(float(o['price'])) == 0.0:\n",
    "            order_book.remove(o)\n",
    "    for o in order_book[:]:\n",
    "        if abs(float(o['vol'])) == 0.0:\n",
    "            order_book.remove(o)\n",
    "\n",
    "    trades = []\n",
    "    assets_trading = list(set([o['asset'] for o in order_book if o]))\n",
    "    for a in assets_trading:\n",
    "        supply = [o for o in order_book if ((o['asset'] == a) and (o['side'] == 'ask') and (o['vol'] > 0.0))]\n",
    "        demand = [o for o in order_book if ((o['asset'] == a) and (o['side'] == 'bid') and (o['vol'] > 0.0))]\n",
    "\n",
    "        # Skip asset's trading session if neither supply nor demand is found\n",
    "        if supply == []:\n",
    "            continue\n",
    "        if demand == []:\n",
    "            continue\n",
    "\n",
    "        # Sorting by price (supply - increasing price; demand - decreasing price)\n",
    "        supply = sorted(supply, key=lambda x: x['price'], reverse=False)\n",
    "        demand = sorted(demand, key=lambda x: x['price'], reverse=True)\n",
    "\n",
    "        supply[0]['agg_vol'] = supply[0]['vol']\n",
    "        demand[0]['agg_vol'] = demand[0]['vol']\n",
    "\n",
    "        for ii in range(1, len(supply)):\n",
    "            supply[ii]['agg_vol'] = supply[ii - 1]['agg_vol'] + supply[ii]['vol']\n",
    "        for ii in range(1, len(demand)):\n",
    "            demand[ii]['agg_vol'] = demand[ii - 1]['agg_vol'] + demand[ii]['vol']\n",
    "        max_vol = min(supply[-1]['agg_vol'], demand[-1]['agg_vol'])  # Works\n",
    "        segments = sorted(list(set([el['agg_vol'] for el in supply if el['agg_vol'] <= max_vol] + \\\n",
    "                                   [el['agg_vol'] for el in demand if el['agg_vol'] <= max_vol])))\n",
    "        if segments == []:\n",
    "            continue\n",
    "\n",
    "        # Set the auction to boolean and use in while loop to stop auction when supply or demand has been depleted\n",
    "        auction_live = True\n",
    "        while auction_live:\n",
    "            if len(supply) < 1:\n",
    "                auction_live = False\n",
    "                continue\n",
    "            if len(demand) < 1:\n",
    "                auction_live = False\n",
    "                continue\n",
    "            # Auction is live, iterate by highest --> lowest buyer bid\n",
    "            d_vol = demand[0]['vol']\n",
    "            d_id = demand[0]['id']\n",
    "            buyer_bid = demand[0]['price']\n",
    "            d_type = demand[0]['instr']\n",
    "            buyer_satisfied = False\n",
    "            while not buyer_satisfied:\n",
    "                # Iterate thru supply until (a) buyer's bidding appetite is satisfied or (b) there's no more supply\n",
    "                \n",
    "                \"\"\"\n",
    "                NOTE: THIS IS WHERE THE SELF-TRADE IS HANDLED\n",
    "                \"\"\"\n",
    "                curr_supply = [s for s in supply ] # if s['id'] != d_id]\n",
    "                \n",
    "                \n",
    "                if len(curr_supply) == 0:\n",
    "                    buyer_satisfied = True\n",
    "                    demand.remove(demand[0])  # since the only seller is itself, we remove it from buyers\n",
    "                    continue\n",
    "                inner_id = supply.index(curr_supply[0])\n",
    "                s_vol = curr_supply[0]['vol']\n",
    "                s_id = curr_supply[0]['id']\n",
    "                asking_price = curr_supply[0]['price']\n",
    "\n",
    "                # Bidder wants to pay lower than asking price\n",
    "                if buyer_bid < asking_price:\n",
    "                    # Skip sellers' placements if the buyer will only bid for less\n",
    "                    # Since the bids are in order, this will cancel the auction\n",
    "                    auction_live = False\n",
    "                    buyer_satisfied = True\n",
    "                    break\n",
    "                elif d_vol > s_vol:\n",
    "                    # If demand is greater than the single seller's supply, we remove supply element\n",
    "                    trade_vol = s_vol\n",
    "                    d_vol -= s_vol\n",
    "                    trades = get_trades(a, s_id, d_id, trade_vol, buyer_bid, trades, d_type)\n",
    "                    supply.remove(supply[inner_id])\n",
    "                elif d_vol < s_vol:\n",
    "                    # If demand is less than the single seller's supply, we subtract the volume sought by the bidder and\n",
    "                    # remove the buyer's demand from the order book. Buyer = satisfied.\n",
    "                    trade_vol = d_vol\n",
    "                    supply[inner_id]['vol'] -= trade_vol\n",
    "                    demand.remove(demand[0])\n",
    "                    buyer_satisfied = True\n",
    "                    trades = get_trades(a, s_id, d_id, trade_vol, buyer_bid, trades, d_type)\n",
    "                elif d_vol == s_vol:\n",
    "                    # If buyer's demand is equal to the seller's supply, remove both. Buyer = satisfied.\n",
    "                    trade_vol = s_vol\n",
    "                    demand.remove(demand[0])\n",
    "                    supply.remove(supply[inner_id])\n",
    "                    buyer_satisfied = True\n",
    "                    trades = get_trades(a, s_id, d_id, trade_vol, buyer_bid, trades, d_type)\n",
    "\n",
    "    return trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f56f445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING CUDA\n",
      "100\n",
      "Pytorch GPU: 0.126820087\n",
      "Pytorch GPU Sort: 0.142751178\n",
      "STARTING CUDA\n",
      "99\n",
      "Pytorch GPU: 0.120966279\n",
      "Pytorch GPU Sort: 0.140845651\n",
      "STARTING CUDA\n",
      "98\n",
      "Pytorch GPU: 0.120196418\n",
      "Pytorch GPU Sort: 0.139414124\n",
      "STARTING CUDA\n",
      "97\n",
      "Pytorch GPU: 0.122624791\n",
      "Pytorch GPU Sort: 0.139496849\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1817757/2558674502.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m#         asset = random.choice(list(range(num_assets)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0masset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnum_assets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mvol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vol_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax_price_ask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#         reg = \"a{}\".format(na)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_randbelow_with_getrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/engs-multi-agent-learning/exet5589/myenv/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mistop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mistart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mistart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty range for randrange() (%d, %d, %d)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "cpu_times = []\n",
    "cuda_times = []\n",
    "cuda_sort_times = []\n",
    "\n",
    "for tests in range(100,0,-1):\n",
    "#     tests = 100\n",
    "    n_iterations = 100\n",
    "    num_assets = 10\n",
    "    max_bid_orders = 10 * tests \n",
    "    max_ask_orders = 10 * tests\n",
    "    min_bid_orders = max_bid_orders\n",
    "    min_ask_orders = max_ask_orders\n",
    "    max_vol_ask = 100\n",
    "    max_price_ask = 100\n",
    "    max_price_bid = 100\n",
    "    max_vol_bid = 100\n",
    "    order_books = []\n",
    "\n",
    "    # generate an order list\n",
    "    delta_t__lk_lst = []\n",
    "    delta_t__cc_lst = []\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "\n",
    "\n",
    "\n",
    "        n_orders_bid = random.randint(min_bid_orders, max_bid_orders)\n",
    "        n_orders_ask = random.randint(min_ask_orders, max_ask_orders)\n",
    "\n",
    "        order_book_bids__lk = []\n",
    "        order_book_asks__lk = []\n",
    "    #     order_book_bids__cc = []\n",
    "    #     order_book_asks__cc = []\n",
    "        for nb in range(n_orders_bid):\n",
    "    #         asset = \"oil\" if random.random() > 0.5 else \"gas\"\n",
    "    #         asset = random.choice(list(range(num_assets)))\n",
    "            asset = nb % num_assets\n",
    "            vol = random.randint(0, max_vol_bid)\n",
    "            price = random.uniform(0, 1) * max_price_bid\n",
    "    #         reg = \"b{}\".format(nb)\n",
    "            reg = nb // num_assets\n",
    "            instr = {0: \"cash\", 1: \"credit\", 2: \"leverage\"}[random.randint(0, 2)]\n",
    "            #tdp_instr = {\"cash\": tdp.CASH, \"credit\": tdp.CREDIT, \"leverage\": tdp.LEVERAGE}[instr]\n",
    "            order_book_bids__lk.append({\"asset\": asset, \"side\": \"bid\", \"vol\": vol, \"price\": price, \"id\": reg,\n",
    "                                      \"instr\": instr}) # instr: cash, credit, leverage\n",
    "            #order_book_bids__cc.append(tdp.Order(tdp.BID, reg, asset, tdp_instr, vol, price))\n",
    "\n",
    "        for na in range(n_orders_ask):\n",
    "    #         asset = \"oil\" if random.random() > 0.5 else \"gas\"\n",
    "    #         asset = random.choice(list(range(num_assets)))\n",
    "            asset = na % num_assets\n",
    "            vol = random.randint(0, max_vol_ask)\n",
    "            price = random.uniform(0, 1) * max_price_ask\n",
    "    #         reg = \"a{}\".format(na)\n",
    "            reg = na // num_assets\n",
    "            order_book_asks__lk.append({\"asset\": asset, \"side\": \"ask\", \"vol\": vol, \"price\": price, \"id\": reg,\n",
    "                                          \"instr\": \"cash\"})\n",
    "          #order_book_asks__cc.append(tdp.Order(tdp.ASK, reg, asset, tdp.CASH, vol, price))\n",
    "\n",
    "        order_book__lk = order_book_bids__lk + order_book_asks__lk\n",
    "        order_books.append(order_book__lk)\n",
    "      #order_book__cc = order_book_bids__cc + order_book_asks__cc\n",
    "\n",
    "    #     order_book_np = np.zeros((1, max_bid_orders // num_assets, num_assets, 4))\n",
    "    #     for i in range(n_iterations):\n",
    "    #         for d in order_books[i]:\n",
    "    #             if d['side'] == 'bid':\n",
    "    #                 order_book_np[i][d['id']][d['asset']][0] = d['price']\n",
    "    #                 order_book_np[i][d['id']][d['asset']][2] = d['vol']\n",
    "    #             else:\n",
    "    #                 order_book_np[i][d['id']][d['asset']][1] = d['price']\n",
    "    #                 order_book_np[i][d['id']][d['asset']][3] = d['vol']\n",
    "\n",
    "    #     order_book = torch.FloatTensor(order_book_np).to(device)\n",
    "    #     env = TradingBatched(1, 4, 4, 8)\n",
    "    #     env.reset()\n",
    "    #     _, r, info = env.step(order_book, debug=True)\n",
    "\n",
    "#         t1 = time.time_ns()\n",
    "#         tradelist__lk = clear_orders(copy.deepcopy(order_book__lk))\n",
    "#         t2 = time.time_ns()\n",
    "\n",
    "#         delta_t__lk = t2 - t1\n",
    "#         delta_t__lk_lst.append(delta_t__lk)\n",
    "\n",
    "    order_book_np = np.zeros((n_iterations, max_bid_orders // num_assets, num_assets, 4))\n",
    "    for i in range(n_iterations):\n",
    "        for d in order_books[i]:\n",
    "            if d['side'] == 'bid':\n",
    "                order_book_np[i][d['id']][d['asset']][0] = d['price']\n",
    "                order_book_np[i][d['id']][d['asset']][2] = d['vol']\n",
    "            else:\n",
    "                order_book_np[i][d['id']][d['asset']][1] = d['price']\n",
    "                order_book_np[i][d['id']][d['asset']][3] = d['vol']\n",
    "    print(\"STARTING CUDA\")\n",
    "    order_book = torch.FloatTensor(order_book_np).to(device)\n",
    "    env = TradingBatched(n_iterations, 10, tests, 8)\n",
    "    env.reset()\n",
    "\n",
    "    t1 = time.time_ns()\n",
    "    _, r, info = env.step(order_book, debug=False)\n",
    "    t2 = time.time_ns()\n",
    "\n",
    "\n",
    "    env2 = TradingBatchedSort(n_iterations, 10, tests, 8)\n",
    "    env2.reset()\n",
    "\n",
    "    t21 = time.time_ns()\n",
    "    _, r, info = env2.step(order_book, debug=False)\n",
    "    t22 = time.time_ns()\n",
    "\n",
    "\n",
    "        #order_book = tdp.OrderBook(False) # (False)\n",
    "        #order_book.insert_batch(order_book__cc)\n",
    "    #     t1 = time.time_ns()\n",
    "    #     #tradelist__cc = order_book.calc_trades()\n",
    "    #     t2 = time.time_ns()\n",
    "        #delta_t__cc = t2 - t1\n",
    "        #delta_t__cc_lst.append(delta_t__cc)\n",
    "\n",
    "        # ensure trade lists generated are equal\n",
    "    #     trade_set__lk = set()\n",
    "    #     for t in tradelist__lk:\n",
    "    #         trade_set__lk.add((t[\"id\"], t[\"asset\"], t[\"cleared_vol\"], t[\"cleared_cash\"]))\n",
    "\n",
    "      #trade_set__cc = set()\n",
    "      #for t in tradelist__cc[\"bid\"]:\n",
    "      #    trade_set__cc.add((t.id, t.asset, t.cleared_volume, t.cleared_cash))\n",
    "      #for t in tradelist__cc[\"ask\"]:\n",
    "      #    trade_set__cc.add((t.id, t.asset, t.cleared_volume, t.cleared_cash))\n",
    "\n",
    "      #diffset = trade_set__lk.symmetric_difference(trade_set__cc)\n",
    "\n",
    "      #try:\n",
    "      #    self.assertEqual(diffset, set(), \"unequal lengths!: {} | {}\".format(order_book__lk, order_book__cc))\n",
    "      #except Exception as e:\n",
    "      #    # Debugger point\n",
    "      #    raise(e)\n",
    "      #    pass\n",
    "\n",
    "    # TODO: Timers for both methods!!!\n",
    "    import numpy as np\n",
    "    print(tests)\n",
    "#     print(f\"Python CPU: {np.sum(delta_t__lk_lst) / (10 ** 9)}\")\n",
    "    print(f\"Pytorch GPU: {(t2 - t1) / (10 ** 9)}\")\n",
    "    print(f\"Pytorch GPU Sort: {(t22 - t21) / (10 ** 9)}\")\n",
    "#     cpu_times.append(np.sum(delta_t__lk_lst) / (10 ** 9))\n",
    "    cuda_times.append((t2 - t1) / (10 ** 9))\n",
    "    cuda_sort_times.append((t22 - t21) / (10 ** 9))\n",
    "\n",
    "# print(\"Time per loop (LK): {} s +- {} s\".format(np.sum(delta_t__lk_lst)/ (10 ** 9), np.std(delta_t__lk_lst) / (10 ** 9)))\n",
    "#print(\"Time per loop (CC): {} s +- {} s\".format(np.mean(delta_t__cc_lst)/ (10 ** 9), np.std(delta_t__cc_lst) / (10 ** 9)))\n",
    "# print(\"Time per loop (LK): {} s +- {} s\".format(np.mean(delta_t__lk_lst)/ (10 ** 9), np.std(delta_t__lk_lst) / (10 ** 9)))\n",
    "\n",
    "#print(\"Speedup: {}\".format(np.mean(delta_t__lk_lst) / np.mean(delta_t__cc_lst)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "194a1350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAEICAYAAAB25L6yAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA2BElEQVR4nO3deXgV5dn48e99tuyEAAHDGpB9X8LixuKKFLUutfZFxX2p/lxqrfq2tWqrba31tW+tfeuGioq7tHVpVUSRqiAgyC4IAcKaELLnrPP8/phJPMSEBMjJOUnuz3XNlXNm5szcz0zOPc95ZuYZMcaglFIqcbniHYBSSqlD00StlFIJThO1UkolOE3USimV4DRRK6VUgtNErZRSCU4TtVJHQURyRcSIiOcIPjtURJbFIq4mrDtfRE5tpmWtFZGpzbGs5iAib4jI9HjH0Zw0UUcRkY9E5ICIJLXgOo2I9D+M+buLSEGdcS0St4jcIyLPx3Id9ayzp4i8LiJFIlIqIqtF5LKWjCGGfg08VPPGSZ7VIlLh7M+3RaRXUxZ0NAeMJizbJyJ/FJECJ7atIvI/NdONMcOMMR818zp/7ezrsIjcU8/0/xKRbSJSKSLzRaRT1OTfAfc3ZzzxponaISK5wEmAAc6ObzSHNAP4V82bVhT3kZoL7AD6AJ2BS4G9cY2oGYhIDjANmF9n0lnGmHQgB7ucf27h0OpzF5AHTAAysOP+Msbr3Az8DHi77gQRGQb8DbgE6AZUAY/VTDfGLAU6iEhejGNsOcYYHey7M+8G/gM8DLxVZ1pn4J9AGfAF8BtgcdT0wcD7QDGwEbgwatozwF+w/+HKgSXAsc60RdgJthKoAH4IdAHeAkqc5X0CuKKW9wZwXhPjngGsc9a7E/ipM77BdQDdgdeBQmArcJMzfjoQBEJOrKuc8ZcBW5x1bAVm1bNtuwPVQKeocWOAIsDbyH6pAEYfYvok4FOnLKuAqVHTOgFzgF3AAWB+1LSrsZNBMfAPoHvUNANcB2xyPvcXQJxpbuxacJFT7huc+T1N3R7OfJcCH9QZlw+cWmf/fR31/nvYCbIM++B1T9S07U4cFc5wXFQ51zvxrAPGRq3rp8BXQCnwMpDcQKxvAbccYh/Uxu3sh5oYKp2Ycp1pM4GVzjyfAiOb8L18PrqczrgHgBej3h+L/b+ZETXuCeBX8c4rzTXEPYBEGZwv7Y+BcdjJqFvUtJecIRUY6nxJFjvT0pz3lwMeYKzzJR7mTH/GSQYTnOkvAC9FLdsA/aPe/xb4P8DrDCdFJQmvs+yMJsa9GzjJeZ0V9SWtdx3Yv7CWYyd/H9APO+mc4XzuHuD5qOWnYSeNQc77nJpy17N9PwSujnr/B+D/mrBfPsA+EF0E9K4zrQewHzuhuYDTnPfZzvS3sRNQllPOKc74k53tOBZIwq61LqqzT94COgK9sQ9a051p1wEbgF7YB4KFzvyew9wefwD+UmdcPt8mvFTgWeC5qOlTgRFOWUdi17i/70zLJeqA4Yz7AfYBeryzf/sDfaLWtRT7INoJO5lf10Csv8A+EPzYWb80FHed8Q9gV0a8zrbeB0zEPtjNdj6X1Mj+ry9R/x24o864CmBc1PufAG/EO6801xD3ABJhAE7ETnJdnPcbgFud125n2qCo+Wtr1Ni14E/qLO9vOEdz7ET9ZNS0GcCGqPd1E/V9zj9i/3riPAVY0JS4nffbgWuBDnWWU+86nC/R9jrj7gLmOK/v4buJugQ4H0hpZBtfBXzovBbsg9vkJuybLOw2x7VABLtGNt6Zdgcwt878/3aSQA5gAVn1LPMp4MGo9+nOdsyN2icnRk1/BbjTef0hUQkNOJ2DE3VTt8cTwO/qjMt3Ek4JEMb+JTDiEMt4BPgf53Uu303U/wZubuCz+cDFUe8fpIEDJ/Z34AbsA2bAiWt2nWWdWuczP3TG1xw0/wr8us48G3EOnocoY32JegF1DirYB6SpUe+vrvl/awuDtlHbZgPvGWOKnPcvOuMAsrG/hDui5o9+3QeYKCIlNQMwCzgmap49Ua+rsBNDQ/6AXUt+T0S2iMidUdNmAO80MW6wE8YMYJuIfCwixzWyjj5A9zpl+W/sdsDvMMZUYn8hrwN2Oye/BjdQrteA40SkOzAZO6l8cojtULOOA8aYO40xw5w4VgLzRUSceH9QJ94TsZN0L6DYGHOgnsV2B7ZFraMCuybeI2qehvZZdw7e/9HLOZztcQC7vbeu7xtjOmLX9G8EPhaRYwBEZKKILBSRQhEpddbTpYHlg70NvjnE9Cb9XxpjIsaYvxhjTsD+lXE/8LSIDKlvfhEZAzwKnGuMKXRG9wFuq7OvemFvz8NVAXSoM64DdvNOjQzsA16b0O4TtYikABcCU0Rkj4jsAW4FRonIKOyfvWGgZ9THos/E7wA+NsZ0jBrSjTHXH0k8xphyY8xtxph+wFnAT0TkFGfyDJyTK02IG2PMF8aYc4Cu2CetXmlkHTuArXXKkmGMmVETXj3x/tsYcxp2ctyAXVOsr1wlwHtOzP8FzDNO1ecwtk0Rdvtwzc/1Hdg16uh404wxv3OmdRKRjvUsahd24gBARNKwz0PsbEIYuzl4//euE2OTtgd22/DAhlbiJMc3sH9FnOiMfhG7Pb2XMSYTu/lKaj5Sz2J2YLffNhtjTLUx5i/YB5qhdaeLSDbwJnCjMebLOrHcX2dfpRpj5h1BGGuBUVHr7Id9YPs6ap4h2Ocs2oR2n6iB72N/GYYCo51hCHZt71JjTAT7BN49IpLq1JAujfr8W8BAEblERLzOML6h2kY99mK3BQMgIjNFpL9TYyxzYouISF/s9rwNTYnbuaRqlohkGmNCUctqcB3YbZZlInKHiKSIiFtEhovI+KhYc0XE5Synm4ic7SS6AHZNJ3KIsr7obLvzndeNEpHfOzF4RCQDuB7YbIzZj/2z+CwROcOJNVlEpopIT2PMbuBd4DERyXL2y+SoOC4XkdFiX9L4ALDEGJPfhJBeAW4S+7LBLKD2F89hbo/3gbEiktxAuUVEzsFu+lnvjM7A/pXgF5EJ2Ae8GoXYTT39osY9CfxURMY5y+svIn04TCJyi7NdU5z9MNuJ5cs683mwT0S/YIx5uc5ingCuc34ViIikicj3nH1a3zq9zrZxAR5n37qdyS9g7/eTnG19H3Z7dHSNegr2/m8b4t32Eu8B+1K3P9Yz/kLsn4Ye7OaPt/n2qo/fc3Bb8SBneiH2T+gPca5UwG6j/k3UvFOBgqj312HX0kqcdd6K3bZXCRQAv3TmuxF49DDi9jnzHIiK+0RnnnrX4UzrDsxzlnEA+JxvT3B1BhY741dg1xo/xr5qoAT4CBh6iG2dgv3zdG3UuN7YCa13A5/5M/bVFxXO9n0LGBI1faITQ7Ez/e2aZWHXup/FPsAcIOrkkrPdv3E+9xbQM2pa3fMGtfvQ+X/4H2c/byXqqo8j2B6vAj+Mep+PfXVMhbOd1hB11QhwAXZTS7kT86McfM7gPmcblACTosq50VnmGmBM1LqirzC5J3pZdeK8Fvskc025lgIz68R9Kt+2k9dcxVQz1OyP6dj/hyXY//OvEnVivM46n3GWFT1cFjX9v7DPwVRin2+JvqJoPPBlvHNLcw41VxOowyAivweOMcbMbnTm5lvnO9iJ+p1GZ1atgogMxT6QTDD6RWw2IvI68FRb+q5oom4Cp7nDB6zGPlq/A1xljJnfgjH8DPizMaa6pdaplEoMmqibwGmjnYfdLLAP+/K732ktSCnVEjRRK6VUgtOrPpRSKsE1e09bAF26dDG5ubmxWLRSSrVJy5cvLzLGZNc3LSaJOjc3l2XL4tLNrlJKtUoisq2hadr0oZRSCU4TtVJKJThN1EopleBi0kZdn1AoREFBAX6/v6VWqRzJycn07NkTr9cb71CUUkegxRJ1QUEBGRkZ5ObmYvcFpFqCMYb9+/dTUFBA37594x2OUuoItFjTh9/vp3PnzpqkW5iI0LlzZ/0lo1Qr1qJt1Jqk40O3u1Ktm55MVEqpZrBg/V6e/GQLEav5u+XQRH0IDzzwQO3r/Px8hg8fHsdolFKJ7NVlBTz32Tbcrub/BauJ+hCiE7VSSjXEGMPS/GLG53aKyfLbTaLOz89n8ODBzJ49m5EjR3LBBRdQVVXFggULOPfcc2vne//99znvvPO48847qa6uZvTo0cyaNQuASCTC1VdfzbBhwzj99NOprra7hl65ciWTJk1i5MiRnHvuuRw4YD9PderUqdxxxx1MmDCBgQMH8skn9T/L9cEHH2TEiBGMGjWKO++8s/azt9xyC8cffzzDhw9n6dKlANxzzz089NBDtZ8dPnw4+fn5zb69lFJN901hBcWVQSb2jU2ibrHL86Ld+8+1rNtV1qzLHNq9A786a9gh59m4cSNPPfUUJ5xwAldccQWPPfYYt912GzfccAOFhYVkZ2czZ84cLr/8cs466yweffRRVq5cCdiJftOmTcybN48nnniCCy+8kNdff52LL76YSy+9lD//+c9MmTKFu+++m3vvvZdHHnkEgHA4zNKlS3nnnXe49957+eCDDw6K6d1332X+/PksWbKE1NRUiouLa6dVVlby6aefsmjRIq644grWrFnTrNtMKdU8lmy1v7cTYpSo202NGqBXr16ccMIJAFx88cUsXrwYEeGSSy7h+eefp6SkhM8++4wzzzyz3s/37duX0aNHAzBu3Djy8/MpLS2lpKSEKVOmADB79mwWLVpU+5nzzjvvoPnr+uCDD7j88stJTU0FoFOnb3f0j370IwAmT55MWVkZJSUlR1V+pVRsLN1aTNeMJPp0To3J8uNSo26s5hsrdS9Tq3lfU4NOTk7mBz/4AR5P/ZslKSmp9rXb7a5t+jiUms+43W7C4fB3phtjGrx8rr54PR4PlmXVjtPro5WKL2MMS7YUM6Fvp5hdCtuuatTbt2/ns88+A2DevHmceOKJAHTv3p3u3bvzm9/8hssuu6x2fq/XSygUOuQyMzMzycrKqm1/njt3bm3tuilOP/10nn76aaqqqgAOavp4+eWXAVi8eDGZmZlkZmaSm5vLihUrAFixYgVbt25t8rqUUs2v4EA1e8r8MWufhjjVqONlyJAhPPvss1x77bUMGDCA66+/vnbarFmzKCwsZOjQobXjrrnmGkaOHMnYsWO5//77G1zus88+y3XXXUdVVRX9+vVjzpw5TY5p+vTprFy5kry8PHw+HzNmzKi92iQrK4vjjz+esrIynn76aQDOP/98nnvuOUaPHs348eMZOHDg4W4GpVQz+rZ9unPM1hGTZybm5eWZug8OWL9+PUOGDGn2dTVVfn4+M2fObPCE3I033siYMWO48sorWziy+k2dOpWHHnqIvLy8ZllevLe/Um3Vz15bxXvr9rLiF6fhOoprqEVkuTGm3i98u6pRN2TcuHGkpaXxxz/+Md6hKKVamaVb7eunjyZJN6bdJOrc3NwGa9PLly9v4Wga99FHH8U7BKVUI/aW+cnfX8XFk/rEdD3t6mSiUko1p6Uxvn66hiZqpZQ6AhWBMK+vKCDN52ZoToeYrqvdNH0opVRzWbhxH794cw27Squ57bSBeNyxrfM2OVGLiBtYBuw0xsyMXUhKKZWY/KEIv5y/hleXF9C/azqvXXcc4/rEttkDDq/p42ZgfawCSTTN2XPe1KlTqXu5olKqddlb5ueHj3/Oq8sLuHFaf96+6cQWSdLQxEQtIj2B7wFPxjacxHG4idoYc9Ct3UqptmPVjhLO+vNiNu8t52+XjOOnZwwiyeNusfU3tUb9CPAzoMFMJCLXiMgyEVlWWFjYHLE1q/q6OX377beb3MXpww8/zPDhwxk+fHhtz3j5+fkMGTKEH//4x4wdO5YdO3bU22UpwKuvvtpod6dKqcR026ur8LpdvPHjEzhj2DEtvv5G26hFZCawzxizXESmNjSfMeZx4HGw70w85ELfvRP2rD6sQBt1zAg483eHnKVuN6fr1q1j/fr1jXZxunz5cubMmcOSJUswxjBx4kSmTJlCVlYWGzduZM6cOTz22GOH7LK0se5OlVKJqaQqyOZ9Fdx+xiAGHZMRlxiaUqM+AThbRPKBl4CTReT5mEYVI3W7Of3Pf/7TpC5OFy9ezLnnnktaWhrp6emcd955tbXiPn36MGnSJODQXZY21t2pUioxrSooBWB0r45xi6HRGrUx5i7gLgCnRv1TY8zFR7XWRmq+sVJft6FN6eL0UP2hpKWlHTRfQ90cNtbdqVIqMa3aUYIIjOiZGbcY2tUNL/V1c9qULk4nT57M/PnzqaqqorKykjfffJOTTjrpO8s/VJelSqnWadWOEo7NTqdDsjduMRxWojbGfNSar6Gu6eZ05MiRFBcX13ZzOmvWLHr16lVvF6ezZs1i7NixXHbZZUyYMIGJEydy1VVXMWbMmO8sf/r06Zx99tnk5eUxevTog55tqJRqfYwxrNxRwqieHeMah3ZzSuJ1cRoL8d7+SrVGO4qrOOnBhfz6+8O5JMYdL2k3p4egXZwqpRqyqqAEgNFxrlG3m0TdUDenidjFqVIqMazcXoLP44rbZXk12tXJRKWUOhyrCkoY3r0DPk98U6UmaqWUqkcoYrF6Zymj4nj9dA1N1EopVY+v95bjD1lxvdGlhiZqpZSqx6od8b8jsYYm6kNobV2dPvLII7U32yiljs7KHQfISvXSu1NqvEPRRH0oramr00gkoolaqWZiWYZl+QcY1atjg91CtKR2k6jr6+a0qqqKBQsWJExXp7t372by5MmMHj2a4cOH184zb948RowYwfDhw7njjjtq509PT+fuu+9m4sSJ3H///ezatYtp06Yxbdq0WGxCpdqNF5duZ0tRJWeP6h7vUIA4XUf9+6W/Z0PxhmZd5uBOg7ljwh2HnKduN6ePPfYYt912GzfccENCdHX64osvcsYZZ/Dzn/+cSCRCVVUVu3bt4o477mD58uVkZWVx+umnM3/+fL7//e9TWVnJ8OHDue+++wB4+umnWbhwIV26dGnWbatUe7Kn1M/v393A8cd25twxPeIdDtCOatTw3W5OFy9ejIgkTFen48ePZ86cOdxzzz2sXr2ajIwMvvjiC6ZOnUp2djYej4dZs2axaNEiwO6J7/zzz2++DaSU4p5/rCUYsXjg3BEJ0ewBcapRN1bzjZX6ujkFEqar08mTJ7No0SLefvttLrnkEm6//XY6dGj4MfTJycm43S33OCCl2rp/r93Dv9bu4WfTB5HbJa3xD7SQdlWjrq+bUyBhujrdtm0bXbt25eqrr+bKK69kxYoVTJw4kY8//piioiIikQjz5s1jypQp9X4+IyOD8vLyJq9PKfWtPaV+fjl/DYOPyeDqk/rFO5yDtJu+PuDbbk6vvfZaBgwYUNvNKdhdnRYWFtbb1enYsWN54YUXars6BWq7Oq3bhDF9+nRWrlxJXl4ePp+PGTNmNPnqkY8++og//OEPeL1e0tPTee6558jJyeG3v/0t06ZNwxjDjBkzOOecc+r9/DXXXMOZZ55JTk4OCxcuPMyto1T7Ve4PcdmcpVQFIzx7xWi87sSqw2o3p4623tVpvLe/UokqFLG44pkv+Oyb/Tx92XgmD8yOSxzazWkjtKtTpdqvn7+5mk82FfHgBSPjlqQb024SdUPdnIJ2dapUe7V4UxGvLCvghmnHcmFer3iH06AWbYiJRTOLapxud6W+yxjDg//eQI+OKdx0yoB4h3NILZaok5OT2b9/vyaNFmaMYf/+/SQnJ8c7FKUSyr/W7OGrglJuPW0gSZ7Evsy1xZo+evbsSUFBAYWFhS21SuVITk6mZ8+e8Q5DqYQRjlj84b2NDOianjB3Hx5KiyVqr9dL3759W2p1SinVoNdXFLClsJK/XTIOtysx7j48lMS6WFAppWJsR3EVj3ywidG9OnL60G7xDqdJ2s1VH0qp9s0Yw0tf7OA3b61DRPjLrKEJ05dHYzRRK6XaPH8owvXPL2fhxkKOP7YzD14wkp5Z8X8gQFNpolZKtXmvLS9g4cZCfvG9IVxxQl9craBdOpomaqVUm2aM4ZlP8xnRI5MrT+zbapo7ounJRKVUm/afzfvZvK+Cy47PbZVJGjRRK6XakD2lfv68YBMVgW/7e3/m0610Sfcxc1ROHCM7Otr0oZRqE/yhCNfMXcZXBaV8sqmIZ64YT1F5kAUb9nHjtP4Jf/fhoWiiVkq1esYYfjF/DV8VlDL7uD7M/XwbVz27jH7ZabhFmDWxT7xDPCqaqJVSrd7zn2/jteUF3HRyf35y+iBG9uzIT19bxaff7OesUd05JrN193WjiVop1aqt2VnKvf9cx7RB2dxy6kAAzh/Xk1DE4rfvbuDqk1p/1xWaqJVSrdqbX+7E7RIe+eGYg66PvmhCby7M69Xqrpmuj171oZRq1T7fsp8xvTuSmer9zrS2kKRBE7VSqhUrrQ6xbncZE/t2jncoMdVoohaRZBFZKiKrRGStiNzbEoEppVRjluUXYwxM6te2E3VT2qgDwMnGmAoR8QKLReRdY8znMY5NKaUO6fMt+/G5XYzp3THeocRUo4na2M/OqnDeep1Bn6ellIq7JVuLGd2rI8ne1nszS1M0qY1aRNwishLYB7xvjFkS06iUUqoR5f4Qa3aWMqlfp3iHEnNNStTGmIgxZjTQE5ggIsPrziMi14jIMhFZps9FVEo1t61Flby7enft+2XbDmAZmNjG26fhMK/6MMaUAB8B0+uZ9rgxJs8Yk5ednd080SmllOP+t9dx/QsreH/dXsBun/a6hbG9s+IcWew15aqPbBHp6LxOAU4FNsQ4LqWUqlVSFeTjrwtxCdz+2ip2l1azZEsxo3p2JMXXttunoWk16hxgoYh8BXyB3Ub9VmzDUkqpb727Zg+hiOGRi8YQDFvc8MIKVu8sZWI7aJ+Gpl318RUwpgViUUqpev195U76dUnjrJE5hCMWP3llFUCbv9Glht6ZqJRKaHtK/SzZWszZo7sjIpw3tifnj+1Jms/NuD5tv30atFMmpVSCe+urXRgDZ4/qXjvuwQtGcsf0QaQltY8UpjVqpVRC+/vKXYzokUm/7PTacW6X0LVD6+5j+nBoolZKJawthRWs3ll6UG26PdJErZRKWH9fuQsRWvWDaZuDJmqlVEKqCoaZ+/k2Jg/IJiczJd7hxJUmaqVUQpr72TaKK4PcdMqAeIcSd5qolVIJpyoY5m+LtjB5YHa7uQTvUDRRK6USTk1t+pZTtTYNmqiVUgmmpjY9ZWB2u+hwqSk0USulEsqzn9q16Zu1Nl1LE7VSKmFs3lfOnxZ8zSmDu2ptOoomaqVUQgiEI/y/eStJ9Xn47Xkj4h1OQmkfN8orpRLe79/dyPrdZTw1O69d3R7eFFqjVkrF3cKN+3j6P1u57PhcThnSLd7hJBxN1EqpuNpaVMmtL69kULcM7jxzcLzDSUiaqJVScXOgMsjlc5biEuHxS8eR7G37j9U6EtpGrZSKi0A4wjVzl7Gr1M+8qyfSp3NavENKWFqjVkq1OGMMd76+mi/yD/DQD0Yxrk/7ePbhkdJErZRqcX9ZuJk3v9zJbacNbPd9TTeFJmqlVIt6Z/VuHnrva74/ujs3ntw/3uG0CpqolVIt5quCEn7yykrG9u7I784fiYjEO6RWQRO1UqpFlFQFuXbucjqnJfG3S/L0Co/DoFd9KKVaxC//vpbC8gBv/vgEsjOS4h1Oq6I1aqVUzP195U7+uWoXt542kBE9M+MdTqujiVopFVO7Sqr5xfw1jOuTxbWT+8U7nFZJE7VSKmZCEYvbXlmFZRkevnAUHremnCOhbdRKqZgIRSxumvcln23Zzx9/MErvPDwKenhTSjW7cMTi1pdX8u6aPdw9cyjnj+sZ75BaNU3USqlmZVmG21/7ire+2s1/zxjMFSf2jXdIrZ4maqVUs/rtu+t588ud/PT0gVwz+dh4h9MmaKJWSjWbZ/6zlSc+sR8AcMM0vT28uWiiVko1i/fW7uHet9Zx2tBu/HLmUL09vBlpolZKHbUv8ou56aUvGdmzI/970RjcLk3SzUkTtVLqqKwuKOWKOV/QvWMKT83OI8WnfXg0N03USqkj9vXeci59egkdUry8cNVEuqRrHx6xoIlaKXVEdhRXcfGTS/C6Xbx49URyMlPiHVKb1WiiFpFeIrJQRNaLyFoRubklAlNKJa7iyiCzn15KIGzx/FX6vMNYa8ot5GHgNmPMChHJAJaLyPvGmHUxjk0plYCqgxGuevYLCkqqefGqiQzslhHvkNq8RmvUxpjdxpgVzutyYD3QI9aBKaUST8Qy3PzSl3y5o4T/vWg0ebn6UNqWcFht1CKSC4wBltQz7RoRWSYiywoLC5spPKVUoohYhttfW8V76/byq5lDmT48J94htRtNTtQikg68DtxijCmrO90Y87gxJs8Yk5ednd2cMSql4syyDHe8/hVvrNjJT04byGUnaP8dLalJiVpEvNhJ+gVjzBuxDUkplUgsy3DnG1/x2vICbj5lADedMiDeIbU7jZ5MFPs+0KeA9caYh2MfklIqUZT5Q9z2yireX7eX/3dyf245VZN0PDTlqo8TgEuA1SKy0hn338aYd2IWlVIq7tbvLuP655dTcKCau2cO5fITcrX/jjhpNFEbYxYDuneUakc+WLeXG+etoEOyl5eumaRXd8SZPopLKXWQjzbu48cvrGBITgZPzM6ja0ZyvENq9zRRK6Vqfbq5iGvnLmdAt3Seu2IimaneeIek0L4+lFKOJVv2c9Vzy8jtnMbcKzVJJxJN1Eop/rlqF5c8tZSczGTmXjWBTmm+eIekomjTh1LtmDGGxxdt4bfvbmB8bhZPXJpHx1RN0olGE7VS7ZRlGe57ax3PfJrP90bm8McfjCLZq53+JyJN1Eq1Q8GwxW2vruKfq3Zx1Yl9+e8ZQ3Dp47MSliZqpdqZykCY655fziebirjzzMFcN+XYeIekGqGJWql2pLQqxGXPLGXVjhIePH8kF47vFe+QVBNoolaqnSiqCHDJU0v5Zl8Fj80ax/Thx8Q7JNVEmqiVagd2lVRz8ZNL2FVazZOz85g8ULsibk00USvVxm0tquTiJ5dQVh1i7pUTGa/9drQ6mqiVasPW7Srj0qeXYhnDi1dPYkTPzHiHpI6AJmql2qjl24q5fM4XpCV5mHvlJPp3TY93SOoIaaJWqo0xxvD859v49Vvr6ZGVwtwrJ9AzKzXeYamjoIlaqTakIhDmrjdW889Vu5g2KJuHLxxNlvbb0eppolaqjdi4p5zrX1hOflElt58xiOunHKt3G7YRmqiVagNeW17AL+avJj3Jy/NXTeT4Y7vEOyTVjDRRK9WKBcIRfvX3tbz0xQ4m9evE//5ojD6RpQ3SRK1UK7Wv3M91c5ezYnsJN07rz62nDcStTR1tkiZqpVqh1QWlXDN3GSVVIf46ayxnjsiJd0gqhjRRK9XK/GPVLn722io6pyXx2vXHMay73sTS1mmiVqqViFiGh97byF8/+obxuVn89eJxdElPindYqgVoolaqFSj3h7jlpZUs2LCPH03ozb1nD8Pn0UeetheaqJVKcDuKq7jq2WVsLqzgvnOGccmkPojoScP2RBO1UglsWX4x185dTihi8ezlEzhxgF4f3R5polYqAUUswxOfbOHh976mR1YKT87O49hs7VSpvdJErVSC2VJYwU9fXcWK7SWcMawbvz9/JB1Ttb+O9kwTtVIJwhjD80u2c//b60jyuPnTRaM5e1R3bY9WmqiVSgT7KwLc8fpXfLB+H5MHZvOHC0bSrYPeCq5smqiViiNjDP9as4df/n0tZf4QvzprKLOPy9Ve79RBNFErFSfb91dx9z/W8NHGQobkdOD5qyYw+JgO8Q5LJSBN1Eq1MGMMcz/fxv1vr8fjEn45cyizj+uDx603sKj6aaJWqgVVBsLcGfUElgfOG0FOZkq8w1IJThO1Ui1k455yfvzCcrbqE1jUYdJErVSMGWN47rNtPPDOejKS9Qks6vA1mqhF5GlgJrDPGDM89iEp1XYUlge48/WvWLBhH9MGZfPgBaPIztAe79ThaUqN+hngUeC52IaiVNsRjljM/XwbD7//NYGwxT1nDWX28bl684o6Io0mamPMIhHJbYFYlGoTlm8r5udvrmHDnnJOGtCFX501jP5dtZ8OdeSarY1aRK4BrgHo3bt3cy1WqVYjYhke/XAzf1rwNTmZKfztknGcPrSb1qLVUWu2RG2MeRx4HCAvL88013KVag12llRz68srWbq1mHPH9OC+c4aRkeyNd1iqjdCrPpQ6CiVVQf768Tc88598PC7h4QtHcd7YnvEOS7UxmqiVOgKBcISnF+fz2EebqQiEOXd0D249bSC9OqXGOzTVBjXl8rx5wFSgi4gUAL8yxjwV68CUSlQLN+zjvrfWsbWoklMGd+X26YO0jw4VU0256uNHLRGIUoluR3EV9/5zHR+s30u/Lmk8e8UEpgzMjndYqh3Qpg+lGhEIR3j84y08unAzbpdw15mDufyEvvoUcNViNFEr1YCiigCvLivg+c+3sbOkmu+NyOEXM4doJ0qqxWmiVqqO/KJKHvnga95evZtQxDCpXyd+d/4IThqgzRwqPjRRK+Uoqgjwvws28eKS7fg8Li6e1IdZE3vTv2tGvENT7ZwmatXuVQbCPLV4K3/7+Bv8YYuLxvfi5lMH0DVDn1moEoMmatVuhSMWLy/bwf+8v4miigBnDOvG7WcM1n45VMLRRK3aHWMMH31dyANvr2fTvgrG52bxt0vGMa5PVrxDU6pemqhVu1FcGWTR14W8tryAxZuLyO2cyv9dPI4zhmnHSSqxaaJWbd6C9Xv5y8LNfLmjBGOgS7qPX84cyiWT+ui10KpV0ESt2qwthRX8+q11LNxYSN8uadx8ygCmDerKiB6Z+qxC1apoolZtzuZ95Tz5yVZeX1FAksfNL743hNnH5+J1a+1ZtU6aqFWbYIzhsy37eWLRFhZuLCTJ4+KH43tx0yl6mZ1q/TRRq1YtHLH419o9PL5oC18VlNIl3cdPThvIxZP60CnNF+/wlGoWmqhVq7R9fxWvLNvBq8t3sLcsQN8uaTxw7gjOG9uDZK873uEp1aw0UatWwRjD5n0VvL9+Lx+s28uK7SW4BKYMzOa+c3pz6pBuuPUEoWqjNFGrhGaMYfHmIh7690ZWFZQCMKJHJrefMYjzxvbQnuxUu6CJWiUkyzJ8vnU//7tgE59vKaZHxxTuPXsYpw/rpslZtTuaqFVC+aawgjdWFDD/y13sLKmmS7qPe84ayo8m9ibJo23Pqn3SRK3iriIQ5p2vdvPKsh0s23YAl8BJA7L52fRBnD70GFJ8mqBV+6aJWsVFUUWABev38t7avSzeXEQgbHFsdhp3nTmYc8f0oGsHvfZZqRqaqFWLMcbw2Tf7eX7JNt5bu5ewZejRMYUfTejNWaNyGNs7SztHUqoemqhVzH1TWME/V+3iHyt3saWoko6pXi4/IZdzx/RkSE6GJmelGqGJWjU7Ywyb9lXwrzV7eHfNHtbvLkMEJuR24oZp/fneyBy9KUWpw6CJWjWLYNjii/xiPtywjw837GNrUSUiMLZ3FnfPHMr3RubQTdudlToimqjVESssD7Bw4z4+XL+PTzYVUhmM4PO4mNSvM1ee2JfTh3bTk4JKNQNN1KrJLMuwemcpH27Yx8KN+/jKuVMwJzOZc8b04ORBXTm+f2dSffpvpVRz0m+UOqTS6hCffVPEgvX7WLixkKKKACIwpldHfnr6QE4e3E1PCCoVY5qo1UGKKgJ8sbWYJVuLWbq1mPV7yjAGMpI9TBmYzcmDuzJlYDad05PiHapS7YYm6naqtCrEtuJKthZVkl9UxdaiClbvLOWbwkoAkr0uxvXJ4pZTBjKpXyfG9snSJ6QoFSeaqNs4yzKs213GqoIS1u0qY93uMrYUVlJaHTpovh4dUxjYLZ0LxvViQt9OjOiRqQ9+VSpBaKJuYyzLsKWoki+3H2Dx5iIWbypif2UQsJsvhuR0YObIHHI7p9G7cyq5ndPo0zlVr2tWKoFpom6F/KEIG/eUs2ZXKRt2l1NSHaIqEKbcH2b9njLK/WEAOqf5OGlAFyYPzGZ8bid6ZqXoST+lgIgVwWBwiQuXuDDGEDZhLGPhFjcel50aLWNRFijjQOAA6/avY+mepSzZvYRgJED/jgMYkDWATsmdqAhVUB4sx+vycseEO5o9Xk3UCSxiGbYWVbB2Vxkb9pSzaW8F3xRWsL24iohlALuW3CU9iVSfmzSfh5kjuzOmV0dG9+5I/+x0XPrUExUjEStC2IRxixu3uGsrAZaxCFthQlaIUCRE0ApSFaqiIlRBRaiCiBUBQBBCVgh/xE8gEsAYg9flxePyYDAEIgH8YT9F1UVsPrCJbw5soixYTte0rhyT1p2s5CwsYxExEXu+yj0UVu2lJFgOxuAWwY2Q5k0l3ZdJSlIHSgMlFFUVUhIqP2TZXAg+l5eAFcJgasdnGGG83096OMSm0j28vOtTAiJ4DGQg5IgPNFG3ffsrAixYv4/31u1h8eYi/CELAK9b6NsljcHHZDBzZA7DundgWPdMrSW3csYYKkOVVIQqCFthjDFETISwFSZswkRMhCRXEineFJLcSVSFqigLllEWKKMsZP+tCFXgFjdp3jTSvGkIQtAKEowE7eXUDMZOnmErTCgSwh/2E4z4CYSrCUdChK0QwbCfymA5laEK/JEAHvHgdrkRcVEZqqIyXEVlJEDIRLCiEhiAR9xYxvrO+KMlxtArHObYYIgsy2LvgZ1s967hK5cbNwa3MfiMRZdwhOGRCFkR+ztjCYSBSpeLcpeLKpfQO2IxNhKhc8TCjcECIghuDC7AbSAiEBAhKEKyZehoWWRGIvSz3AzuMgR3n3GQ1hmqS4hUHSAQLCPFshATAV96s5a9hibqFuYPRSg4UM2OA1Vs31/Ftv1VFByoYk+Zn92lfgrLA4B9cu+Heb0Y1asjQ3I6cGx2up7cq0d1uJqKYAUpnhRSPCm4Xe7aGl0wEqQ6XE11uBp/xF/7k9YtbkJWiGAkiD/ip8RfQnFVIQcqduHxJJOW3Ik0XzqIU2u0wnaNMHCA8uoDVPiLqfAfoDJQisGQ5EklyZuGiBAIVROK+KkIVXIgVE5xqAq/CZPm8pHuSSbJ5SVohQhaIaojIcqsAOFmTmyNEWPwGUgyFsnG4DXgNQYP9us0y+IYyyLJGCwRak47pxpDumWRahl8xuDF4HESWwghLODCWZazTK+zzFTLIt1AmicZr3jAWBhj4REPyZ4kkl0+cHkIAyEB8fhITupIUnIWHVI7k5zWFVI7g9sL1QegqhiCFeD22YMnGdKyIa2LPZ8vHbwp4HJD+W4o2QHle8CXBilZkNLR/ozbXi8AVhhMBCIhCPshHLDnT+sK6dmQ0R3cB6dMN5DaAvtME3UzsSxDmT/E/sog+8oC7C3zs6fMz55Sf+3rXSXV7C0LHPS5FK+bXp1SyMlMYcgxHejdOZWpg7IZmtMhpjXlUCREabCUUCSEx+XB4/LU/hQNWSEiVgQLC8tYGAxuceMS+0BRFayk0n+AKv8BQuEA4UiAUCSIBRiBsIlQVrGH0sq9lFbvx+Vyk+ROwedJJmwidi0uEiBiWRgsjDH4I34qw9VUhqsJmAhhYxHGEDQR/FYYvwkjCB3EQ4bLhwH2RKooJXJQudxQZ0zz8hpDhmWRblmkWQYXhoAIAREshGRj4XMSUz/LYlwkQrIxVIqLSpfgF8FnDElOoszETUfxku5OwuP24XZ7cbm8eAx4rAguEyZghfFbIQImQqo7iQ7edDr4MujgyyDDm0a6L4NIJExlsJyqUAUYg1fceF0evO4kPJ4kPO5k3G4vXpcXt8sF4gZPkp34XB5wee3Xbh8kd4DkTPCkQCRoD1bYTn5JGXbyckWlDmO+TXLidpbntgdx/vrS7M/H49df1yEtv85m1qRELSLTgT9hfw+eNMb8LqZRJYBg2KKkOkhpVYjS6m+H4spg7VBYHmBvuZ99ZQH2VwaJWBZggSuMSAiXBElPMnRKS6JLRipj+3nJyXCTnW7ROSVEhs+PV6oIhaudf2AXESKs21bOsk3lVAZKKfEXUxIooSxUhdflxufy4nN58VshKiJ+qqwQiOCKSqRiWYixCFohqkyIaitMyKm1GaAKQ6XEvhbndpKaAYLOT0mPU5vzGYMbgwACJFuGNGORZkE64DEGD+AzQorLQ7Lbh4VQbgKUU4kxMFq8dHOn0MGVjN+EqTJhgsbCK4IXF15xk+pOIsWdRJI7CcuECTk1ZJ/LS5Lbi8+dRFZyJzqldiMrrSuhsJ+q6mIqAyWIMbidGniKL50MXweSkjPtGllaF7sGJy6oLgF/CRjLTmRJGZDUwRky7MQVrIBA2be1NF8aeNPA42u27Z3RbEtSiabRRC0ibuAvwGlAAfCFiPzDGLMuppFZFqFQFcFgOf5gJcFgOYFgOcFQFaFQFSEriD/opyrsJxj04w/5CUVChCOR2sEfDhAIBwlGAgTCfoJWgIDzczdoggStEGEihJwTEgZj1w4whDGExRARg+VUAgwQFnt8yGWICJh0sNIhU+yfbN8pBlDkDAScoajpm6FDJEKWZZFhhAjgxxAUIcUY0gykIYgxtW2DBjAuN0ZcpImbbPGQ4k7CKy47KRpIETeZLh8d3cn4XB7CiB27uPC6fXhdPtxuj31GHBdgiFhhLCsCGNK8aaR600n1Zdjzu314XF5cGFyWhUuEDhk9SM/sgyujG1gRCFVBqNquYbm9dm3O5XEOUALeVDupeVPiU+uKkhmLhXo6QWqnWCxZtQNNqVFPADYbY7YAiMhLwDlAsyfqM54cTqXLEBS7BhaJwRfWgyFZDMkYklzgMYJHajZETf0OUhC8CD4R7EYBcImQJC67hicePOJCENxin0ipOWPtc/tI8qSQ7Em1L/MxFhgLEcHrSsLnSSLJk0KKL52UpA4keVPto4CxEITU5I6kpnQiNaUz7vSukNLp4LaxSNj5WVln+1gRu4anJxeValOakqh7ADui3hcAE+vOJCLXANcA9O7d+4iCGWB1xYqAGy8u8eLGi1t8eMSLW5Jwu5JwuZLxuOxanM/tw+P24fWk4PUk43V78Xm9+DxuvF4PKb5UUn0ppCSl0TEtk4zULNLT0klKin+t7ai4G9htLr1pRam2qCmJur6M9p0GTmPM48DjAHl5eUfUAProNR8eyceUUqpNa8r1XgVAr6j3PYFdsQlHKaVUXU1J1F8AA0Skr4j4gIuAf8Q2LKWUUjUabfowxoRF5Ebg39iX5z1tjFkb88iUUkoBTbyO2hjzDvBOjGNRSilVD70nWSmlEpwmaqWUSnCaqJVSKsFpolZKqQQnxjR/5zwiUghsO8KPd+GwesNoE9pjmaF9lrs9lhnaZ7kPt8x9jDHZ9U2ISaI+GiKyzBiTF+84WlJ7LDO0z3K3xzJD+yx3c5ZZmz6UUirBaaJWSqkEl4iJ+vF4BxAH7bHM0D7L3R7LDO2z3M1W5oRro1ZKKXWwRKxRK6WUiqKJWimlElzCJGoRmS4iG0Vks4jcGe94YkVEeonIQhFZLyJrReRmZ3wnEXlfRDY5f7PiHWtzExG3iHwpIm8579tDmTuKyGsissHZ58e19XKLyK3O//YaEZknIsltscwi8rSI7BORNVHjGiyniNzl5LeNInLG4awrIRJ11AN0zwSGAj8SkaHxjSpmwsBtxpghwCTgBqesdwILjDEDgAXO+7bmZmB91Pv2UOY/Af8yxgwGRmGXv82WW0R6ADcBecaY4dhdI19E2yzzM8D0OuPqLafzHb8IGOZ85jEn7zWNMSbuA3Ac8O+o93cBd8U7rhYq+9+xn/C+EchxxuUAG+MdWzOXs6fzj3sy8JYzrq2XuQOwFeekfdT4Nltuvn3GaifsbpTfAk5vq2UGcoE1je3bujkNu3//45q6noSoUVP/A3R7xCmWFiMiucAYYAnQzRizG8D52zWOocXCI8DPACtqXFsvcz+gEJjjNPk8KSJptOFyG2N2Ag8B24HdQKkx5j3acJnraKicR5XjEiVRN+kBum2JiKQDrwO3GGPK4h1PLInITGCfMWZ5vGNpYR5gLPBXY8wYoJK28ZO/QU6b7DlAX6A7kCYiF8c3qoRwVDkuURJ1u3qAroh4sZP0C8aYN5zRe0Ukx5meA+yLV3wxcAJwtojkAy8BJ4vI87TtMoP9f11gjFnivH8NO3G35XKfCmw1xhQaY0LAG8DxtO0yR2uonEeV4xIlUbebB+iKiABPAeuNMQ9HTfoHMNt5PRu77bpNMMbcZYzpaYzJxd63HxpjLqYNlxnAGLMH2CEig5xRpwDraNvl3g5MEpFU53/9FOwTqG25zNEaKuc/gItEJElE+gIDgKVNXmq8G+OjGtdnAF8D3wA/j3c8MSznidg/eb4CVjrDDKAz9sm2Tc7fTvGONUbln8q3JxPbfJmB0cAyZ3/PB7LaermBe4ENwBpgLpDUFssMzMNuhw9h15ivPFQ5gZ87+W0jcObhrEtvIVdKqQSXKE0fSimlGqCJWimlEpwmaqWUSnCaqJVSKsFpolZKqQSniVoppRKcJmqllEpw/x/Ub1bitd19IwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(cpu_times[::-1], label=\"python cpu\")\n",
    "plt.plot(cuda_times[::-1], label=\"pytorch\")\n",
    "plt.plot(cuda_sort_times[::-1], label=\"pytorch sort\")\n",
    "plt.title(\"Agents/Assets v. Seconds (Batch Size 10)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "04be095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cuda_times[::-1], label=\"pytorch\")\n",
    "plt.plot(cuda_sort_times[::-1], label=\"pytorch sort\")\n",
    "plt.title(\"Agents/Assets v. Seconds\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29ac5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_alloc = 10000*np.ones((1, n_orders_bid // num_assets, num_assets))\n",
    "cur_cash = np.zeros((1, n_orders_bid // num_assets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86337e15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'asset': 0,\n",
       "  'side': 'bid',\n",
       "  'vol': 67,\n",
       "  'price': 85.75687677196541,\n",
       "  'id': 0,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 1,\n",
       "  'side': 'bid',\n",
       "  'vol': 75,\n",
       "  'price': 88.94581491683304,\n",
       "  'id': 0,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 2,\n",
       "  'side': 'bid',\n",
       "  'vol': 93,\n",
       "  'price': 68.94155137246898,\n",
       "  'id': 0,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 3,\n",
       "  'side': 'bid',\n",
       "  'vol': 88,\n",
       "  'price': 82.82738098452694,\n",
       "  'id': 0,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 4,\n",
       "  'side': 'bid',\n",
       "  'vol': 66,\n",
       "  'price': 12.824223925434586,\n",
       "  'id': 0,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 0,\n",
       "  'side': 'bid',\n",
       "  'vol': 11,\n",
       "  'price': 26.201170830365818,\n",
       "  'id': 1,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 1,\n",
       "  'side': 'bid',\n",
       "  'vol': 43,\n",
       "  'price': 54.731683146710495,\n",
       "  'id': 1,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 2,\n",
       "  'side': 'bid',\n",
       "  'vol': 54,\n",
       "  'price': 25.291157423731836,\n",
       "  'id': 1,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 3,\n",
       "  'side': 'bid',\n",
       "  'vol': 16,\n",
       "  'price': 22.998322688153984,\n",
       "  'id': 1,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 4,\n",
       "  'side': 'bid',\n",
       "  'vol': 29,\n",
       "  'price': 76.26712801177462,\n",
       "  'id': 1,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 0,\n",
       "  'side': 'bid',\n",
       "  'vol': 21,\n",
       "  'price': 9.955191504274552,\n",
       "  'id': 2,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 1,\n",
       "  'side': 'bid',\n",
       "  'vol': 86,\n",
       "  'price': 79.94415642501053,\n",
       "  'id': 2,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 2,\n",
       "  'side': 'bid',\n",
       "  'vol': 26,\n",
       "  'price': 41.31968758783779,\n",
       "  'id': 2,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'bid',\n",
       "  'vol': 78,\n",
       "  'price': 96.72012369304097,\n",
       "  'id': 2,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 4,\n",
       "  'side': 'bid',\n",
       "  'vol': 39,\n",
       "  'price': 9.315566554859412,\n",
       "  'id': 2,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 0,\n",
       "  'side': 'bid',\n",
       "  'vol': 49,\n",
       "  'price': 11.44940277940002,\n",
       "  'id': 3,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 1,\n",
       "  'side': 'bid',\n",
       "  'vol': 76,\n",
       "  'price': 65.75882518661265,\n",
       "  'id': 3,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 2,\n",
       "  'side': 'bid',\n",
       "  'vol': 74,\n",
       "  'price': 68.68827368678421,\n",
       "  'id': 3,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'bid',\n",
       "  'vol': 48,\n",
       "  'price': 34.79575719309136,\n",
       "  'id': 3,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 4,\n",
       "  'side': 'bid',\n",
       "  'vol': 77,\n",
       "  'price': 89.65488816642353,\n",
       "  'id': 3,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 0,\n",
       "  'side': 'bid',\n",
       "  'vol': 14,\n",
       "  'price': 71.90799419707791,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 1,\n",
       "  'side': 'bid',\n",
       "  'vol': 70,\n",
       "  'price': 36.276054463608844,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 2,\n",
       "  'side': 'bid',\n",
       "  'vol': 72,\n",
       "  'price': 12.786050848109575,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'bid',\n",
       "  'vol': 66,\n",
       "  'price': 50.47110758676929,\n",
       "  'id': 4,\n",
       "  'instr': 'leverage'},\n",
       " {'asset': 4,\n",
       "  'side': 'bid',\n",
       "  'vol': 34,\n",
       "  'price': 7.42163617840178,\n",
       "  'id': 4,\n",
       "  'instr': 'credit'},\n",
       " {'asset': 0,\n",
       "  'side': 'ask',\n",
       "  'vol': 53,\n",
       "  'price': 60.44113403631704,\n",
       "  'id': 0,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 1,\n",
       "  'side': 'ask',\n",
       "  'vol': 1,\n",
       "  'price': 17.401638129895826,\n",
       "  'id': 0,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 2,\n",
       "  'side': 'ask',\n",
       "  'vol': 40,\n",
       "  'price': 40.2803282388351,\n",
       "  'id': 0,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'ask',\n",
       "  'vol': 13,\n",
       "  'price': 77.82615735332267,\n",
       "  'id': 0,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 4,\n",
       "  'side': 'ask',\n",
       "  'vol': 58,\n",
       "  'price': 68.23709406326972,\n",
       "  'id': 0,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 0,\n",
       "  'side': 'ask',\n",
       "  'vol': 12,\n",
       "  'price': 44.13719387512535,\n",
       "  'id': 1,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 1,\n",
       "  'side': 'ask',\n",
       "  'vol': 91,\n",
       "  'price': 60.46202779624511,\n",
       "  'id': 1,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 2,\n",
       "  'side': 'ask',\n",
       "  'vol': 31,\n",
       "  'price': 18.992631134378534,\n",
       "  'id': 1,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'ask',\n",
       "  'vol': 47,\n",
       "  'price': 33.81810628210091,\n",
       "  'id': 1,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 4,\n",
       "  'side': 'ask',\n",
       "  'vol': 50,\n",
       "  'price': 12.799281995579282,\n",
       "  'id': 1,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 0,\n",
       "  'side': 'ask',\n",
       "  'vol': 48,\n",
       "  'price': 63.03026211139492,\n",
       "  'id': 2,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 1,\n",
       "  'side': 'ask',\n",
       "  'vol': 27,\n",
       "  'price': 32.747055894519136,\n",
       "  'id': 2,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 2,\n",
       "  'side': 'ask',\n",
       "  'vol': 47,\n",
       "  'price': 17.88688418884209,\n",
       "  'id': 2,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'ask',\n",
       "  'vol': 90,\n",
       "  'price': 6.560196964944542,\n",
       "  'id': 2,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 4,\n",
       "  'side': 'ask',\n",
       "  'vol': 81,\n",
       "  'price': 66.03079045497225,\n",
       "  'id': 2,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 0,\n",
       "  'side': 'ask',\n",
       "  'vol': 26,\n",
       "  'price': 61.18171520410072,\n",
       "  'id': 3,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 1,\n",
       "  'side': 'ask',\n",
       "  'vol': 72,\n",
       "  'price': 22.281735564208383,\n",
       "  'id': 3,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 2,\n",
       "  'side': 'ask',\n",
       "  'vol': 66,\n",
       "  'price': 86.17280144499064,\n",
       "  'id': 3,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'ask',\n",
       "  'vol': 54,\n",
       "  'price': 4.219255922290078,\n",
       "  'id': 3,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 4,\n",
       "  'side': 'ask',\n",
       "  'vol': 30,\n",
       "  'price': 46.949882516290565,\n",
       "  'id': 3,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 0,\n",
       "  'side': 'ask',\n",
       "  'vol': 75,\n",
       "  'price': 90.48396741732535,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 1,\n",
       "  'side': 'ask',\n",
       "  'vol': 47,\n",
       "  'price': 76.85366428589523,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 2,\n",
       "  'side': 'ask',\n",
       "  'vol': 61,\n",
       "  'price': 18.974165819078394,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 3,\n",
       "  'side': 'ask',\n",
       "  'vol': 66,\n",
       "  'price': 73.66189023243041,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'},\n",
       " {'asset': 4,\n",
       "  'side': 'ask',\n",
       "  'vol': 39,\n",
       "  'price': 31.826819683777828,\n",
       "  'id': 4,\n",
       "  'instr': 'cash'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_books[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18bcab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tradelist__lk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b94fb948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0ba4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tradelist__lk:\n",
    "    cur_alloc[0][t['id']][t['asset']] += t['cleared_vol']\n",
    "    cur_cash[0][t['id']] += t['cleared_cash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e88389d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[10014., 10074., 10053., 10088., 10000.],\n",
       "        [ 9988.,  9909.,  9969.,  9953.,  9979.],\n",
       "        [10000., 10059.,  9965.,  9988., 10000.],\n",
       "        [ 9984.,  9958., 10074.,  9946., 10060.],\n",
       "        [10014., 10000.,  9939., 10025.,  9961.]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_alloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73ad4a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-19063.85246865,  15362.75507039,  -1710.56478309,\n",
       "           142.6288047 ,   5269.03337664]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6bd327d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10014., 10074., 10053., 10088., 10000.],\n",
       "        [ 9988.,  9909.,  9969.,  9953.,  9979.],\n",
       "        [10000., 10059.,  9965.,  9988., 10000.],\n",
       "        [ 9984.,  9958., 10074.,  9946., 10060.],\n",
       "        [10014., 10000.,  9939., 10025.,  9961.]], device='cuda:0')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.cur_alloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c449349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19063.8535,  15362.7549,  -1710.5649,    142.6283,   5269.0332],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.cash[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6f2e082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10014., 10074., 10053., 10088., 10000.],\n",
       "        [ 9988.,  9909.,  9969.,  9953.,  9979.],\n",
       "        [10000., 10059.,  9965.,  9988., 10000.],\n",
       "        [ 9984.,  9958., 10074.,  9946., 10060.],\n",
       "        [10014., 10000.,  9939., 10025.,  9961.]], device='cuda:0')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.cur_alloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb8db646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-19063.8535,  15362.7549,  -1710.5649,    142.6283,   5269.0332],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2.cash[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
